

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>pyspark.sql.pandas.group_ops &#8212; PySpark 4.0.0-preview2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/pyspark.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/pyspark/sql/pandas/group_ops';</script>
    <link rel="canonical" href="https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/pandas/group_ops.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Matomo -->
    <script type="text/javascript">
        var _paq = window._paq = window._paq || [];
        /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
        _paq.push(["disableCookies"]);
        _paq.push(['trackPageView']);
        _paq.push(['enableLinkTracking']);
        (function() {
            var u="https://analytics.apache.org/";
            _paq.push(['setTrackerUrl', u+'matomo.php']);
            _paq.push(['setSiteId', '40']);
            var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
            g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
        })();
    </script>
    <!-- End Matomo Code -->

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../../../index.html">
  
  
  
  
    
    
      
    
    
    <img src="https://spark.apache.org/images/spark-logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="https://spark.apache.org/images/spark-logo-rev.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../index.html">
                        Overview
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../getting_started/index.html">
                        Getting Started
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../user_guide/index.html">
                        User Guides
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../reference/index.html">
                        API Reference
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../development/index.html">
                        Development
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../migration_guide/index.html">
                        Migration Guides
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item"><!--
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<div id="version-button" class="dropdown">
    <button type="button" class="btn btn-secondary btn-sm navbar-btn dropdown-toggle" id="version_switcher_button" data-toggle="dropdown">
        4.0.0-preview2
        <span class="caret"></span>
    </button>
    <div id="version_switcher" class="dropdown-menu list-group-flush py-0" aria-labelledby="version_switcher_button">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
</div>

<script type="text/javascript">
// Function to construct the target URL from the JSON components
function buildURL(entry) {
    var template = "https://spark.apache.org/docs/{version}/api/python/index.html";  // supplied by jinja
    template = template.replace("{version}", entry.version);
    return template;
}

// Function to check if corresponding page path exists in other version of docs
// and, if so, go there instead of the homepage of the other docs version
function checkPageExistsAndRedirect(event) {
    const currentFilePath = "_modules/pyspark/sql/pandas/group_ops.html",
          otherDocsHomepage = event.target.getAttribute("href");
    let tryUrl = `${otherDocsHomepage}${currentFilePath}`;
    $.ajax({
        type: 'HEAD',
        url: tryUrl,
        // if the page exists, go there
        success: function() {
            location.href = tryUrl;
        }
    }).fail(function() {
        location.href = otherDocsHomepage;
    });
    return false;
}

// Function to populate the version switcher
(function () {
    // get JSON config
    $.getJSON("https://spark.apache.org/static/versions.json", function(data, textStatus, jqXHR) {
        // create the nodes first (before AJAX calls) to ensure the order is
        // correct (for now, links will go to doc version homepage)
        $.each(data, function(index, entry) {
            // if no custom name specified (e.g., "latest"), use version string
            if (!("name" in entry)) {
                entry.name = entry.version;
            }
            // construct the appropriate URL, and add it to the dropdown
            entry.url = buildURL(entry);
            const node = document.createElement("a");
            node.setAttribute("class", "list-group-item list-group-item-action py-1");
            node.setAttribute("href", `${entry.url}`);
            node.textContent = `${entry.name}`;
            node.onclick = checkPageExistsAndRedirect;
            $("#version_switcher").append(node);
        });
    });
})();
</script></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/apache/spark" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/pyspark" title="PyPI" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-solid fa-box"></i></span>
            <label class="sr-only">PyPI</label></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../index.html">
                        Overview
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../getting_started/index.html">
                        Getting Started
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../user_guide/index.html">
                        User Guides
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../reference/index.html">
                        API Reference
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../development/index.html">
                        Development
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../migration_guide/index.html">
                        Migration Guides
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item"><!--
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<div id="version-button" class="dropdown">
    <button type="button" class="btn btn-secondary btn-sm navbar-btn dropdown-toggle" id="version_switcher_button" data-toggle="dropdown">
        4.0.0-preview2
        <span class="caret"></span>
    </button>
    <div id="version_switcher" class="dropdown-menu list-group-flush py-0" aria-labelledby="version_switcher_button">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
</div>

<script type="text/javascript">
// Function to construct the target URL from the JSON components
function buildURL(entry) {
    var template = "https://spark.apache.org/docs/{version}/api/python/index.html";  // supplied by jinja
    template = template.replace("{version}", entry.version);
    return template;
}

// Function to check if corresponding page path exists in other version of docs
// and, if so, go there instead of the homepage of the other docs version
function checkPageExistsAndRedirect(event) {
    const currentFilePath = "_modules/pyspark/sql/pandas/group_ops.html",
          otherDocsHomepage = event.target.getAttribute("href");
    let tryUrl = `${otherDocsHomepage}${currentFilePath}`;
    $.ajax({
        type: 'HEAD',
        url: tryUrl,
        // if the page exists, go there
        success: function() {
            location.href = tryUrl;
        }
    }).fail(function() {
        location.href = otherDocsHomepage;
    });
    return false;
}

// Function to populate the version switcher
(function () {
    // get JSON config
    $.getJSON("https://spark.apache.org/static/versions.json", function(data, textStatus, jqXHR) {
        // create the nodes first (before AJAX calls) to ensure the order is
        // correct (for now, links will go to doc version homepage)
        $.each(data, function(index, entry) {
            // if no custom name specified (e.g., "latest"), use version string
            if (!("name" in entry)) {
                entry.name = entry.version;
            }
            // construct the appropriate URL, and add it to the dropdown
            entry.url = buildURL(entry);
            const node = document.createElement("a");
            node.setAttribute("class", "list-group-item list-group-item-action py-1");
            node.setAttribute("href", `${entry.url}`);
            node.textContent = `${entry.name}`;
            node.onclick = checkPageExistsAndRedirect;
            $("#version_switcher").append(node);
        });
    });
})();
</script></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/apache/spark" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/pyspark" title="PyPI" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-solid fa-box"></i></span>
            <label class="sr-only">PyPI</label></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">pyspark.sql.pandas.group_ops</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <h1>Source code for pyspark.sql.pandas.group_ops</h1><div class="highlight"><pre>
<span></span><span class="c1">#</span>
<span class="c1"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
<span class="c1"># contributor license agreements.  See the NOTICE file distributed with</span>
<span class="c1"># this work for additional information regarding copyright ownership.</span>
<span class="c1"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
<span class="c1"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
<span class="c1"># the License.  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1">#</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">cast</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">pyspark.errors</span> <span class="kn">import</span> <span class="n">PySparkTypeError</span>
<span class="kn">from</span> <span class="nn">pyspark.util</span> <span class="kn">import</span> <span class="n">PythonEvalType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.column</span> <span class="kn">import</span> <span class="n">Column</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.dataframe</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.streaming.state</span> <span class="kn">import</span> <span class="n">GroupStateTimeout</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.streaming.stateful_processor_api_client</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">StatefulProcessorApiClient</span><span class="p">,</span>
    <span class="n">StatefulProcessorHandleState</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.streaming.stateful_processor</span> <span class="kn">import</span> <span class="n">StatefulProcessor</span><span class="p">,</span> <span class="n">StatefulProcessorHandle</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">_parse_datatype_string</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql.pandas._typing</span> <span class="kn">import</span> <span class="p">(</span>
        <span class="n">GroupedMapPandasUserDefinedFunction</span><span class="p">,</span>
        <span class="n">PandasGroupedMapFunction</span><span class="p">,</span>
        <span class="n">PandasGroupedMapFunctionWithState</span><span class="p">,</span>
        <span class="n">PandasCogroupedMapFunction</span><span class="p">,</span>
        <span class="n">ArrowGroupedMapFunction</span><span class="p">,</span>
        <span class="n">ArrowCogroupedMapFunction</span><span class="p">,</span>
        <span class="n">DataFrameLike</span> <span class="k">as</span> <span class="n">PandasDataFrameLike</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql.group</span> <span class="kn">import</span> <span class="n">GroupedData</span>


<span class="k">class</span> <span class="nc">PandasGroupedOpsMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Min-in for pandas grouped operations. Currently, only :class:`GroupedData`</span>
<span class="sd">    can use this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">udf</span><span class="p">:</span> <span class="s2">&quot;GroupedMapPandasUserDefinedFunction&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        It is an alias of :meth:`pyspark.sql.GroupedData.applyInPandas`; however, it takes a</span>
<span class="sd">        :meth:`pyspark.sql.functions.pandas_udf` whereas</span>
<span class="sd">        :meth:`pyspark.sql.GroupedData.applyInPandas` takes a Python native function.</span>

<span class="sd">        .. versionadded:: 2.3.0</span>

<span class="sd">        .. versionchanged:: 3.4.0</span>
<span class="sd">            Support Spark Connect.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        udf : :func:`pyspark.sql.functions.pandas_udf`</span>
<span class="sd">            a grouped map user-defined function returned by</span>
<span class="sd">            :func:`pyspark.sql.functions.pandas_udf`.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        It is preferred to use :meth:`pyspark.sql.GroupedData.applyInPandas` over this</span>
<span class="sd">        API. This API will be deprecated in the future releases.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.functions import pandas_udf, PandasUDFType</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">        ...     (&quot;id&quot;, &quot;v&quot;))</span>
<span class="sd">        &gt;&gt;&gt; @pandas_udf(&quot;id long, v double&quot;, PandasUDFType.GROUPED_MAP)  # doctest: +SKIP</span>
<span class="sd">        ... def normalize(pdf):</span>
<span class="sd">        ...     v = pdf.v</span>
<span class="sd">        ...     return pdf.assign(v=(v - v.mean()) / v.std())</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&quot;id&quot;).apply(normalize).show()  # doctest: +SKIP</span>
<span class="sd">        +---+-------------------+</span>
<span class="sd">        | id|                  v|</span>
<span class="sd">        +---+-------------------+</span>
<span class="sd">        |  1|-0.7071067811865475|</span>
<span class="sd">        |  1| 0.7071067811865475|</span>
<span class="sd">        |  2|-0.8320502943378437|</span>
<span class="sd">        |  2|-0.2773500981126146|</span>
<span class="sd">        |  2| 1.1094003924504583|</span>
<span class="sd">        +---+-------------------+</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        pyspark.sql.functions.pandas_udf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Columns are special because hasattr always return True</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">udf</span><span class="p">,</span> <span class="n">Column</span><span class="p">)</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">udf</span><span class="p">,</span> <span class="s2">&quot;func&quot;</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span>
                <span class="n">udf</span><span class="o">.</span><span class="n">evalType</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="o">!=</span> <span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF</span>
            <span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="n">PySparkTypeError</span><span class="p">(</span>
                <span class="n">errorClass</span><span class="o">=</span><span class="s2">&quot;INVALID_UDF_EVAL_TYPE&quot;</span><span class="p">,</span>
                <span class="n">messageParameters</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;eval_type&quot;</span><span class="p">:</span> <span class="s2">&quot;SQL_GROUPED_MAP_PANDAS_UDF&quot;</span><span class="p">},</span>
            <span class="p">)</span>

        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;It is preferred to use &#39;applyInPandas&#39; over this &quot;</span>
            <span class="s2">&quot;API. This API will be deprecated in the future releases. See SPARK-28264 for &quot;</span>
            <span class="s2">&quot;more details.&quot;</span><span class="p">,</span>
            <span class="ne">UserWarning</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">applyInPandas</span><span class="p">(</span><span class="n">udf</span><span class="o">.</span><span class="n">func</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">udf</span><span class="o">.</span><span class="n">returnType</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

    <span class="k">def</span> <span class="nf">applyInPandas</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="s2">&quot;PandasGroupedMapFunction&quot;</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Maps each group of the current :class:`DataFrame` using a pandas udf and returns the result</span>
<span class="sd">        as a `DataFrame`.</span>

<span class="sd">        The function should take a `pandas.DataFrame` and return another</span>
<span class="sd">        `pandas.DataFrame`. Alternatively, the user can pass a function that takes</span>
<span class="sd">        a tuple of the grouping key(s) and a `pandas.DataFrame`.</span>
<span class="sd">        For each group, all columns are passed together as a `pandas.DataFrame`</span>
<span class="sd">        to the user-function and the returned `pandas.DataFrame` are combined as a</span>
<span class="sd">        :class:`DataFrame`.</span>

<span class="sd">        The `schema` should be a :class:`StructType` describing the schema of the returned</span>
<span class="sd">        `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match</span>
<span class="sd">        the field names in the defined schema if specified as strings, or match the</span>
<span class="sd">        field data types by position if not strings, e.g. integer indices.</span>
<span class="sd">        The length of the returned `pandas.DataFrame` can be arbitrary.</span>

<span class="sd">        .. versionadded:: 3.0.0</span>

<span class="sd">        .. versionchanged:: 3.4.0</span>
<span class="sd">            Support Spark Connect.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        func : function</span>
<span class="sd">            a Python native function that takes a `pandas.DataFrame` and outputs a</span>
<span class="sd">            `pandas.DataFrame`, or that takes one tuple (grouping keys) and a</span>
<span class="sd">            `pandas.DataFrame` and outputs a `pandas.DataFrame`.</span>
<span class="sd">        schema : :class:`pyspark.sql.types.DataType` or str</span>
<span class="sd">            the return type of the `func` in PySpark. The value can be either a</span>
<span class="sd">            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import pandas as pd  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.functions import ceil</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">        ...     (&quot;id&quot;, &quot;v&quot;))  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; def normalize(pdf):</span>
<span class="sd">        ...     v = pdf.v</span>
<span class="sd">        ...     return pdf.assign(v=(v - v.mean()) / v.std())</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&quot;id&quot;).applyInPandas(</span>
<span class="sd">        ...     normalize, schema=&quot;id long, v double&quot;).show()  # doctest: +SKIP</span>
<span class="sd">        +---+-------------------+</span>
<span class="sd">        | id|                  v|</span>
<span class="sd">        +---+-------------------+</span>
<span class="sd">        |  1|-0.7071067811865475|</span>
<span class="sd">        |  1| 0.7071067811865475|</span>
<span class="sd">        |  2|-0.8320502943378437|</span>
<span class="sd">        |  2|-0.2773500981126146|</span>
<span class="sd">        |  2| 1.1094003924504583|</span>
<span class="sd">        +---+-------------------+</span>

<span class="sd">        Alternatively, the user can pass a function that takes two arguments.</span>
<span class="sd">        In this case, the grouping key(s) will be passed as the first argument and the data will</span>
<span class="sd">        be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy</span>
<span class="sd">        data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in</span>
<span class="sd">        as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.</span>
<span class="sd">        This is useful when the user does not want to hardcode grouping key(s) in the function.</span>

<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">        ...     (&quot;id&quot;, &quot;v&quot;))  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; def mean_func(key, pdf):</span>
<span class="sd">        ...     # key is a tuple of one numpy.int64, which is the value</span>
<span class="sd">        ...     # of &#39;id&#39; for the current group</span>
<span class="sd">        ...     return pd.DataFrame([key + (pdf.v.mean(),)])</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&#39;id&#39;).applyInPandas(</span>
<span class="sd">        ...     mean_func, schema=&quot;id long, v double&quot;).show()  # doctest: +SKIP</span>
<span class="sd">        +---+---+</span>
<span class="sd">        | id|  v|</span>
<span class="sd">        +---+---+</span>
<span class="sd">        |  1|1.5|</span>
<span class="sd">        |  2|6.0|</span>
<span class="sd">        +---+---+</span>

<span class="sd">        &gt;&gt;&gt; def sum_func(key, pdf):</span>
<span class="sd">        ...     # key is a tuple of two numpy.int64s, which is the values</span>
<span class="sd">        ...     # of &#39;id&#39; and &#39;ceil(df.v / 2)&#39; for the current group</span>
<span class="sd">        ...     return pd.DataFrame([key + (pdf.v.sum(),)])</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(df.id, ceil(df.v / 2)).applyInPandas(</span>
<span class="sd">        ...     sum_func, schema=&quot;id long, `ceil(v / 2)` long, v double&quot;).show()  # doctest: +SKIP</span>
<span class="sd">        +---+-----------+----+</span>
<span class="sd">        | id|ceil(v / 2)|   v|</span>
<span class="sd">        +---+-----------+----+</span>
<span class="sd">        |  2|          5|10.0|</span>
<span class="sd">        |  1|          1| 3.0|</span>
<span class="sd">        |  2|          3| 5.0|</span>
<span class="sd">        |  2|          2| 3.0|</span>
<span class="sd">        +---+-----------+----+</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function requires a full shuffle. All the data of a group will be loaded</span>
<span class="sd">        into memory, so the user should be aware of the potential OOM risk if data is skewed</span>
<span class="sd">        and certain groups are too large to fit in memory.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        pyspark.sql.functions.pandas_udf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">GroupedData</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span><span class="p">,</span> <span class="n">PandasUDFType</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">GroupedData</span><span class="p">)</span>

        <span class="n">udf</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">functionType</span><span class="o">=</span><span class="n">PandasUDFType</span><span class="o">.</span><span class="n">GROUPED_MAP</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span>
        <span class="n">udf_column</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">])</span>
        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jgd</span><span class="o">.</span><span class="n">flatMapGroupsInPandas</span><span class="p">(</span><span class="n">udf_column</span><span class="o">.</span><span class="n">_jc</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">applyInPandasWithState</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">func</span><span class="p">:</span> <span class="s2">&quot;PandasGroupedMapFunctionWithState&quot;</span><span class="p">,</span>
        <span class="n">outputStructType</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">stateStructType</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">outputMode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">timeoutConf</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies the given function to each group of data, while maintaining a user-defined</span>
<span class="sd">        per-group state. The result Dataset will represent the flattened record returned by the</span>
<span class="sd">        function.</span>

<span class="sd">        For a streaming :class:`DataFrame`, the function will be invoked first for all input groups</span>
<span class="sd">        and then for all timed out states where the input data is set to be empty. Updates to each</span>
<span class="sd">        group&#39;s state will be saved across invocations.</span>

<span class="sd">        The function should take parameters (key, Iterator[`pandas.DataFrame`], state) and</span>
<span class="sd">        return another Iterator[`pandas.DataFrame`]. The grouping key(s) will be passed as a tuple</span>
<span class="sd">        of numpy data types, e.g., `numpy.int32` and `numpy.float64`. The state will be passed as</span>
<span class="sd">        :class:`pyspark.sql.streaming.state.GroupState`.</span>

<span class="sd">        For each group, all columns are passed together as `pandas.DataFrame` to the user-function,</span>
<span class="sd">        and the returned `pandas.DataFrame` across all invocations are combined as a</span>
<span class="sd">        :class:`DataFrame`. Note that the user function should not make a guess of the number of</span>
<span class="sd">        elements in the iterator. To process all data, the user function needs to iterate all</span>
<span class="sd">        elements and process them. On the other hand, the user function is not strictly required to</span>
<span class="sd">        iterate through all elements in the iterator if it intends to read a part of data.</span>

<span class="sd">        The `outputStructType` should be a :class:`StructType` describing the schema of all</span>
<span class="sd">        elements in the returned value, `pandas.DataFrame`. The column labels of all elements in</span>
<span class="sd">        returned `pandas.DataFrame` must either match the field names in the defined schema if</span>
<span class="sd">        specified as strings, or match the field data types by position if not strings,</span>
<span class="sd">        e.g. integer indices.</span>

<span class="sd">        The `stateStructType` should be :class:`StructType` describing the schema of the</span>
<span class="sd">        user-defined state. The value of the state will be presented as a tuple, as well as the</span>
<span class="sd">        update should be performed with the tuple. The corresponding Python types for</span>
<span class="sd">        :class:DataType are supported. Please refer to the page</span>
<span class="sd">        https://spark.apache.org/docs/latest/sql-ref-datatypes.html (Python tab).</span>

<span class="sd">        The size of each `pandas.DataFrame` in both the input and output can be arbitrary. The</span>
<span class="sd">        number of `pandas.DataFrame` in both the input and output can also be arbitrary.</span>

<span class="sd">        .. versionadded:: 3.4.0</span>

<span class="sd">        .. versionchanged:: 3.5.0</span>
<span class="sd">            Supports Spark Connect.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        func : function</span>
<span class="sd">            a Python native function to be called on every group. It should take parameters</span>
<span class="sd">            (key, Iterator[`pandas.DataFrame`], state) and return Iterator[`pandas.DataFrame`].</span>
<span class="sd">            Note that the type of the key is tuple and the type of the state is</span>
<span class="sd">            :class:`pyspark.sql.streaming.state.GroupState`.</span>
<span class="sd">        outputStructType : :class:`pyspark.sql.types.DataType` or str</span>
<span class="sd">            the type of the output records. The value can be either a</span>
<span class="sd">            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>
<span class="sd">        stateStructType : :class:`pyspark.sql.types.DataType` or str</span>
<span class="sd">            the type of the user-defined state. The value can be either a</span>
<span class="sd">            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>
<span class="sd">        outputMode : str</span>
<span class="sd">            the output mode of the function.</span>
<span class="sd">        timeoutConf : str</span>
<span class="sd">            timeout configuration for groups that do not receive data for a while. valid values</span>
<span class="sd">            are defined in :class:`pyspark.sql.streaming.state.GroupStateTimeout`.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import pandas as pd  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.streaming.state import GroupStateTimeout</span>
<span class="sd">        &gt;&gt;&gt; def count_fn(key, pdf_iter, state):</span>
<span class="sd">        ...     assert isinstance(state, GroupStateImpl)</span>
<span class="sd">        ...     total_len = 0</span>
<span class="sd">        ...     for pdf in pdf_iter:</span>
<span class="sd">        ...         total_len += len(pdf)</span>
<span class="sd">        ...     state.update((total_len,))</span>
<span class="sd">        ...     yield pd.DataFrame({&quot;id&quot;: [key[0]], &quot;countAsString&quot;: [str(total_len)]})</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&quot;id&quot;).applyInPandasWithState(</span>
<span class="sd">        ...     count_fn, outputStructType=&quot;id long, countAsString string&quot;,</span>
<span class="sd">        ...     stateStructType=&quot;len long&quot;, outputMode=&quot;Update&quot;,</span>
<span class="sd">        ...     timeoutConf=GroupStateTimeout.NoTimeout) # doctest: +SKIP</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function requires a full shuffle.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">GroupedData</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">GroupedData</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">timeoutConf</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">GroupStateTimeout</span><span class="o">.</span><span class="n">NoTimeout</span><span class="p">,</span>
            <span class="n">GroupStateTimeout</span><span class="o">.</span><span class="n">ProcessingTimeTimeout</span><span class="p">,</span>
            <span class="n">GroupStateTimeout</span><span class="o">.</span><span class="n">EventTimeTimeout</span><span class="p">,</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputStructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">outputStructType</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">StructType</span><span class="p">,</span> <span class="n">_parse_datatype_string</span><span class="p">(</span><span class="n">outputStructType</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stateStructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">stateStructType</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">StructType</span><span class="p">,</span> <span class="n">_parse_datatype_string</span><span class="p">(</span><span class="n">stateStructType</span><span class="p">))</span>

        <span class="n">udf</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span>  <span class="c1"># type: ignore[call-overload]</span>
            <span class="n">returnType</span><span class="o">=</span><span class="n">outputStructType</span><span class="p">,</span>
            <span class="n">functionType</span><span class="o">=</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_PANDAS_UDF_WITH_STATE</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span>
        <span class="n">udf_column</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">])</span>
        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jgd</span><span class="o">.</span><span class="n">applyInPandasWithState</span><span class="p">(</span>
            <span class="n">udf_column</span><span class="o">.</span><span class="n">_jc</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="o">.</span><span class="n">parseDataType</span><span class="p">(</span><span class="n">outputStructType</span><span class="o">.</span><span class="n">json</span><span class="p">()),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="o">.</span><span class="n">parseDataType</span><span class="p">(</span><span class="n">stateStructType</span><span class="o">.</span><span class="n">json</span><span class="p">()),</span>
            <span class="n">outputMode</span><span class="p">,</span>
            <span class="n">timeoutConf</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">transformWithStateInPandas</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">statefulProcessor</span><span class="p">:</span> <span class="n">StatefulProcessor</span><span class="p">,</span>
        <span class="n">outputStructType</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
        <span class="n">outputMode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">timeMode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Invokes methods defined in the stateful processor used in arbitrary state API v2. It</span>
<span class="sd">        requires protobuf, pandas and pyarrow as dependencies to process input/state data. We</span>
<span class="sd">        allow the user to act on per-group set of input rows along with keyed state and the user</span>
<span class="sd">        can choose to output/return 0 or more rows.</span>

<span class="sd">        For a streaming dataframe, we will repeatedly invoke the interface methods for new rows</span>
<span class="sd">        in each trigger and the user&#39;s state/state variables will be stored persistently across</span>
<span class="sd">        invocations.</span>

<span class="sd">        The `statefulProcessor` should be a Python class that implements the interface defined in</span>
<span class="sd">        :class:`StatefulProcessor`.</span>

<span class="sd">        The `outputStructType` should be a :class:`StructType` describing the schema of all</span>
<span class="sd">        elements in the returned value, `pandas.DataFrame`. The column labels of all elements in</span>
<span class="sd">        returned `pandas.DataFrame` must either match the field names in the defined schema if</span>
<span class="sd">        specified as strings, or match the field data types by position if not strings,</span>
<span class="sd">        e.g. integer indices.</span>

<span class="sd">        The size of each `pandas.DataFrame` in both the input and output can be arbitrary. The</span>
<span class="sd">        number of `pandas.DataFrame` in both the input and output can also be arbitrary.</span>

<span class="sd">        .. versionadded:: 4.0.0</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        statefulProcessor : :class:`pyspark.sql.streaming.stateful_processor.StatefulProcessor`</span>
<span class="sd">            Instance of StatefulProcessor whose functions will be invoked by the operator.</span>
<span class="sd">        outputStructType : :class:`pyspark.sql.types.DataType` or str</span>
<span class="sd">            The type of the output records. The value can be either a</span>
<span class="sd">            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>
<span class="sd">        outputMode : str</span>
<span class="sd">            The output mode of the stateful processor.</span>
<span class="sd">        timeMode : str</span>
<span class="sd">            The time mode semantics of the stateful processor for timers and TTL.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from typing import Iterator</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; import pandas as pd # doctest: +SKIP</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql import Row</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.functions import col, split</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.streaming import StatefulProcessor, StatefulProcessorHandle</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.types import IntegerType, LongType, StringType, StructField, StructType</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; spark.conf.set(&quot;spark.sql.streaming.stateStore.providerClass&quot;,</span>
<span class="sd">        ...     &quot;org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider&quot;)</span>
<span class="sd">        ... # Below is a simple example to find erroneous sensors from temperature sensor data. The</span>
<span class="sd">        ... # processor returns a count of total readings, while keeping erroneous reading counts</span>
<span class="sd">        ... # in streaming state. A violation is defined when the temperature is above 100.</span>
<span class="sd">        ... # The input data is a DataFrame with the following schema:</span>
<span class="sd">        ... #    `id: string, temperature: long`.</span>
<span class="sd">        ... # The output schema and state schema are defined as below.</span>
<span class="sd">        &gt;&gt;&gt; output_schema = StructType([</span>
<span class="sd">        ...     StructField(&quot;id&quot;, StringType(), True),</span>
<span class="sd">        ...     StructField(&quot;count&quot;, IntegerType(), True)</span>
<span class="sd">        ... ])</span>
<span class="sd">        &gt;&gt;&gt; state_schema = StructType([</span>
<span class="sd">        ...     StructField(&quot;value&quot;, IntegerType(), True)</span>
<span class="sd">        ... ])</span>
<span class="sd">        &gt;&gt;&gt; class SimpleStatefulProcessor(StatefulProcessor):</span>
<span class="sd">        ...     def init(self, handle: StatefulProcessorHandle):</span>
<span class="sd">        ...         self.num_violations_state = handle.getValueState(&quot;numViolations&quot;, state_schema)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def handleInputRows(self, key, rows):</span>
<span class="sd">        ...         new_violations = 0</span>
<span class="sd">        ...         count = 0</span>
<span class="sd">        ...         exists = self.num_violations_state.exists()</span>
<span class="sd">        ...         if exists:</span>
<span class="sd">        ...             existing_violations_row = self.num_violations_state.get()</span>
<span class="sd">        ...             existing_violations = existing_violations_row[0]</span>
<span class="sd">        ...         else:</span>
<span class="sd">        ...             existing_violations = 0</span>
<span class="sd">        ...         for pdf in rows:</span>
<span class="sd">        ...             pdf_count = pdf.count()</span>
<span class="sd">        ...             count += pdf_count.get(&#39;temperature&#39;)</span>
<span class="sd">        ...             violations_pdf = pdf.loc[pdf[&#39;temperature&#39;] &gt; 100]</span>
<span class="sd">        ...             new_violations += violations_pdf.count().get(&#39;temperature&#39;)</span>
<span class="sd">        ...         updated_violations = new_violations + existing_violations</span>
<span class="sd">        ...         self.num_violations_state.update((updated_violations,))</span>
<span class="sd">        ...         yield pd.DataFrame({&#39;id&#39;: key, &#39;count&#39;: count})</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def close(self) -&gt; None:</span>
<span class="sd">        ...         pass</span>

<span class="sd">        Input DataFrame:</span>
<span class="sd">        +---+-----------+</span>
<span class="sd">        | id|temperature|</span>
<span class="sd">        +---+-----------+</span>
<span class="sd">        |  0|        123|</span>
<span class="sd">        |  0|         23|</span>
<span class="sd">        |  1|         33|</span>
<span class="sd">        |  1|        188|</span>
<span class="sd">        |  1|         88|</span>
<span class="sd">        +---+-----------+</span>

<span class="sd">        &gt;&gt;&gt; df.groupBy(&quot;value&quot;).transformWithStateInPandas(statefulProcessor =</span>
<span class="sd">        ...     SimpleStatefulProcessor(), outputStructType=output_schema, outputMode=&quot;Update&quot;,</span>
<span class="sd">        ...     timeMode=&quot;None&quot;) # doctest: +SKIP</span>

<span class="sd">        Output DataFrame:</span>
<span class="sd">        +---+-----+</span>
<span class="sd">        | id|count|</span>
<span class="sd">        +---+-----+</span>
<span class="sd">        |  0|    2|</span>
<span class="sd">        |  1|    3|</span>
<span class="sd">        +---+-----+</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function requires a full shuffle.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">GroupedData</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">GroupedData</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">transformWithStateUDF</span><span class="p">(</span>
            <span class="n">statefulProcessorApiClient</span><span class="p">:</span> <span class="n">StatefulProcessorApiClient</span><span class="p">,</span>
            <span class="n">key</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
            <span class="n">inputRows</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="s2">&quot;PandasDataFrameLike&quot;</span><span class="p">],</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="s2">&quot;PandasDataFrameLike&quot;</span><span class="p">]:</span>
            <span class="n">handle</span> <span class="o">=</span> <span class="n">StatefulProcessorHandle</span><span class="p">(</span><span class="n">statefulProcessorApiClient</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">statefulProcessorApiClient</span><span class="o">.</span><span class="n">handle_state</span> <span class="o">==</span> <span class="n">StatefulProcessorHandleState</span><span class="o">.</span><span class="n">CREATED</span><span class="p">:</span>
                <span class="n">statefulProcessor</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
                <span class="n">statefulProcessorApiClient</span><span class="o">.</span><span class="n">set_handle_state</span><span class="p">(</span>
                    <span class="n">StatefulProcessorHandleState</span><span class="o">.</span><span class="n">INITIALIZED</span>
                <span class="p">)</span>

            <span class="n">statefulProcessorApiClient</span><span class="o">.</span><span class="n">set_implicit_key</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">statefulProcessor</span><span class="o">.</span><span class="n">handleInputRows</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">inputRows</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">result</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputStructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">outputStructType</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">StructType</span><span class="p">,</span> <span class="n">_parse_datatype_string</span><span class="p">(</span><span class="n">outputStructType</span><span class="p">))</span>

        <span class="n">udf</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span>
            <span class="n">transformWithStateUDF</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">returnType</span><span class="o">=</span><span class="n">outputStructType</span><span class="p">,</span>
            <span class="n">functionType</span><span class="o">=</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_TRANSFORM_WITH_STATE_PANDAS_UDF</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span>
        <span class="n">udf_column</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">])</span>

        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jgd</span><span class="o">.</span><span class="n">transformWithStateInPandas</span><span class="p">(</span>
            <span class="n">udf_column</span><span class="o">.</span><span class="n">_jc</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="o">.</span><span class="n">parseDataType</span><span class="p">(</span><span class="n">outputStructType</span><span class="o">.</span><span class="n">json</span><span class="p">()),</span>
            <span class="n">outputMode</span><span class="p">,</span>
            <span class="n">timeMode</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">applyInArrow</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="s2">&quot;ArrowGroupedMapFunction&quot;</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DataFrame&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Maps each group of the current :class:`DataFrame` using an Arrow udf and returns the result</span>
<span class="sd">        as a `DataFrame`.</span>

<span class="sd">        The function should take a `pyarrow.Table` and return another</span>
<span class="sd">        `pyarrow.Table`. Alternatively, the user can pass a function that takes</span>
<span class="sd">        a tuple of `pyarrow.Scalar` grouping key(s) and a `pyarrow.Table`.</span>
<span class="sd">        For each group, all columns are passed together as a `pyarrow.Table`</span>
<span class="sd">        to the user-function and the returned `pyarrow.Table` are combined as a</span>
<span class="sd">        :class:`DataFrame`.</span>

<span class="sd">        The `schema` should be a :class:`StructType` describing the schema of the returned</span>
<span class="sd">        `pyarrow.Table`. The column labels of the returned `pyarrow.Table` must either match</span>
<span class="sd">        the field names in the defined schema if specified as strings, or match the</span>
<span class="sd">        field data types by position if not strings, e.g. integer indices.</span>
<span class="sd">        The length of the returned `pyarrow.Table` can be arbitrary.</span>

<span class="sd">        .. versionadded:: 4.0.0</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        func : function</span>
<span class="sd">            a Python native function that takes a `pyarrow.Table` and outputs a</span>
<span class="sd">            `pyarrow.Table`, or that takes one tuple (grouping keys) and a</span>
<span class="sd">            `pyarrow.Table` and outputs a `pyarrow.Table`.</span>
<span class="sd">        schema : :class:`pyspark.sql.types.DataType` or str</span>
<span class="sd">            the return type of the `func` in PySpark. The value can be either a</span>
<span class="sd">            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; from pyspark.sql.functions import ceil</span>
<span class="sd">        &gt;&gt;&gt; import pyarrow  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; import pyarrow.compute as pc  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">        ...     (&quot;id&quot;, &quot;v&quot;))  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; def normalize(table):</span>
<span class="sd">        ...     v = table.column(&quot;v&quot;)</span>
<span class="sd">        ...     norm = pc.divide(pc.subtract(v, pc.mean(v)), pc.stddev(v, ddof=1))</span>
<span class="sd">        ...     return table.set_column(1, &quot;v&quot;, norm)</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&quot;id&quot;).applyInArrow(</span>
<span class="sd">        ...     normalize, schema=&quot;id long, v double&quot;).show()  # doctest: +SKIP</span>
<span class="sd">        +---+-------------------+</span>
<span class="sd">        | id|                  v|</span>
<span class="sd">        +---+-------------------+</span>
<span class="sd">        |  1|-0.7071067811865475|</span>
<span class="sd">        |  1| 0.7071067811865475|</span>
<span class="sd">        |  2|-0.8320502943378437|</span>
<span class="sd">        |  2|-0.2773500981126146|</span>
<span class="sd">        |  2| 1.1094003924504583|</span>
<span class="sd">        +---+-------------------+</span>

<span class="sd">        Alternatively, the user can pass a function that takes two arguments.</span>
<span class="sd">        In this case, the grouping key(s) will be passed as the first argument and the data will</span>
<span class="sd">        be passed as the second argument. The grouping key(s) will be passed as a tuple of Arrow</span>
<span class="sd">        scalars types, e.g., `pyarrow.Int32Scalar` and `pyarrow.FloatScalar`. The data will still</span>
<span class="sd">        be passed in as a `pyarrow.Table` containing all columns from the original Spark DataFrame.</span>
<span class="sd">        This is useful when the user does not want to hardcode grouping key(s) in the function.</span>

<span class="sd">        &gt;&gt;&gt; df = spark.createDataFrame(</span>
<span class="sd">        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],</span>
<span class="sd">        ...     (&quot;id&quot;, &quot;v&quot;))  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; def mean_func(key, table):</span>
<span class="sd">        ...     # key is a tuple of one pyarrow.Int64Scalar, which is the value</span>
<span class="sd">        ...     # of &#39;id&#39; for the current group</span>
<span class="sd">        ...     mean = pc.mean(table.column(&quot;v&quot;))</span>
<span class="sd">        ...     return pyarrow.Table.from_pydict({&quot;id&quot;: [key[0].as_py()], &quot;v&quot;: [mean.as_py()]})</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(&#39;id&#39;).applyInArrow(</span>
<span class="sd">        ...     mean_func, schema=&quot;id long, v double&quot;)  # doctest: +SKIP</span>
<span class="sd">        +---+---+</span>
<span class="sd">        | id|  v|</span>
<span class="sd">        +---+---+</span>
<span class="sd">        |  1|1.5|</span>
<span class="sd">        |  2|6.0|</span>
<span class="sd">        +---+---+</span>

<span class="sd">        &gt;&gt;&gt; def sum_func(key, table):</span>
<span class="sd">        ...     # key is a tuple of two pyarrow.Int64Scalars, which is the values</span>
<span class="sd">        ...     # of &#39;id&#39; and &#39;ceil(df.v / 2)&#39; for the current group</span>
<span class="sd">        ...     sum = pc.sum(table.column(&quot;v&quot;))</span>
<span class="sd">        ...     return pyarrow.Table.from_pydict({</span>
<span class="sd">        ...         &quot;id&quot;: [key[0].as_py()],</span>
<span class="sd">        ...         &quot;ceil(v / 2)&quot;: [key[1].as_py()],</span>
<span class="sd">        ...         &quot;v&quot;: [sum.as_py()]</span>
<span class="sd">        ...     })</span>
<span class="sd">        &gt;&gt;&gt; df.groupby(df.id, ceil(df.v / 2)).applyInArrow(</span>
<span class="sd">        ...     sum_func, schema=&quot;id long, `ceil(v / 2)` long, v double&quot;).show()  # doctest: +SKIP</span>
<span class="sd">        +---+-----------+----+</span>
<span class="sd">        | id|ceil(v / 2)|   v|</span>
<span class="sd">        +---+-----------+----+</span>
<span class="sd">        |  2|          5|10.0|</span>
<span class="sd">        |  1|          1| 3.0|</span>
<span class="sd">        |  2|          3| 5.0|</span>
<span class="sd">        |  2|          2| 3.0|</span>
<span class="sd">        +---+-----------+----+</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function requires a full shuffle. All the data of a group will be loaded</span>
<span class="sd">        into memory, so the user should be aware of the potential OOM risk if data is skewed</span>
<span class="sd">        and certain groups are too large to fit in memory.</span>

<span class="sd">        This API is unstable, and for developers.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        pyspark.sql.functions.pandas_udf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">GroupedData</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">GroupedData</span><span class="p">)</span>

        <span class="c1"># The usage of the pandas_udf is internal so type checking is disabled.</span>
        <span class="n">udf</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">functionType</span><span class="o">=</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_GROUPED_MAP_ARROW_UDF</span>
        <span class="p">)</span>  <span class="c1"># type: ignore[call-overload]</span>
        <span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_df</span>
        <span class="n">udf_column</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">])</span>
        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_jgd</span><span class="o">.</span><span class="n">flatMapGroupsInArrow</span><span class="p">(</span><span class="n">udf_column</span><span class="o">.</span><span class="n">_jc</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">cogroup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="s2">&quot;GroupedData&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;PandasCogroupedOps&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Cogroups this group with another group so that we can run cogrouped operations.</span>

<span class="sd">        .. versionadded:: 3.0.0</span>

<span class="sd">        .. versionchanged:: 3.4.0</span>
<span class="sd">            Support Spark Connect.</span>

<span class="sd">        See :class:`PandasCogroupedOps` for the operations that can be run.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">GroupedData</span>

        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">GroupedData</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">PandasCogroupedOps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<div class="viewcode-block" id="PandasCogroupedOps"><a class="viewcode-back" href="../../../../reference/pyspark.sql/api/pyspark.sql.PandasCogroupedOps.html#pyspark.sql.PandasCogroupedOps">[docs]</a><span class="k">class</span> <span class="nc">PandasCogroupedOps</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A logical grouping of two :class:`GroupedData`,</span>
<span class="sd">    created by :func:`GroupedData.cogroup`.</span>

<span class="sd">    .. versionadded:: 3.0.0</span>

<span class="sd">    .. versionchanged:: 3.4.0</span>
<span class="sd">        Support Spark Connect.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gd1</span><span class="p">:</span> <span class="s2">&quot;GroupedData&quot;</span><span class="p">,</span> <span class="n">gd2</span><span class="p">:</span> <span class="s2">&quot;GroupedData&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gd1</span> <span class="o">=</span> <span class="n">gd1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gd2</span> <span class="o">=</span> <span class="n">gd2</span>

<div class="viewcode-block" id="PandasCogroupedOps.applyInPandas"><a class="viewcode-back" href="../../../../reference/pyspark.sql/api/pyspark.sql.PandasCogroupedOps.applyInPandas.html#pyspark.sql.PandasCogroupedOps.applyInPandas">[docs]</a>    <span class="k">def</span> <span class="nf">applyInPandas</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="s2">&quot;PandasCogroupedMapFunction&quot;</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies a function to each cogroup using pandas and returns the result</span>
<span class="sd">        as a `DataFrame`.</span>

<span class="sd">        The function should take two `pandas.DataFrame`\\s and return another</span>
<span class="sd">        `pandas.DataFrame`. Alternatively, the user can pass a function that takes</span>
<span class="sd">        a tuple of the grouping key(s) and the two `pandas.DataFrame`\\s.</span>
<span class="sd">        For each side of the cogroup, all columns are passed together as a</span>
<span class="sd">        `pandas.DataFrame` to the user-function and the returned `pandas.DataFrame` are combined as</span>
<span class="sd">        a :class:`DataFrame`.</span>

<span class="sd">        The `schema` should be a :class:`StructType` describing the schema of the returned</span>
<span class="sd">        `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match</span>
<span class="sd">        the field names in the defined schema if specified as strings, or match the</span>
<span class="sd">        field data types by position if not strings, e.g. integer indices.</span>
<span class="sd">        The length of the returned `pandas.DataFrame` can be arbitrary.</span>

<span class="sd">        .. versionadded:: 3.0.0</span>

<span class="sd">        .. versionchanged:: 3.4.0</span>
<span class="sd">            Support Spark Connect.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        func : function</span>
<span class="sd">            a Python native function that takes two `pandas.DataFrame`\\s, and</span>
<span class="sd">            outputs a `pandas.DataFrame`, or that takes one tuple (grouping keys) and two</span>
<span class="sd">            ``pandas.DataFrame``\\s, and outputs a ``pandas.DataFrame``.</span>
<span class="sd">        schema : :class:`pyspark.sql.types.DataType` or str</span>
<span class="sd">            the return type of the `func` in PySpark. The value can be either a</span>
<span class="sd">            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; df1 = spark.createDataFrame(</span>
<span class="sd">        ...     [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],</span>
<span class="sd">        ...     (&quot;time&quot;, &quot;id&quot;, &quot;v1&quot;))</span>
<span class="sd">        &gt;&gt;&gt; df2 = spark.createDataFrame(</span>
<span class="sd">        ...     [(20000101, 1, &quot;x&quot;), (20000101, 2, &quot;y&quot;)],</span>
<span class="sd">        ...     (&quot;time&quot;, &quot;id&quot;, &quot;v2&quot;))</span>
<span class="sd">        &gt;&gt;&gt; def asof_join(l, r):</span>
<span class="sd">        ...     return pd.merge_asof(l, r, on=&quot;time&quot;, by=&quot;id&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df1.groupby(&quot;id&quot;).cogroup(df2.groupby(&quot;id&quot;)).applyInPandas(</span>
<span class="sd">        ...     asof_join, schema=&quot;time int, id int, v1 double, v2 string&quot;</span>
<span class="sd">        ... ).show()  # doctest: +SKIP</span>
<span class="sd">        +--------+---+---+---+</span>
<span class="sd">        |    time| id| v1| v2|</span>
<span class="sd">        +--------+---+---+---+</span>
<span class="sd">        |20000101|  1|1.0|  x|</span>
<span class="sd">        |20000102|  1|3.0|  x|</span>
<span class="sd">        |20000101|  2|2.0|  y|</span>
<span class="sd">        |20000102|  2|4.0|  y|</span>
<span class="sd">        +--------+---+---+---+</span>

<span class="sd">        Alternatively, the user can define a function that takes three arguments.  In this case,</span>
<span class="sd">        the grouping key(s) will be passed as the first argument and the data will be passed as the</span>
<span class="sd">        second and third arguments.  The grouping key(s) will be passed as a tuple of numpy data</span>
<span class="sd">        types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in as two</span>
<span class="sd">        `pandas.DataFrame` containing all columns from the original Spark DataFrames.</span>

<span class="sd">        &gt;&gt;&gt; def asof_join(k, l, r):</span>
<span class="sd">        ...     if k == (1,):</span>
<span class="sd">        ...         return pd.merge_asof(l, r, on=&quot;time&quot;, by=&quot;id&quot;)</span>
<span class="sd">        ...     else:</span>
<span class="sd">        ...         return pd.DataFrame(columns=[&#39;time&#39;, &#39;id&#39;, &#39;v1&#39;, &#39;v2&#39;])</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; df1.groupby(&quot;id&quot;).cogroup(df2.groupby(&quot;id&quot;)).applyInPandas(</span>
<span class="sd">        ...     asof_join, &quot;time int, id int, v1 double, v2 string&quot;).show()  # doctest: +SKIP</span>
<span class="sd">        +--------+---+---+---+</span>
<span class="sd">        |    time| id| v1| v2|</span>
<span class="sd">        +--------+---+---+---+</span>
<span class="sd">        |20000101|  1|1.0|  x|</span>
<span class="sd">        |20000102|  1|3.0|  x|</span>
<span class="sd">        +--------+---+---+---+</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function requires a full shuffle. All the data of a cogroup will be loaded</span>
<span class="sd">        into memory, so the user should be aware of the potential OOM risk if data is skewed</span>
<span class="sd">        and certain groups are too large to fit in memory.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        pyspark.sql.functions.pandas_udf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span>

        <span class="c1"># The usage of the pandas_udf is internal so type checking is disabled.</span>
        <span class="n">udf</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">functionType</span><span class="o">=</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_COGROUPED_MAP_PANDAS_UDF</span>
        <span class="p">)</span>  <span class="c1"># type: ignore[call-overload]</span>

        <span class="n">all_cols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_cols</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gd1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_cols</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gd2</span><span class="p">)</span>
        <span class="n">udf_column</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="o">*</span><span class="n">all_cols</span><span class="p">)</span>
        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gd1</span><span class="o">.</span><span class="n">_jgd</span><span class="o">.</span><span class="n">flatMapCoGroupsInPandas</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gd2</span><span class="o">.</span><span class="n">_jgd</span><span class="p">,</span> <span class="n">udf_column</span><span class="o">.</span><span class="n">_jc</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gd1</span><span class="o">.</span><span class="n">session</span><span class="p">)</span></div>

<div class="viewcode-block" id="PandasCogroupedOps.applyInArrow"><a class="viewcode-back" href="../../../../reference/pyspark.sql/api/pyspark.sql.PandasCogroupedOps.applyInArrow.html#pyspark.sql.PandasCogroupedOps.applyInArrow">[docs]</a>    <span class="k">def</span> <span class="nf">applyInArrow</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">:</span> <span class="s2">&quot;ArrowCogroupedMapFunction&quot;</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">StructType</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DataFrame&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Applies a function to each cogroup using Arrow and returns the result</span>
<span class="sd">        as a `DataFrame`.</span>

<span class="sd">        The function should take two `pyarrow.Table`\\s and return another</span>
<span class="sd">        `pyarrow.Table`. Alternatively, the user can pass a function that takes</span>
<span class="sd">        a tuple of `pyarrow.Scalar` grouping key(s) and the two `pyarrow.Table`\\s.</span>
<span class="sd">        For each side of the cogroup, all columns are passed together as a</span>
<span class="sd">        `pyarrow.Table` to the user-function and the returned `pyarrow.Table` are combined as</span>
<span class="sd">        a :class:`DataFrame`.</span>

<span class="sd">        The `schema` should be a :class:`StructType` describing the schema of the returned</span>
<span class="sd">        `pyarrow.Table`. The column labels of the returned `pyarrow.Table` must either match</span>
<span class="sd">        the field names in the defined schema if specified as strings, or match the</span>
<span class="sd">        field data types by position if not strings, e.g. integer indices.</span>
<span class="sd">        The length of the returned `pyarrow.Table` can be arbitrary.</span>

<span class="sd">        .. versionadded:: 4.0.0</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        func : function</span>
<span class="sd">            a Python native function that takes two `pyarrow.Table`\\s, and</span>
<span class="sd">            outputs a `pyarrow.Table`, or that takes one tuple (grouping keys) and two</span>
<span class="sd">            ``pyarrow.Table``\\s, and outputs a ``pyarrow.Table``.</span>
<span class="sd">        schema : :class:`pyspark.sql.types.DataType` or str</span>
<span class="sd">            the return type of the `func` in PySpark. The value can be either a</span>
<span class="sd">            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; import pyarrow  # doctest: +SKIP</span>
<span class="sd">        &gt;&gt;&gt; df1 = spark.createDataFrame([(1, 1.0), (2, 2.0), (1, 3.0), (2, 4.0)], (&quot;id&quot;, &quot;v1&quot;))</span>
<span class="sd">        &gt;&gt;&gt; df2 = spark.createDataFrame([(1, &quot;x&quot;), (2, &quot;y&quot;)], (&quot;id&quot;, &quot;v2&quot;))</span>
<span class="sd">        &gt;&gt;&gt; def summarize(l, r):</span>
<span class="sd">        ...     return pyarrow.Table.from_pydict({</span>
<span class="sd">        ...         &quot;left&quot;: [l.num_rows],</span>
<span class="sd">        ...         &quot;right&quot;: [r.num_rows]</span>
<span class="sd">        ...     })</span>
<span class="sd">        &gt;&gt;&gt; df1.groupby(&quot;id&quot;).cogroup(df2.groupby(&quot;id&quot;)).applyInArrow(</span>
<span class="sd">        ...     summarize, schema=&quot;left long, right long&quot;</span>
<span class="sd">        ... ).show()  # doctest: +SKIP</span>
<span class="sd">        +----+-----+</span>
<span class="sd">        |left|right|</span>
<span class="sd">        +----+-----+</span>
<span class="sd">        |   2|    1|</span>
<span class="sd">        |   2|    1|</span>
<span class="sd">        +----+-----+</span>

<span class="sd">        Alternatively, the user can define a function that takes three arguments.  In this case,</span>
<span class="sd">        the grouping key(s) will be passed as the first argument and the data will be passed as the</span>
<span class="sd">        second and third arguments.  The grouping key(s) will be passed as a tuple of Arrow scalars</span>
<span class="sd">        types, e.g., `pyarrow.Int32Scalar` and `pyarrow.FloatScalar`. The data will still be passed</span>
<span class="sd">        in as two `pyarrow.Table`\\s containing all columns from the original Spark DataFrames.</span>

<span class="sd">        &gt;&gt;&gt; def summarize(key, l, r):</span>
<span class="sd">        ...     return pyarrow.Table.from_pydict({</span>
<span class="sd">        ...         &quot;key&quot;: [key[0].as_py()],</span>
<span class="sd">        ...         &quot;left&quot;: [l.num_rows],</span>
<span class="sd">        ...         &quot;right&quot;: [r.num_rows]</span>
<span class="sd">        ...     })</span>
<span class="sd">        &gt;&gt;&gt; df1.groupby(&quot;id&quot;).cogroup(df2.groupby(&quot;id&quot;)).applyInArrow(</span>
<span class="sd">        ...     summarize, schema=&quot;key long, left long, right long&quot;</span>
<span class="sd">        ... ).show()  # doctest: +SKIP</span>
<span class="sd">        +---+----+-----+</span>
<span class="sd">        |key|left|right|</span>
<span class="sd">        +---+----+-----+</span>
<span class="sd">        |  1|   2|    1|</span>
<span class="sd">        |  2|   2|    1|</span>
<span class="sd">        +---+----+-----+</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function requires a full shuffle. All the data of a cogroup will be loaded</span>
<span class="sd">        into memory, so the user should be aware of the potential OOM risk if data is skewed</span>
<span class="sd">        and certain groups are too large to fit in memory.</span>

<span class="sd">        This API is unstable, and for developers.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        pyspark.sql.functions.pandas_udf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql.pandas.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span>

        <span class="c1"># The usage of the pandas_udf is internal so type checking is disabled.</span>
        <span class="n">udf</span> <span class="o">=</span> <span class="n">pandas_udf</span><span class="p">(</span>
            <span class="n">func</span><span class="p">,</span> <span class="n">returnType</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span> <span class="n">functionType</span><span class="o">=</span><span class="n">PythonEvalType</span><span class="o">.</span><span class="n">SQL_COGROUPED_MAP_ARROW_UDF</span>
        <span class="p">)</span>  <span class="c1"># type: ignore[call-overload]</span>

        <span class="n">all_cols</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_cols</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gd1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_cols</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gd2</span><span class="p">)</span>
        <span class="n">udf_column</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="o">*</span><span class="n">all_cols</span><span class="p">)</span>
        <span class="n">jdf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gd1</span><span class="o">.</span><span class="n">_jgd</span><span class="o">.</span><span class="n">flatMapCoGroupsInArrow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gd2</span><span class="o">.</span><span class="n">_jgd</span><span class="p">,</span> <span class="n">udf_column</span><span class="o">.</span><span class="n">_jc</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">jdf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gd1</span><span class="o">.</span><span class="n">session</span><span class="p">)</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_extract_cols</span><span class="p">(</span><span class="n">gd</span><span class="p">:</span> <span class="s2">&quot;GroupedData&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Column</span><span class="p">]:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">gd</span><span class="o">.</span><span class="n">_df</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span></div>


<span class="k">def</span> <span class="nf">_test</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">doctest</span>
    <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
    <span class="kn">import</span> <span class="nn">pyspark.sql.pandas.group_ops</span>

    <span class="n">globs</span> <span class="o">=</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">pandas</span><span class="o">.</span><span class="n">group_ops</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">master</span><span class="p">(</span><span class="s2">&quot;local[4]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;sql.pandas.group tests&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="n">globs</span><span class="p">[</span><span class="s2">&quot;spark&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spark</span>
    <span class="p">(</span><span class="n">failure_count</span><span class="p">,</span> <span class="n">test_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">doctest</span><span class="o">.</span><span class="n">testmod</span><span class="p">(</span>
        <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">pandas</span><span class="o">.</span><span class="n">group_ops</span><span class="p">,</span>
        <span class="n">globs</span><span class="o">=</span><span class="n">globs</span><span class="p">,</span>
        <span class="n">optionflags</span><span class="o">=</span><span class="n">doctest</span><span class="o">.</span><span class="n">ELLIPSIS</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">NORMALIZE_WHITESPACE</span> <span class="o">|</span> <span class="n">doctest</span><span class="o">.</span><span class="n">REPORT_NDIFF</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">failure_count</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">_test</span><span class="p">()</span>
</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item"><p class="copyright">
    Copyright @ 2024 The Apache Software Foundation, Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License, Version 2.0</a>.
</p></div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>