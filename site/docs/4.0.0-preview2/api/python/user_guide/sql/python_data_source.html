

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Python Data Source API &#8212; PySpark 4.0.0-preview2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/pyspark.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'user_guide/sql/python_data_source';</script>
    <link rel="canonical" href="https://spark.apache.org/docs/latest/api/python/user_guide/sql/python_data_source.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Python to Spark Type Conversions" href="type_conversions.html" />
    <link rel="prev" title="Python User-defined Table Functions (UDTFs)" href="python_udtf.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Matomo -->
    <script type="text/javascript">
        var _paq = window._paq = window._paq || [];
        /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
        _paq.push(["disableCookies"]);
        _paq.push(['trackPageView']);
        _paq.push(['enableLinkTracking']);
        (function() {
            var u="https://analytics.apache.org/";
            _paq.push(['setTrackerUrl', u+'matomo.php']);
            _paq.push(['setSiteId', '40']);
            var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
            g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
        })();
    </script>
    <!-- End Matomo Code -->

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
    
    
      
    
    
    <img src="https://spark.apache.org/images/spark-logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="https://spark.apache.org/images/spark-logo-rev.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../index.html">
                        Overview
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../getting_started/index.html">
                        Getting Started
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../index.html">
                        User Guides
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../reference/index.html">
                        API Reference
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../development/index.html">
                        Development
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../migration_guide/index.html">
                        Migration Guides
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item"><!--
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<div id="version-button" class="dropdown">
    <button type="button" class="btn btn-secondary btn-sm navbar-btn dropdown-toggle" id="version_switcher_button" data-toggle="dropdown">
        4.0.0-preview2
        <span class="caret"></span>
    </button>
    <div id="version_switcher" class="dropdown-menu list-group-flush py-0" aria-labelledby="version_switcher_button">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
</div>

<script type="text/javascript">
// Function to construct the target URL from the JSON components
function buildURL(entry) {
    var template = "https://spark.apache.org/docs/{version}/api/python/index.html";  // supplied by jinja
    template = template.replace("{version}", entry.version);
    return template;
}

// Function to check if corresponding page path exists in other version of docs
// and, if so, go there instead of the homepage of the other docs version
function checkPageExistsAndRedirect(event) {
    const currentFilePath = "user_guide/sql/python_data_source.html",
          otherDocsHomepage = event.target.getAttribute("href");
    let tryUrl = `${otherDocsHomepage}${currentFilePath}`;
    $.ajax({
        type: 'HEAD',
        url: tryUrl,
        // if the page exists, go there
        success: function() {
            location.href = tryUrl;
        }
    }).fail(function() {
        location.href = otherDocsHomepage;
    });
    return false;
}

// Function to populate the version switcher
(function () {
    // get JSON config
    $.getJSON("https://spark.apache.org/static/versions.json", function(data, textStatus, jqXHR) {
        // create the nodes first (before AJAX calls) to ensure the order is
        // correct (for now, links will go to doc version homepage)
        $.each(data, function(index, entry) {
            // if no custom name specified (e.g., "latest"), use version string
            if (!("name" in entry)) {
                entry.name = entry.version;
            }
            // construct the appropriate URL, and add it to the dropdown
            entry.url = buildURL(entry);
            const node = document.createElement("a");
            node.setAttribute("class", "list-group-item list-group-item-action py-1");
            node.setAttribute("href", `${entry.url}`);
            node.textContent = `${entry.name}`;
            node.onclick = checkPageExistsAndRedirect;
            $("#version_switcher").append(node);
        });
    });
})();
</script></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/apache/spark" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/pyspark" title="PyPI" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-solid fa-box"></i></span>
            <label class="sr-only">PyPI</label></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../index.html">
                        Overview
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../getting_started/index.html">
                        Getting Started
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="../index.html">
                        User Guides
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../reference/index.html">
                        API Reference
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../development/index.html">
                        Development
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../migration_guide/index.html">
                        Migration Guides
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item"><!--
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to You under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<div id="version-button" class="dropdown">
    <button type="button" class="btn btn-secondary btn-sm navbar-btn dropdown-toggle" id="version_switcher_button" data-toggle="dropdown">
        4.0.0-preview2
        <span class="caret"></span>
    </button>
    <div id="version_switcher" class="dropdown-menu list-group-flush py-0" aria-labelledby="version_switcher_button">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
</div>

<script type="text/javascript">
// Function to construct the target URL from the JSON components
function buildURL(entry) {
    var template = "https://spark.apache.org/docs/{version}/api/python/index.html";  // supplied by jinja
    template = template.replace("{version}", entry.version);
    return template;
}

// Function to check if corresponding page path exists in other version of docs
// and, if so, go there instead of the homepage of the other docs version
function checkPageExistsAndRedirect(event) {
    const currentFilePath = "user_guide/sql/python_data_source.html",
          otherDocsHomepage = event.target.getAttribute("href");
    let tryUrl = `${otherDocsHomepage}${currentFilePath}`;
    $.ajax({
        type: 'HEAD',
        url: tryUrl,
        // if the page exists, go there
        success: function() {
            location.href = tryUrl;
        }
    }).fail(function() {
        location.href = otherDocsHomepage;
    });
    return false;
}

// Function to populate the version switcher
(function () {
    // get JSON config
    $.getJSON("https://spark.apache.org/static/versions.json", function(data, textStatus, jqXHR) {
        // create the nodes first (before AJAX calls) to ensure the order is
        // correct (for now, links will go to doc version homepage)
        $.each(data, function(index, entry) {
            // if no custom name specified (e.g., "latest"), use version string
            if (!("name" in entry)) {
                entry.name = entry.version;
            }
            // construct the appropriate URL, and add it to the dropdown
            entry.url = buildURL(entry);
            const node = document.createElement("a");
            node.setAttribute("class", "list-group-item list-group-item-action py-1");
            node.setAttribute("href", `${entry.url}`);
            node.textContent = `${entry.name}`;
            node.onclick = checkPageExistsAndRedirect;
            $("#version_switcher").append(node);
        });
    });
})();
</script></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/apache/spark" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/pyspark" title="PyPI" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-solid fa-box"></i></span>
            <label class="sr-only">PyPI</label></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../python_packaging.html">Python Package Management</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Spark SQL</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="arrow_pandas.html">Apache Arrow in PySpark</a></li>
<li class="toctree-l2"><a class="reference internal" href="python_udtf.html">Python User-defined Table Functions (UDTFs)</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Python Data Source API</a></li>
<li class="toctree-l2"><a class="reference internal" href="type_conversions.html">Python to Spark Type Conversions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pandas_on_spark/index.html">Pandas API on Spark</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../pandas_on_spark/options.html">Options and settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pandas_on_spark/pandas_pyspark.html">From/to pandas and PySpark DataFrames</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pandas_on_spark/transform_apply.html">Transform and apply a function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pandas_on_spark/types.html">Type Support in Pandas API on Spark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pandas_on_spark/typehints.html">Type Hints in Pandas API on Spark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pandas_on_spark/from_to_dbms.html">From/to other DBMSes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pandas_on_spark/best_practices.html">Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pandas_on_spark/supported_pandas_api.html">Supported pandas API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pandas_on_spark/faq.html">FAQ</a></li>
</ul>
</li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">User Guides</a></li>
    
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Spark SQL</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Python Data Source API</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="python-data-source-api">
<h1>Python Data Source API<a class="headerlink" href="#python-data-source-api" title="Permalink to this headline">#</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">#</a></h2>
<p>The Python Data Source API is a new feature introduced in Spark 4.0, enabling developers to read from custom data sources and write to custom data sinks in Python.
This guide provides a comprehensive overview of the API and instructions on how to create, use, and manage Python data sources.</p>
</section>
<section id="simple-example">
<h2>Simple Example<a class="headerlink" href="#simple-example" title="Permalink to this headline">#</a></h2>
<p>Here’s a simple Python data source that generates exactly two rows of synthetic data.
This example demonstrates how to set up a custom data source without using external libraries, focusing on the essentials needed to get it up and running quickly.</p>
<p><strong>Step 1: Define the data source</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.datasource</span> <span class="kn">import</span> <span class="n">DataSource</span><span class="p">,</span> <span class="n">DataSourceReader</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="n">StringType</span><span class="p">,</span> <span class="n">StructField</span><span class="p">,</span> <span class="n">StructType</span>

<span class="k">class</span> <span class="nc">SimpleDataSource</span><span class="p">(</span><span class="n">DataSource</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A simple data source for PySpark that generates exactly two rows of synthetic data.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;simple&quot;</span>

    <span class="k">def</span> <span class="nf">schema</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">StructType</span><span class="p">([</span>
            <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">()),</span>
            <span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">())</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">reader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">StructType</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">SimpleDataSourceReader</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">SimpleDataSourceReader</span><span class="p">(</span><span class="n">DataSourceReader</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="s2">&quot;Alice&quot;</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="k">yield</span> <span class="p">(</span><span class="s2">&quot;Bob&quot;</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Step 2: Register the data source</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">spark</span><span class="o">.</span><span class="n">dataSource</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">SimpleDataSource</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Step 3: Read from the data source</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;simple&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># +-----+---+</span>
<span class="c1"># | name|age|</span>
<span class="c1"># +-----+---+</span>
<span class="c1"># |Alice| 20|</span>
<span class="c1"># |  Bob| 30|</span>
<span class="c1"># +-----+---+</span>
</pre></div>
</div>
</section>
<section id="creating-a-python-data-source">
<h2>Creating a Python Data Source<a class="headerlink" href="#creating-a-python-data-source" title="Permalink to this headline">#</a></h2>
<p>To create a custom Python data source, you’ll need to subclass the <code class="xref py py-class docutils literal notranslate"><span class="pre">DataSource</span></code> base classes and implement the necessary methods for reading and writing data.</p>
<p>This example demonstrates creating a simple data source to generate synthetic data using the <cite>faker</cite> library. Ensure the <cite>faker</cite> library is installed and accessible in your Python environment.</p>
<p><strong>Define the Data Source</strong></p>
<p>Start by creating a new subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">DataSource</span></code> with the source name, schema.</p>
<p>In order to be used as source or sink in batch or streaming query, corresponding method of DataSource needs to be implemented.</p>
<p>Method that needs to be implemented for a capability:</p>
<table class="table">
<colgroup>
<col style="width: 23%" />
<col style="width: 42%" />
<col style="width: 35%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>source</p></th>
<th class="head"><p>sink</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>batch</p></td>
<td><p>reader()</p></td>
<td><p>writer()</p></td>
</tr>
<tr class="row-odd"><td><p>streaming</p></td>
<td><p>streamReader()
or
simpleStreamReader()</p></td>
<td><p>streamWriter()</p></td>
</tr>
</tbody>
</table>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.datasource</span> <span class="kn">import</span> <span class="n">DataSource</span><span class="p">,</span> <span class="n">DataSourceReader</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span>

<span class="k">class</span> <span class="nc">FakeDataSource</span><span class="p">(</span><span class="n">DataSource</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A fake data source for PySpark to generate synthetic data using the `faker` library.</span>
<span class="sd">    Options:</span>
<span class="sd">    - numRows: specify number of rows to generate. Default value is 3.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;fake&quot;</span>

    <span class="k">def</span> <span class="nf">schema</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;name string, date string, zipcode string, state string&quot;</span>

    <span class="k">def</span> <span class="nf">reader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">StructType</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">FakeDataSourceReader</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">writer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">overwrite</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">FakeDataSourceWriter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">streamReader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">StructType</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">FakeStreamReader</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">)</span>

    <span class="c1"># Please skip the implementation of this method if streamReader has been implemented.</span>
    <span class="k">def</span> <span class="nf">simpleStreamReader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">StructType</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">SimpleStreamReader</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">streamWriter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">overwrite</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">FakeStreamWriter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="implementing-batch-reader-and-writer-for-python-data-source">
<h2>Implementing Batch Reader and Writer for Python Data Source<a class="headerlink" href="#implementing-batch-reader-and-writer-for-python-data-source" title="Permalink to this headline">#</a></h2>
<p><strong>Implement the Reader</strong></p>
<p>Define the reader logic to generate synthetic data. Use the <cite>faker</cite> library to populate each field in the schema.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FakeDataSourceReader</span><span class="p">(</span><span class="n">DataSourceReader</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span> <span class="n">StructType</span> <span class="o">=</span> <span class="n">schema</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">options</span> <span class="o">=</span> <span class="n">options</span>

    <span class="k">def</span> <span class="nf">read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">faker</span> <span class="kn">import</span> <span class="n">Faker</span>
        <span class="n">fake</span> <span class="o">=</span> <span class="n">Faker</span><span class="p">()</span>
        <span class="c1"># Note: every value in this `self.options` dictionary is a string.</span>
        <span class="n">num_rows</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;numRows&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_rows</span><span class="p">):</span>
            <span class="n">row</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="o">.</span><span class="n">fields</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">fake</span><span class="p">,</span> <span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">)()</span>
                <span class="n">row</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="k">yield</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Implement the Writer</strong></p>
<p>Create a fake data source writer that processes each partition of data, counts the rows, and either
prints the total count of rows after a successful write or the number of failed tasks if the writing process fails.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Iterator</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.datasource</span> <span class="kn">import</span> <span class="n">DataSource</span><span class="p">,</span> <span class="n">DataSourceWriter</span><span class="p">,</span> <span class="n">WriterCommitMessage</span>

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SimpleCommitMessage</span><span class="p">(</span><span class="n">WriterCommitMessage</span><span class="p">):</span>
    <span class="n">partition_id</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">count</span><span class="p">:</span> <span class="nb">int</span>

<span class="k">class</span> <span class="nc">FakeDataSourceWriter</span><span class="p">(</span><span class="n">DataSourceWriter</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rows</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Row</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">SimpleCommitMessage</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">TaskContext</span>

        <span class="n">context</span> <span class="o">=</span> <span class="n">TaskContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="n">partition_id</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">()</span>
        <span class="n">cnt</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">SimpleCommitMessage</span><span class="p">(</span><span class="n">partition_id</span><span class="o">=</span><span class="n">partition_id</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="n">cnt</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">commit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">SimpleCommitMessage</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">total_count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">count</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total number of rows: </span><span class="si">{</span><span class="n">total_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">abort</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">SimpleCommitMessage</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">failed_count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">message</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of failed tasks: </span><span class="si">{</span><span class="n">failed_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="implementing-streaming-reader-and-writer-for-python-data-source">
<h2>Implementing Streaming Reader and Writer for Python Data Source<a class="headerlink" href="#implementing-streaming-reader-and-writer-for-python-data-source" title="Permalink to this headline">#</a></h2>
<p><strong>Implement the Stream Reader</strong></p>
<p>This is a dummy streaming data reader that generate 2 rows in every microbatch. The streamReader instance has a integer offset that increase by 2 in every microbatch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RangePartition</span><span class="p">(</span><span class="n">InputPartition</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start</span> <span class="o">=</span> <span class="n">start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end</span> <span class="o">=</span> <span class="n">end</span>

<span class="k">class</span> <span class="nc">FakeStreamReader</span><span class="p">(</span><span class="n">DataSourceStreamReader</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">initialOffset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the initial start offset of the reader.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;offset&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">latestOffset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the current latest offset that the next microbatch will read to.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current</span> <span class="o">+=</span> <span class="mi">2</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;offset&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">current</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">partitions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Plans the partitioning of the current microbatch defined by start and end offset,</span>
<span class="sd">        it needs to return a sequence of :class:`InputPartition` object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">RangePartition</span><span class="p">(</span><span class="n">start</span><span class="p">[</span><span class="s2">&quot;offset&quot;</span><span class="p">],</span> <span class="n">end</span><span class="p">[</span><span class="s2">&quot;offset&quot;</span><span class="p">])]</span>

    <span class="k">def</span> <span class="nf">commit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This is invoked when the query has finished processing data before end offset, this can be used to clean up resource.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Takes a partition as an input and read an iterator of tuples from the data source.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="n">partition</span><span class="o">.</span><span class="n">start</span><span class="p">,</span> <span class="n">partition</span><span class="o">.</span><span class="n">end</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Implement the Simple Stream Reader</strong></p>
<p>If the data source has low throughput and doesn’t require partitioning, you can implement SimpleDataSourceStreamReader instead of DataSourceStreamReader.</p>
<p>One of simpleStreamReader() and streamReader() must be implemented for readable streaming data source. And simpleStreamReader() will only be invoked when streamReader() is not implemented.</p>
<p>This is the same dummy streaming reader that generate 2 rows every batch implemented with SimpleDataSourceStreamReader interface.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleStreamReader</span><span class="p">(</span><span class="n">SimpleDataSourceStreamReader</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">initialOffset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the initial start offset of the reader.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;offset&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Takes start offset as an input, return an iterator of tuples and the start offset of next read.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">start</span><span class="p">[</span><span class="s2">&quot;offset&quot;</span><span class="p">]</span>
        <span class="n">it</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">([(</span><span class="n">i</span><span class="p">,)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_idx</span><span class="p">,</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)])</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;offset&quot;</span><span class="p">:</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">readBetweenOffsets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">end</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Takes start and end offset as input and read an iterator of data deterministically.</span>
<span class="sd">        This is called whe query replay batches during restart or after failure.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">start</span><span class="p">[</span><span class="s2">&quot;offset&quot;</span><span class="p">]</span>
        <span class="n">end_idx</span> <span class="o">=</span> <span class="n">end</span><span class="p">[</span><span class="s2">&quot;offset&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">([(</span><span class="n">i</span><span class="p">,)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_idx</span><span class="p">,</span> <span class="n">end_idx</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">commit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This is invoked when the query has finished processing data before end offset, this can be used to clean up resource.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>
</pre></div>
</div>
<p><strong>Implement the Stream Writer</strong></p>
<p>This is a streaming data writer that write the metadata information of each microbatch to a local path.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleCommitMessage</span><span class="p">(</span><span class="n">WriterCommitMessage</span><span class="p">):</span>
   <span class="n">partition_id</span><span class="p">:</span> <span class="nb">int</span>
   <span class="n">count</span><span class="p">:</span> <span class="nb">int</span>

<span class="k">class</span> <span class="nc">FakeStreamWriter</span><span class="p">(</span><span class="n">DataSourceStreamWriter</span><span class="p">):</span>
   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">options</span> <span class="o">=</span> <span class="n">options</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;path&quot;</span><span class="p">)</span>
       <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

   <span class="k">def</span> <span class="nf">write</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iterator</span><span class="p">):</span>
<span class="w">       </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">       Write the data and return the commit message of that partition</span>
<span class="sd">       &quot;&quot;&quot;</span>
       <span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">TaskContext</span>
       <span class="n">context</span> <span class="o">=</span> <span class="n">TaskContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
       <span class="n">partition_id</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">partitionId</span><span class="p">()</span>
       <span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
       <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>
           <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
       <span class="k">return</span> <span class="n">SimpleCommitMessage</span><span class="p">(</span><span class="n">partition_id</span><span class="o">=</span><span class="n">partition_id</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="n">cnt</span><span class="p">)</span>

   <span class="k">def</span> <span class="nf">commit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">,</span> <span class="n">batchId</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">       </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">       Receives a sequence of :class:`WriterCommitMessage` when all write tasks succeed and decides what to do with it.</span>
<span class="sd">       In this FakeStreamWriter, we write the metadata of the microbatch(number of rows and partitions) into a json file inside commit().</span>
<span class="sd">       &quot;&quot;&quot;</span>
       <span class="n">status</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">num_partitions</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">messages</span><span class="p">),</span> <span class="n">rows</span><span class="o">=</span><span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">count</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">))</span>
       <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">batchId</span><span class="si">}</span><span class="s2">.json&quot;</span><span class="p">),</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
           <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">status</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

   <span class="k">def</span> <span class="nf">abort</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">,</span> <span class="n">batchId</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">       </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">       Receives a sequence of :class:`WriterCommitMessage` from successful tasks when some tasks fail and decides what to do with it.</span>
<span class="sd">       In this FakeStreamWriter, we write a failure message into a txt file inside abort().</span>
<span class="sd">       &quot;&quot;&quot;</span>
       <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">batchId</span><span class="si">}</span><span class="s2">.txt&quot;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
           <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;failed in batch </span><span class="si">{</span><span class="n">batchId</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="serialization-requirement">
<h2>Serialization Requirement<a class="headerlink" href="#serialization-requirement" title="Permalink to this headline">#</a></h2>
<p>User defined DataSource, DataSourceReader, DataSourceWriter, DataSourceStreamReader and DataSourceStreamWriter and their methods must be able to be serialized by pickle.</p>
<p>For library that are used inside a method, it must be imported inside the method. For example, TaskContext must be imported inside the read() method in the code below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">TaskContext</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">TaskContext</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="using-a-python-data-source">
<h2>Using a Python Data Source<a class="headerlink" href="#using-a-python-data-source" title="Permalink to this headline">#</a></h2>
<p><strong>Use a Python Data Source in Batch Query</strong></p>
<p>After defining your data source, it must be registered before usage.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">dataSource</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">FakeDataSource</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Read From a Python Data Source</strong></p>
<p>Read from the fake datasource with the default schema and options:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;fake&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># +-----------+----------+-------+-------+</span>
<span class="c1"># |       name|      date|zipcode|  state|</span>
<span class="c1"># +-----------+----------+-------+-------+</span>
<span class="c1"># |Carlos Cobb|2018-07-15|  73003|Indiana|</span>
<span class="c1"># | Eric Scott|1991-08-22|  10085|  Idaho|</span>
<span class="c1"># | Amy Martin|1988-10-28|  68076| Oregon|</span>
<span class="c1"># +-----------+----------+-------+-------+</span>
</pre></div>
</div>
<p>Read from the fake datasource with a custom schema:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;fake&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">schema</span><span class="p">(</span><span class="s2">&quot;name string, company string&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># +---------------------+--------------+</span>
<span class="c1"># |name                 |company       |</span>
<span class="c1"># +---------------------+--------------+</span>
<span class="c1"># |Tanner Brennan       |Adams Group   |</span>
<span class="c1"># |Leslie Maxwell       |Santiago Group|</span>
<span class="c1"># |Mrs. Jacqueline Brown|Maynard Inc   |</span>
<span class="c1"># +---------------------+--------------+</span>
</pre></div>
</div>
<p>Read from the fake datasource with a different number of rows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;fake&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;numRows&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># +--------------+----------+-------+------------+</span>
<span class="c1"># |          name|      date|zipcode|       state|</span>
<span class="c1"># +--------------+----------+-------+------------+</span>
<span class="c1"># |  Pam Mitchell|1988-10-20|  23788|   Tennessee|</span>
<span class="c1"># |Melissa Turner|1996-06-14|  30851|      Nevada|</span>
<span class="c1"># |  Brian Ramsey|2021-08-21|  55277|  Washington|</span>
<span class="c1"># |  Caitlin Reed|1983-06-22|  89813|Pennsylvania|</span>
<span class="c1"># | Douglas James|2007-01-18|  46226|     Alabama|</span>
<span class="c1"># +--------------+----------+-------+------------+</span>
</pre></div>
</div>
<p><strong>Write To a Python Data Source</strong></p>
<p>To write data to a custom location, make sure that you specify the <cite>mode()</cite> clause. Supported modes are <cite>append</cite> and <cite>overwrite</cite>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;fake&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="s2">&quot;append&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1"># You can check the Spark log (standard error) to see the output of the write operation.</span>
<span class="c1"># Total number of rows: 10</span>
</pre></div>
</div>
<p><strong>Use a Python Data Source in Streaming Query</strong></p>
<p>Once we register the python data source, we can also use it in streaming queries as source of readStream() or sink of writeStream() by passing short name or full name to format().</p>
<p>Start a query that read from fake python data source and write to console</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;fake&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;console&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="c1"># +---+</span>
<span class="c1"># | id|</span>
<span class="c1"># +---+</span>
<span class="c1"># |  0|</span>
<span class="c1"># |  1|</span>
<span class="c1"># +---+</span>
<span class="c1"># +---+</span>
<span class="c1"># | id|</span>
<span class="c1"># +---+</span>
<span class="c1"># |  2|</span>
<span class="c1"># |  3|</span>
<span class="c1"># +---+</span>
</pre></div>
</div>
<p>We can also use the same data source in streaming reader and writer</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">readStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;fake&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span><span class="o">.</span><span class="n">writeStream</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;fake&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="s2">&quot;/output_path&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="python-data-source-reader-with-direct-arrow-batch-support-for-improved-performance">
<h2>Python Data Source Reader with direct Arrow Batch support for improved performance<a class="headerlink" href="#python-data-source-reader-with-direct-arrow-batch-support-for-improved-performance" title="Permalink to this headline">#</a></h2>
<p>The Python Datasource Reader supports direct yielding of Arrow Batches, which can significantly improve data processing performance. By using the efficient Arrow format,
this feature avoids the overhead of traditional row-by-row data processing, resulting in performance improvements of up to one order of magnitude, especially with large datasets.</p>
<p><strong>Enabling Arrow Batch Support</strong>:
To enable this feature, configure your custom DataSource to yield Arrow batches by returning <cite>pyarrow.RecordBatch</cite> objects within the <cite>read</cite> method of your <cite>DataSourceReader</cite>
(or <cite>DataSourceStreamReader</cite>) implementation. This method simplifies data handling and reduces the number of I/O operations, particularly beneficial for large-scale data processing tasks.</p>
<p><strong>Arrow Batch Example</strong>:
The following example demonstrates how to implement a basic Data Source using Arrow Batch support.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.datasource</span> <span class="kn">import</span> <span class="n">DataSource</span><span class="p">,</span> <span class="n">DataSourceReader</span><span class="p">,</span> <span class="n">InputPartition</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">import</span> <span class="nn">pyarrow</span> <span class="k">as</span> <span class="nn">pa</span>

<span class="c1"># Define the ArrowBatchDataSource</span>
<span class="k">class</span> <span class="nc">ArrowBatchDataSource</span><span class="p">(</span><span class="n">DataSource</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Data Source for testing Arrow Batch Serialization</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;arrowbatch&quot;</span>

    <span class="k">def</span> <span class="nf">schema</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;key int, value string&quot;</span>

    <span class="k">def</span> <span class="nf">reader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ArrowBatchDataSourceReader</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">)</span>

<span class="c1"># Define the ArrowBatchDataSourceReader</span>
<span class="k">class</span> <span class="nc">ArrowBatchDataSourceReader</span><span class="p">(</span><span class="n">DataSourceReader</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">schema</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">options</span> <span class="o">=</span> <span class="n">options</span>

    <span class="k">def</span> <span class="nf">read</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">partition</span><span class="p">):</span>
        <span class="c1"># Create Arrow Record Batch</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="n">pa</span><span class="o">.</span><span class="n">int32</span><span class="p">())</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;one&quot;</span><span class="p">,</span> <span class="s2">&quot;two&quot;</span><span class="p">,</span> <span class="s2">&quot;three&quot;</span><span class="p">,</span> <span class="s2">&quot;four&quot;</span><span class="p">,</span> <span class="s2">&quot;five&quot;</span><span class="p">],</span> <span class="nb">type</span><span class="o">=</span><span class="n">pa</span><span class="o">.</span><span class="n">string</span><span class="p">())</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">schema</span><span class="p">([(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="n">pa</span><span class="o">.</span><span class="n">int32</span><span class="p">()),</span> <span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="n">pa</span><span class="o">.</span><span class="n">string</span><span class="p">())])</span>
        <span class="n">record_batch</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">RecordBatch</span><span class="o">.</span><span class="n">from_arrays</span><span class="p">([</span><span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">],</span> <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">record_batch</span>

    <span class="k">def</span> <span class="nf">partitions</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Define the number of partitions</span>
        <span class="n">num_part</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">InputPartition</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_part</span><span class="p">)]</span>

<span class="c1"># Initialize the Spark Session</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;ArrowBatchExample&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="c1"># Register the ArrowBatchDataSource</span>
<span class="n">spark</span><span class="o">.</span><span class="n">dataSource</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">ArrowBatchDataSource</span><span class="p">)</span>

<span class="c1"># Load data using the custom data source</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;arrowbatch&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="python_udtf.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Python User-defined Table Functions (UDTFs)</p>
      </div>
    </a>
    <a class="right-next"
       href="type_conversions.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Python to Spark Type Conversions</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-example">Simple Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-python-data-source">Creating a Python Data Source</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-batch-reader-and-writer-for-python-data-source">Implementing Batch Reader and Writer for Python Data Source</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-streaming-reader-and-writer-for-python-data-source">Implementing Streaming Reader and Writer for Python Data Source</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serialization-requirement">Serialization Requirement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-a-python-data-source">Using a Python Data Source</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-data-source-reader-with-direct-arrow-batch-support-for-improved-performance">Python Data Source Reader with direct Arrow Batch support for improved performance</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="../../_sources/user_guide/sql/python_data_source.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item"><p class="copyright">
    Copyright @ 2024 The Apache Software Foundation, Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License, Version 2.0</a>.
</p></div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.3.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>