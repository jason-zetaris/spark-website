<!DOCTYPE html ><html><head><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" name="viewport"/><title>Spark 4.0.0-preview2 ScalaDoc  - org.apache.spark.sql.api.Dataset</title><meta content="Spark 4.0.0 - preview2 ScalaDoc - org.apache.spark.sql.api.Dataset" name="description"/><meta content="Spark 4.0.0 preview2 ScalaDoc org.apache.spark.sql.api.Dataset" name="keywords"/><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><link href="../../../../../lib/index.css" media="screen" type="text/css" rel="stylesheet"/><link href="../../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet"/><link href="../../../../../lib/print.css" media="print" type="text/css" rel="stylesheet"/><link href="../../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css"/><script type="text/javascript" src="../../../../../lib/jquery.min.js"></script><script type="text/javascript" src="../../../../../lib/index.js"></script><script type="text/javascript" src="../../../../../index.js"></script><script type="text/javascript" src="../../../../../lib/scheduler.js"></script><script type="text/javascript" src="../../../../../lib/template.js"></script><script type="text/javascript">/* this variable can be used by the JS to determine the path to the root document */
var toRoot = '../../../../../';</script></head><body><div id="search"><span id="doc-title">Spark 4.0.0-preview2 ScalaDoc<span id="doc-version"></span></span> <span class="close-results"><span class="left">&lt;</span> Back</span><div id="textfilter"><span class="input"><input autocapitalize="none" placeholder="Search" id="index-input" type="text" accesskey="/"/><i class="clear material-icons"></i><i id="search-icon" class="material-icons"></i></span></div></div><div id="search-results"><div id="search-progress"><div id="progress-fill"></div></div><div id="results-content"><div id="entity-results"></div><div id="member-results"></div></div></div><div id="content-scroll-container" style="-webkit-overflow-scrolling: touch;"><div id="content-container" style="-webkit-overflow-scrolling: touch;"><div id="subpackage-spacer"><div id="packages"><h1>Packages</h1><ul><li class="indented0 " name="_root_.root" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="_root_" class="anchorToMember"></a><a id="root:_root_" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../../../index.html" title=""><span class="name">root</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../../../index.html" name="_root_" id="_root_" class="extype">root</a></dd></dl></div></li><li class="indented1 " name="_root_.org" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="org" class="anchorToMember"></a><a id="org:org" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../../index.html" title=""><span class="name">org</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../../../index.html" name="_root_" id="_root_" class="extype">root</a></dd></dl></div></li><li class="indented2 " name="org.apache" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="apache" class="anchorToMember"></a><a id="apache:apache" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../index.html" title=""><span class="name">apache</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../../index.html" name="org" id="org" class="extype">org</a></dd></dl></div></li><li class="indented3 " name="org.apache.spark" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="spark" class="anchorToMember"></a><a id="spark:spark" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../index.html" title="Core Spark functionality."><span class="name">spark</span></a></span><p class="shortcomment cmt">Core Spark functionality.</p><div class="fullcomment"><div class="comment cmt"><p>Core Spark functionality. <a href="../../SparkContext.html" name="org.apache.spark.SparkContext" id="org.apache.spark.SparkContext" class="extype">org.apache.spark.SparkContext</a> serves as the main entry point to
Spark, while <a href="../../rdd/RDD.html" name="org.apache.spark.rdd.RDD" id="org.apache.spark.rdd.RDD" class="extype">org.apache.spark.rdd.RDD</a> is the data type representing a distributed collection,
and provides most parallel operations.</p><p>In addition, <a href="../../rdd/PairRDDFunctions.html" name="org.apache.spark.rdd.PairRDDFunctions" id="org.apache.spark.rdd.PairRDDFunctions" class="extype">org.apache.spark.rdd.PairRDDFunctions</a> contains operations available only on RDDs
of key-value pairs, such as <code>groupByKey</code> and <code>join</code>; <a href="../../rdd/DoubleRDDFunctions.html" name="org.apache.spark.rdd.DoubleRDDFunctions" id="org.apache.spark.rdd.DoubleRDDFunctions" class="extype">org.apache.spark.rdd.DoubleRDDFunctions</a>
contains operations available only on RDDs of Doubles; and
<a href="../../rdd/SequenceFileRDDFunctions.html" name="org.apache.spark.rdd.SequenceFileRDDFunctions" id="org.apache.spark.rdd.SequenceFileRDDFunctions" class="extype">org.apache.spark.rdd.SequenceFileRDDFunctions</a> contains operations available on RDDs that can
be saved as SequenceFiles. These operations are automatically available on any RDD of the right
type (e.g. RDD[(Int, Int)] through implicit conversions.</p><p>Java programmers should reference the <a href="../../api/java/index.html" name="org.apache.spark.api.java" id="org.apache.spark.api.java" class="extype">org.apache.spark.api.java</a> package
for Spark programming APIs in Java.</p><p>Classes and methods marked with <span class="experimental badge" style="float: none;">
Experimental</span> are user-facing features which have not been officially adopted by the
Spark project. These are subject to change or removal in minor releases.</p><p>Classes and methods marked with <span class="developer badge" style="float: none;">
Developer API</span> are intended for advanced users want to extend Spark through lower
level interfaces. These are subject to changes or removal in minor releases.
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../index.html" name="org.apache" id="org.apache" class="extype">apache</a></dd></dl></div></li><li class="indented4 " name="org.apache.spark.sql" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="sql" class="anchorToMember"></a><a id="sql:sql" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../index.html" title="Allows the execution of relational queries, including those expressed in SQL using Spark."><span class="name">sql</span></a></span><p class="shortcomment cmt">Allows the execution of relational queries, including those expressed in SQL using Spark.</p><div class="fullcomment"><div class="comment cmt"><p>Allows the execution of relational queries, including those expressed in SQL using Spark.
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../index.html" name="org.apache.spark" id="org.apache.spark" class="extype">spark</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.api" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="api" class="anchorToMember"></a><a id="api:api" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="index.html" title="Contains API classes that are specific to a single language (i.e."><span class="name">api</span></a></span><p class="shortcomment cmt">Contains API classes that are specific to a single language (i.e.</p><div class="fullcomment"><div class="comment cmt"><p>Contains API classes that are specific to a single language (i.e. Java).
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented6 " name="org.apache.spark.sql.api.java" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="java" class="anchorToMember"></a><a id="java:java" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/java/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="java/index.html" title=""><span class="name">java</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="index.html" name="org.apache.spark.sql.api" id="org.apache.spark.sql.api" class="extype">api</a></dd></dl></div></li><li class="current-entities indented5"><span class="separator"></span> <a href="Catalog.html" title="Catalog interface for Spark." class="class"></a><a href="Catalog.html" title="Catalog interface for Spark.">Catalog</a></li><li class="current-entities indented5"><span class="separator"></span> <a href="DataFrameNaFunctions.html" title="Functionality for working with missing data in DataFrames." class="class"></a><a href="DataFrameNaFunctions.html" title="Functionality for working with missing data in DataFrames.">DataFrameNaFunctions</a></li><li class="current-entities indented5"><span class="separator"></span> <a href="DataFrameReader.html" title="Interface used to load a Dataset from external storage systems (e.g." class="class"></a><a href="DataFrameReader.html" title="Interface used to load a Dataset from external storage systems (e.g.">DataFrameReader</a></li><li class="current-entities indented5"><span class="separator"></span> <a href="DataFrameStatFunctions.html" title="Statistic functions for DataFrames." class="class"></a><a href="DataFrameStatFunctions.html" title="Statistic functions for DataFrames.">DataFrameStatFunctions</a></li><li class="current-entities indented5"><span class="separator"></span> <a href="" title="A Dataset is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations." class="class"></a><a href="" title="A Dataset is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations.">Dataset</a></li><li class="current-entities indented5"><span class="separator"></span> <a href="KeyValueGroupedDataset.html" title="A Dataset has been logically grouped by a user specified grouping key." class="class"></a><a href="KeyValueGroupedDataset.html" title="A Dataset has been logically grouped by a user specified grouping key.">KeyValueGroupedDataset</a></li><li class="current-entities indented5"><span class="separator"></span> <a href="RelationalGroupedDataset.html" title="A set of methods for aggregations on a DataFrame, created by groupBy, cube or rollup (and also pivot)." class="class"></a><a href="RelationalGroupedDataset.html" title="A set of methods for aggregations on a DataFrame, created by groupBy, cube or rollup (and also pivot).">RelationalGroupedDataset</a></li><li class="current-entities indented5"><span class="separator"></span> <a href="SparkSession.html" title="The entry point to programming Spark with the Dataset and DataFrame API." class="class"></a><a href="SparkSession.html" title="The entry point to programming Spark with the Dataset and DataFrame API.">SparkSession</a></li><li class="current-entities indented5"><span class="separator"></span> <a href="StreamingQuery.html" title="A handle to a query that is executing continuously in the background as new data arrives." class="trait"></a><a href="StreamingQuery.html" title="A handle to a query that is executing continuously in the background as new data arrives.">StreamingQuery</a></li><li class="current-entities indented5"><span class="separator"></span> <a href="UDFRegistration.html" title="Functions for registering user-defined functions." class="class"></a><a href="UDFRegistration.html" title="Functions for registering user-defined functions.">UDFRegistration</a></li></ul></div></div><div id="content"><body class="class type"><div id="definition"><div class="big-circle class">c</div><p id="owner"><a href="../../../../index.html" name="org" id="org" class="extype">org</a>.<a href="../../../index.html" name="org.apache" id="org.apache" class="extype">apache</a>.<a href="../../index.html" name="org.apache.spark" id="org.apache.spark" class="extype">spark</a>.<a href="../index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a>.<a href="index.html" name="org.apache.spark.sql.api" id="org.apache.spark.sql.api" class="extype">api</a></p><h1>Dataset<span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html" title="Permalink"><i class="material-icons"></i></a></span></h1><h3><span class="morelinks"></span></h3></div><h4 id="signature" class="signature"><span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">class</span></span> <span class="symbol"><span class="name">Dataset</span><span class="tparams">[<span name="T">T</span>, <span name="DS">DS<span class="tparams">[<span name="U">U</span>]</span> &lt;: <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>[<span name="org.apache.spark.sql.api.Dataset.DS.U" class="extype">U</span>, <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>]</span>]</span><span class="result"> extends <span name="scala.Serializable" class="extype">Serializable</span></span></span></h4><div id="comment" class="fullcommenttop"><div class="comment cmt"><p>A Dataset is a strongly typed collection of domain-specific objects that can be transformed in
parallel using functional or relational operations. Each Dataset also has an untyped view
called a <code>DataFrame</code>, which is a Dataset of <a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">org.apache.spark.sql.Row</a>.</p><p>Operations available on Datasets are divided into transformations and actions. Transformations
are the ones that produce new Datasets, and actions are the ones that trigger computation and
return results. Example transformations include map, filter, select, and aggregate (<code>groupBy</code>).
Example actions count, show, or writing data out to file systems.</p><p>Datasets are "lazy", i.e. computations are only triggered when an action is invoked.
Internally, a Dataset represents a logical plan that describes the computation required to
produce the data. When an action is invoked, Spark's query optimizer optimizes the logical plan
and generates a physical plan for efficient execution in a parallel and distributed manner. To
explore the logical plan as well as optimized physical plan, use the <code>explain</code> function.</p><p>To efficiently support domain-specific objects, an <a href="../Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">org.apache.spark.sql.Encoder</a> is
required. The encoder maps the domain specific type <code>T</code> to Spark's internal type system. For
example, given a class <code>Person</code> with two fields, <code>name</code> (string) and <code>age</code> (int), an encoder is
used to tell Spark to generate code at runtime to serialize the <code>Person</code> object into a binary
structure. This binary structure often has much lower memory footprint as well as are optimized
for efficiency in data processing (e.g. in a columnar format). To understand the internal
binary representation for data, use the <code>schema</code> function.</p><p>There are typically two ways to create a Dataset. The most common way is by pointing Spark to
some files on storage systems, using the <code>read</code> function available on a <code>SparkSession</code>.</p><pre><span class="kw">val</span> people = spark.read.parquet(<span class="lit">"..."</span>).as[Person]  <span class="cmt">// Scala</span>
Dataset&lt;Person&gt; people = spark.read().parquet(<span class="lit">"..."</span>).as(Encoders.bean(Person.<span class="kw">class</span>)); <span class="cmt">// Java</span></pre><p>Datasets can also be created through transformations available on existing Datasets. For
example, the following creates a new Dataset by applying a filter on the existing one:</p><pre><span class="kw">val</span> names = people.map(_.name)  <span class="cmt">// in Scala; names is a Dataset[String]</span>
Dataset&lt;<span class="std">String</span>&gt; names = people.map(
  (MapFunction&lt;Person, <span class="std">String</span>&gt;) p -&gt; p.name, Encoders.STRING()); <span class="cmt">// Java</span></pre><p>Dataset operations can also be untyped, through various domain-specific-language (DSL)
functions defined in: Dataset (this class), <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a>, and
<a href="../functions$.html" name="org.apache.spark.sql.functions" id="org.apache.spark.sql.functions" class="extype">org.apache.spark.sql.functions</a>. These operations are very similar to the operations
available in the data frame abstraction in R or Python.</p><p>To select a column from the Dataset, use <code>apply</code> method in Scala and <code>col</code> in Java.</p><pre><span class="kw">val</span> ageCol = people(<span class="lit">"age"</span>)  <span class="cmt">// in Scala</span>
Column ageCol = people.col(<span class="lit">"age"</span>); <span class="cmt">// in Java</span></pre><p>Note that the <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> type can also be manipulated through its various
functions.</p><pre><span class="cmt">// The following creates a new column that increases everybody's age by 10.</span>
people(<span class="lit">"age"</span>) + <span class="num">10</span>  <span class="cmt">// in Scala</span>
people.col(<span class="lit">"age"</span>).plus(<span class="num">10</span>);  <span class="cmt">// in Java</span></pre><p>A more concrete example in Scala:</p><pre><span class="cmt">// To create Dataset[Row] using SparkSession</span>
<span class="kw">val</span> people = spark.read.parquet(<span class="lit">"..."</span>)
<span class="kw">val</span> department = spark.read.parquet(<span class="lit">"..."</span>)

people.filter(<span class="lit">"age &gt; 30"</span>)
  .join(department, people(<span class="lit">"deptId"</span>) === department(<span class="lit">"id"</span>))
  .groupBy(department(<span class="lit">"name"</span>), people(<span class="lit">"gender"</span>))
  .agg(avg(people(<span class="lit">"salary"</span>)), max(people(<span class="lit">"age"</span>)))</pre><p>and in Java:</p><pre><span class="cmt">// To create Dataset&lt;Row&gt; using SparkSession</span>
Dataset&lt;Row&gt; people = spark.read().parquet(<span class="lit">"..."</span>);
Dataset&lt;Row&gt; department = spark.read().parquet(<span class="lit">"..."</span>);

people.filter(people.col(<span class="lit">"age"</span>).gt(<span class="num">30</span>))
  .join(department, people.col(<span class="lit">"deptId"</span>).equalTo(department.col(<span class="lit">"id"</span>)))
  .groupBy(department.col(<span class="lit">"name"</span>), people.col(<span class="lit">"gender"</span>))
  .agg(avg(people.col(<span class="lit">"salary"</span>)), max(people.col(<span class="lit">"age"</span>)));</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Stable</span><span class="args">()</span> </dd><dt>Source</dt><dd><a href="https://github.com/apache/spark/tree/v4.0.0-preview2/sql/api/src/main/scala/org/apache/spark/sql/api/Dataset.scala" target="_blank">Dataset.scala</a></dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl><div class="toggleContainer"><div class="toggle block"><span>Linear Supertypes</span><div class="superTypes hiddenContent"><a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/io/Serializable.html#java.io.Serializable" name="java.io.Serializable" id="java.io.Serializable" class="extype">Serializable</a>, <span name="scala.AnyRef" class="extype">AnyRef</span>, <span name="scala.Any" class="extype">Any</span></div></div></div><div class="toggleContainer"><div class="toggle block"><span>Known Subclasses</span><div class="subClasses hiddenContent"><a href="../Dataset.html" name="org.apache.spark.sql.Dataset" id="org.apache.spark.sql.Dataset" class="extype">Dataset</a></div></div></div></div><div id="mbrsel"><div class="toggle"></div><div id="memberfilter"><i class="material-icons arrow"></i><span class="input"><input placeholder="Filter all members" id="mbrsel-input" type="text" accesskey="/"/></span><i class="clear material-icons"></i></div><div id="filterby"><div id="order"><span class="filtertype">Ordering</span><ol><li class="group out"><span>Grouped</span></li><li class="alpha in"><span>Alphabetic</span></li><li class="inherit out"><span>By Inheritance</span></li></ol></div><div class="ancestors"><span class="filtertype">Inherited<br/></span><ol id="linearization"><li class="in" name="org.apache.spark.sql.api.Dataset"><span>Dataset</span></li><li class="in" name="java.io.Serializable"><span>Serializable</span></li><li class="in" name="scala.AnyRef"><span>AnyRef</span></li><li class="in" name="scala.Any"><span>Any</span></li></ol></div><div class="ancestors"><span class="filtertype"></span><ol><li class="hideall out"><span>Hide All</span></li><li class="showall in"><span>Show All</span></li></ol></div><div id="visbl"><span class="filtertype">Visibility</span><ol><li class="public in"><span>Public</span></li><li class="protected out"><span>Protected</span></li></ol></div></div></div><div id="template"><div id="allMembers"><div id="constructors" class="members"><h3>Instance Constructors</h3><ol><li class="indented0 " name="org.apache.spark.sql.api.Dataset#&lt;init&gt;" group="Ungrouped" fullComment="no" data-isabs="false" visbl="pub"><a id="&lt;init&gt;():org.apache.spark.sql.api.Dataset[T,DS]" class="anchorToMember"></a><a id="&lt;init&gt;:Dataset[T,DS]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#&lt;init&gt;():org.apache.spark.sql.api.Dataset[T,DS]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">new</span></span> <span class="symbol"><span class="name">Dataset</span><span class="params">()</span></span></li></ol></div><div id="types" class="types members"><h3>Type Members</h3><ol><li class="indented0 " name="org.apache.spark.sql.api.Dataset.RGD" group="Ungrouped" fullComment="no" data-isabs="true" visbl="pub"><a id="RGD&lt;:org.apache.spark.sql.api.RelationalGroupedDataset[DS]" class="anchorToMember"></a><a id="RGD:RGD" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#RGD&lt;:org.apache.spark.sql.api.RelationalGroupedDataset[DS]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">type</span></span> <span class="symbol"><span class="name">RGD</span><span class="result"> &lt;: <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.api.RelationalGroupedDataset" id="org.apache.spark.sql.api.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a>[<span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>]</span></span></li></ol></div><div class="values members"><h3>Abstract Value Members</h3><ol><li class="indented0 " name="org.apache.spark.sql.api.Dataset#as" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="as(alias:String):DS[T]" class="anchorToMember"></a><a id="as(String):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#as(alias:String):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">as</span><span class="params">(<span name="alias">alias: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with an alias set.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with an alias set.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#as" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="as[U](implicitevidence$1:org.apache.spark.sql.Encoder[U]):DS[U]" class="anchorToMember"></a><a id="as[U](Encoder[U]):DS[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#as[U](implicitevidence$1:org.apache.spark.sql.Encoder[U]):DS[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">as</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <a href="../Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.api.Dataset.as.U" class="extype">U</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.as.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset where each record has been mapped on to the specified type.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset where each record has been mapped on to the specified type. The method
used to map columns depend on the type of <code>U</code>:</p><ul><li>When <code>U</code> is a class, fields for the
class will be mapped to columns of the same name (case sensitivity is determined by
<code>spark.sql.caseSensitive</code>).</li><li>When <code>U</code> is a tuple, the columns will be mapped by
ordinal (i.e. the first column will be assigned to <code>_1</code>).</li><li>When <code>U</code> is a primitive
type (i.e. String, Int, etc), then the first column of the <code>DataFrame</code> will be used.</li></ul><p>If the schema of the Dataset does not match the desired <code>U</code> type, you can use <code>select</code> along
with <code>alias</code> or <code>as</code> to rearrange or rename as required.</p><p>Note that <code>as[]</code> only changes the view of the data that is passed into typed operations, such
as <code>map()</code>, and does not eagerly project away any columns that are not present in the
specified class.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#cache" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="cache():DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#cache():DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">cache</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Persist this Dataset with the default storage level (<code>MEMORY_AND_DISK</code>).</p><div class="fullcomment"><div class="comment cmt"><p>Persist this Dataset with the default storage level (<code>MEMORY_AND_DISK</code>).
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#checkpoint" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="prt"><a id="checkpoint(eager:Boolean,reliableCheckpoint:Boolean):DS[T]" class="anchorToMember"></a><a id="checkpoint(Boolean,Boolean):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#checkpoint(eager:Boolean,reliableCheckpoint:Boolean):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">checkpoint</span><span class="params">(<span name="eager">eager: <span name="scala.Boolean" class="extype">Boolean</span></span>, <span name="reliableCheckpoint">reliableCheckpoint: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a checkpointed version of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a checkpointed version of this Dataset.
</p></div><dl class="paramcmts block"><dt class="param">eager</dt><dd class="cmt"><p>
  Whether to checkpoint this dataframe immediately</p></dd><dt class="param">reliableCheckpoint</dt><dd class="cmt"><p>
  Whether to create a reliable checkpoint saved to files inside the checkpoint directory. If
  false creates a local checkpoint using the caching subsystem</p></dd></dl><dl class="attributes block"><dt>Attributes</dt><dd>protected </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#coalesce" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="coalesce(numPartitions:Int):DS[T]" class="anchorToMember"></a><a id="coalesce(Int):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#coalesce(numPartitions:Int):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">coalesce</span><span class="params">(<span name="numPartitions">numPartitions: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset that has exactly <code>numPartitions</code> partitions, when the fewer partitions
are requested.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset that has exactly <code>numPartitions</code> partitions, when the fewer partitions
are requested. If a larger number of partitions is requested, it will stay at the current
number of partitions. Similar to coalesce defined on an <code>RDD</code>, this operation results in a
narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not be a
shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.</p><p>However, if you're doing a drastic coalesce, e.g. to numPartitions = 1, this may result in
your computation taking place on fewer nodes than you like (e.g. one node in the case of
numPartitions = 1). To avoid this, you can call repartition. This will add a shuffle step,
but means the current upstream partitions will be executed in parallel (per whatever the
current partitioning is).
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#col" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="col(colName:String):org.apache.spark.sql.Column" class="anchorToMember"></a><a id="col(String):Column" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#col(colName:String):org.apache.spark.sql.Column" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">col</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span></span><p class="shortcomment cmt">Selects column based on the column name and returns it as a <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Selects column based on the column name and returns it as a <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  The column name can also reference to a nested column like <code>a.b</code>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#colRegex" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="colRegex(colName:String):org.apache.spark.sql.Column" class="anchorToMember"></a><a id="colRegex(String):Column" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#colRegex(colName:String):org.apache.spark.sql.Column" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">colRegex</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span></span><p class="shortcomment cmt">Selects column based on the column name specified as a regex and returns it as
<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Selects column based on the column name specified as a regex and returns it as
<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#collect" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="collect():Array[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#collect():Array[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">collect</span><span class="params">()</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns an array that contains all rows in this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns an array that contains all rows in this Dataset.</p><p>Running collect requires moving all the data into the application's driver process, and doing
so on a very large dataset can crash the driver process with OutOfMemoryError.</p><p>For Java API, use <a href="#collectAsList():java.util.List[T]" name="org.apache.spark.sql.api.Dataset#collectAsList" id="org.apache.spark.sql.api.Dataset#collectAsList" class="extmbr">collectAsList</a>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#collectAsList" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="collectAsList():java.util.List[T]" class="anchorToMember"></a><a id="collectAsList():List[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#collectAsList():java.util.List[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">collectAsList</span><span class="params">()</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/List.html#java.util.List" name="java.util.List" id="java.util.List" class="extype">List</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a Java list that contains all rows in this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a Java list that contains all rows in this Dataset.</p><p>Running collect requires moving all the data into the application's driver process, and doing
so on a very large dataset can crash the driver process with OutOfMemoryError.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#count" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="count():Long" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#count():Long" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">count</span><span class="params">()</span><span class="result">: <span name="scala.Long" class="extype">Long</span></span></span><p class="shortcomment cmt">Returns the number of rows in the Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the number of rows in the Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#createTempView" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="prt"><a id="createTempView(viewName:String,replace:Boolean,global:Boolean):Unit" class="anchorToMember"></a><a id="createTempView(String,Boolean,Boolean):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#createTempView(viewName:String,replace:Boolean,global:Boolean):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">createTempView</span><span class="params">(<span name="viewName">viewName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="replace">replace: <span name="scala.Boolean" class="extype">Boolean</span></span>, <span name="global">global: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Attributes</dt><dd>protected </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#crossJoin" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="crossJoin(right:DS[_]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="crossJoin(DS[_]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#crossJoin(right:DS[_]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">crossJoin</span><span class="params">(<span name="right">right: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Explicit cartesian join with another <code>DataFrame</code>.</p><div class="fullcomment"><div class="comment cmt"><p>Explicit cartesian join with another <code>DataFrame</code>.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>
  Right side of the join operation.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.1.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  Cartesian joins are very expensive without an extra filter that can be pushed down.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#cube" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="cube(cols:org.apache.spark.sql.Column*):Dataset.this.RGD" class="anchorToMember"></a><a id="cube(Column*):RGD" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#cube(cols:org.apache.spark.sql.Column*):Dataset.this.RGD" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">cube</span><span class="params">(<span name="cols">cols: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="#RGD&lt;:org.apache.spark.sql.api.RelationalGroupedDataset[DS]" name="org.apache.spark.sql.api.Dataset.RGD" id="org.apache.spark.sql.api.Dataset.RGD" class="extmbr">RGD</a></span></span><p class="shortcomment cmt">Create a multi-dimensional cube for the current Dataset using the specified columns, so we
can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Create a multi-dimensional cube for the current Dataset using the specified columns, so we
can run aggregation on them. See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.api.RelationalGroupedDataset" id="org.apache.spark.sql.api.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate
functions.</p><pre><span class="cmt">// Compute the average for all numeric columns cubed by department and group.</span>
ds.cube($<span class="lit">"department"</span>, $<span class="lit">"group"</span>).avg()

<span class="cmt">// Compute the max age and average salary, cubed by department and gender.</span>
ds.cube($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#describe" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="describe(cols:String*):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="describe(String*):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#describe(cols:String*):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">describe</span><span class="params">(<span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
and max.</p><div class="fullcomment"><div class="comment cmt"><p>Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
and max. If no columns are given, this function computes statistics for all numerical or
string columns.</p><p>This function is meant for exploratory data analysis, as we make no guarantee about the
backward compatibility of the schema of the resulting Dataset. If you want to
programmatically compute summary statistics, use the <code>agg</code> function instead.</p><pre>ds.describe(<span class="lit">"age"</span>, <span class="lit">"height"</span>).show()

<span class="cmt">// output:</span>
<span class="cmt">// summary age   height</span>
<span class="cmt">// count   10.0  10.0</span>
<span class="cmt">// mean    53.3  178.05</span>
<span class="cmt">// stddev  11.6  15.7</span>
<span class="cmt">// min     18.0  163.0</span>
<span class="cmt">// max     92.0  192.0</span></pre><p>Use <a href="#summary(statistics:String*):DS[org.apache.spark.sql.Row]" name="org.apache.spark.sql.api.Dataset#summary" id="org.apache.spark.sql.api.Dataset#summary" class="extmbr">summary</a> for expanded statistics and control over which statistics to compute.
</p></div><dl class="paramcmts block"><dt class="param">cols</dt><dd class="cmt"><p>
  Columns to compute statistics on.</p></dd></dl><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#drop" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="drop(col:org.apache.spark.sql.Column,cols:org.apache.spark.sql.Column*):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="drop(Column,Column*):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#drop(col:org.apache.spark.sql.Column,cols:org.apache.spark.sql.Column*):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">drop</span><span class="params">(<span name="col">col: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="cols">cols: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Returns a new Dataset with columns dropped.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with columns dropped.</p><p>This method can only be used to drop top level columns. This is a no-op if the Dataset
doesn't have a columns with an equivalent expression.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#drop" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="drop(colNames:String*):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="drop(String*):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#drop(colNames:String*):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">drop</span><span class="params">(<span name="colNames">colNames: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Returns a new Dataset with columns dropped.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with columns dropped. This is a no-op if schema doesn't contain column
name(s).</p><p>This method can only be used to drop top level columns. the colName string is treated
literally without further interpretation.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#dropDuplicates" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="dropDuplicates(colNames:Seq[String]):DS[T]" class="anchorToMember"></a><a id="dropDuplicates(Seq[String]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#dropDuplicates(colNames:Seq[String]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicates</span><span class="params">(<span name="colNames">colNames: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset with duplicate rows removed, considering only the
subset of columns.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset with duplicate rows removed, considering only the
subset of columns.</p><p>For a static batch <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, it just drops duplicate rows. For a streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, it
will keep all data across triggers as intermediate state to drop duplicates rows. You can use
<a href="#withWatermark(eventTime:String,delayThreshold:String):DS[T]" name="org.apache.spark.sql.api.Dataset#withWatermark" id="org.apache.spark.sql.api.Dataset#withWatermark" class="extmbr">withWatermark</a> to limit how late the duplicate data can be and system will accordingly
limit the state. In addition, too late data older than watermark will be dropped to avoid any
possibility of duplicates.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#dropDuplicates" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="dropDuplicates():DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#dropDuplicates():DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicates</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset that contains only the unique rows from this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset that contains only the unique rows from this Dataset. This is an alias
for <code>distinct</code>.</p><p>For a static batch <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, it just drops duplicate rows. For a streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, it
will keep all data across triggers as intermediate state to drop duplicates rows. You can use
<a href="#withWatermark(eventTime:String,delayThreshold:String):DS[T]" name="org.apache.spark.sql.api.Dataset#withWatermark" id="org.apache.spark.sql.api.Dataset#withWatermark" class="extmbr">withWatermark</a> to limit how late the duplicate data can be and system will accordingly
limit the state. In addition, too late data older than watermark will be dropped to avoid any
possibility of duplicates.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#dropDuplicatesWithinWatermark" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="dropDuplicatesWithinWatermark(colNames:Seq[String]):DS[T]" class="anchorToMember"></a><a id="dropDuplicatesWithinWatermark(Seq[String]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#dropDuplicatesWithinWatermark(colNames:Seq[String]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicatesWithinWatermark</span><span class="params">(<span name="colNames">colNames: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><p>This only works with streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, and watermark for the input <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> must be
set via <a href="#withWatermark(eventTime:String,delayThreshold:String):DS[T]" name="org.apache.spark.sql.api.Dataset#withWatermark" id="org.apache.spark.sql.api.Dataset#withWatermark" class="extmbr">withWatermark</a>.</p><p>For a streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, this will keep all data across triggers as intermediate state to
drop duplicated rows. The state will be kept to guarantee the semantic, "Events are
deduplicated as long as the time distance of earliest and latest events are smaller than the
delay threshold of watermark." Users are encouraged to set the delay threshold of watermark
longer than max timestamp differences among duplicated events.</p><p>Note: too late data older than watermark will be dropped.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.5.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#dropDuplicatesWithinWatermark" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="dropDuplicatesWithinWatermark():DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#dropDuplicatesWithinWatermark():DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicatesWithinWatermark</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with duplicates rows removed, within watermark.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with duplicates rows removed, within watermark.</p><p>This only works with streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, and watermark for the input <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> must be
set via <a href="#withWatermark(eventTime:String,delayThreshold:String):DS[T]" name="org.apache.spark.sql.api.Dataset#withWatermark" id="org.apache.spark.sql.api.Dataset#withWatermark" class="extmbr">withWatermark</a>.</p><p>For a streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, this will keep all data across triggers as intermediate state to
drop duplicated rows. The state will be kept to guarantee the semantic, "Events are
deduplicated as long as the time distance of earliest and latest events are smaller than the
delay threshold of watermark." Users are encouraged to set the delay threshold of watermark
longer than max timestamp differences among duplicated events.</p><p>Note: too late data older than watermark will be dropped.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.5.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#encoder" group="Ungrouped" fullComment="no" data-isabs="true" visbl="pub"><a id="encoder:org.apache.spark.sql.Encoder[T]" class="anchorToMember"></a><a id="encoder:Encoder[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#encoder:org.apache.spark.sql.Encoder[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">val</span></span> <span class="symbol"><span class="name">encoder</span><span class="result">: <a href="../Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#except" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="except(other:DS[T]):DS[T]" class="anchorToMember"></a><a id="except(DS[T]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#except(other:DS[T]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">except</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing rows in this Dataset but not in another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing rows in this Dataset but not in another Dataset. This is
equivalent to <code>EXCEPT DISTINCT</code> in SQL.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  Equality checking is performed directly on the encoded representation of the data and thus
  is not affected by a custom <code>equals</code> function defined on <code>T</code>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#exceptAll" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="exceptAll(other:DS[T]):DS[T]" class="anchorToMember"></a><a id="exceptAll(DS[T]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#exceptAll(other:DS[T]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">exceptAll</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing rows in this Dataset but not in another Dataset while
preserving the duplicates.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing rows in this Dataset but not in another Dataset while
preserving the duplicates. This is equivalent to <code>EXCEPT ALL</code> in SQL.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.4.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  Equality checking is performed directly on the encoded representation of the data and thus
  is not affected by a custom <code>equals</code> function defined on <code>T</code>. Also as standard in SQL, this
  function resolves columns by position (not by name).</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#explain" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="explain(mode:String):Unit" class="anchorToMember"></a><a id="explain(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#explain(mode:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">explain</span><span class="params">(<span name="mode">mode: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Prints the plans (logical and physical) with a format specified by a given explain mode.</p><div class="fullcomment"><div class="comment cmt"><p>Prints the plans (logical and physical) with a format specified by a given explain mode.
</p></div><dl class="paramcmts block"><dt class="param">mode</dt><dd class="cmt"><p>
  specifies the expected output format of plans.</p><ul><li><code>simple</code> Print only a physical
  plan.</li><li><code>extended</code>: Print both logical and physical plans.</li><li><code>codegen</code>: Print
  a physical plan and generated codes if they are available.</li><li><code>cost</code>: Print a logical
  plan and statistics if they are available.</li><li><code>formatted</code>: Split explain output into
  two sections: a physical plan outline and node details.</li></ul></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#filter" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="filter(func:org.apache.spark.api.java.function.FilterFunction[T]):DS[T]" class="anchorToMember"></a><a id="filter(FilterFunction[T]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#filter(func:org.apache.spark.api.java.function.FilterFunction[T]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">filter</span><span class="params">(<span name="func">func: <a href="../../api/java/function/FilterFunction.html" name="org.apache.spark.api.java.function.FilterFunction" id="org.apache.spark.api.java.function.FilterFunction" class="extype">FilterFunction</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Java-specific) Returns a new Dataset that only contains elements where <code>func</code> returns
<code>true</code>.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Returns a new Dataset that only contains elements where <code>func</code> returns
<code>true</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#filter" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="filter(func:T=&gt;Boolean):DS[T]" class="anchorToMember"></a><a id="filter((T)=&gt;Boolean):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#filter(func:T=&gt;Boolean):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">filter</span><span class="params">(<span name="func">func: (<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>) =&gt; <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset that only contains elements where <code>func</code> returns
<code>true</code>.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset that only contains elements where <code>func</code> returns
<code>true</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#filter" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="filter(condition:org.apache.spark.sql.Column):DS[T]" class="anchorToMember"></a><a id="filter(Column):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#filter(condition:org.apache.spark.sql.Column):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">filter</span><span class="params">(<span name="condition">condition: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Filters rows using the given condition.</p><div class="fullcomment"><div class="comment cmt"><p>Filters rows using the given condition.</p><pre><span class="cmt">// The following are equivalent:</span>
peopleDs.filter($<span class="lit">"age"</span> &gt; <span class="num">15</span>)
peopleDs.where($<span class="lit">"age"</span> &gt; <span class="num">15</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#foreachPartition" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="foreachPartition(f:Iterator[T]=&gt;Unit):Unit" class="anchorToMember"></a><a id="foreachPartition((Iterator[T])=&gt;Unit):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#foreachPartition(f:Iterator[T]=&gt;Unit):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">foreachPartition</span><span class="params">(<span name="f">f: (<span name="scala.Iterator" class="extype">Iterator</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]) =&gt; <span name="scala.Unit" class="extype">Unit</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Applies a function <code>f</code> to each partition of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Applies a function <code>f</code> to each partition of this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#groupBy" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="groupBy(cols:org.apache.spark.sql.Column*):Dataset.this.RGD" class="anchorToMember"></a><a id="groupBy(Column*):RGD" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#groupBy(cols:org.apache.spark.sql.Column*):Dataset.this.RGD" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">groupBy</span><span class="params">(<span name="cols">cols: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="#RGD&lt;:org.apache.spark.sql.api.RelationalGroupedDataset[DS]" name="org.apache.spark.sql.api.Dataset.RGD" id="org.apache.spark.sql.api.Dataset.RGD" class="extmbr">RGD</a></span></span><p class="shortcomment cmt">Groups the Dataset using the specified columns, so we can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Groups the Dataset using the specified columns, so we can run aggregation on them. See
<a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.api.RelationalGroupedDataset" id="org.apache.spark.sql.api.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate functions.</p><pre><span class="cmt">// Compute the average for all numeric columns grouped by department.</span>
ds.groupBy($<span class="lit">"department"</span>).avg()

<span class="cmt">// Compute the max age and average salary, grouped by department and gender.</span>
ds.groupBy($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#groupingSets" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="groupingSets(groupingSets:Seq[Seq[org.apache.spark.sql.Column]],cols:org.apache.spark.sql.Column*):Dataset.this.RGD" class="anchorToMember"></a><a id="groupingSets(Seq[Seq[Column]],Column*):RGD" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#groupingSets(groupingSets:Seq[Seq[org.apache.spark.sql.Column]],cols:org.apache.spark.sql.Column*):Dataset.this.RGD" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">groupingSets</span><span class="params">(<span name="groupingSets">groupingSets: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Seq" class="extype">Seq</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]]</span>, <span name="cols">cols: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="#RGD&lt;:org.apache.spark.sql.api.RelationalGroupedDataset[DS]" name="org.apache.spark.sql.api.Dataset.RGD" id="org.apache.spark.sql.api.Dataset.RGD" class="extmbr">RGD</a></span></span><p class="shortcomment cmt">Create multi-dimensional aggregation for the current Dataset using the specified grouping
sets, so we can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Create multi-dimensional aggregation for the current Dataset using the specified grouping
sets, so we can run aggregation on them. See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.api.RelationalGroupedDataset" id="org.apache.spark.sql.api.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the
available aggregate functions.</p><pre><span class="cmt">// Compute the average for all numeric columns group by specific grouping sets.</span>
ds.groupingSets(<span class="std">Seq</span>(<span class="std">Seq</span>($<span class="lit">"department"</span>, $<span class="lit">"group"</span>), <span class="std">Seq</span>()), $<span class="lit">"department"</span>, $<span class="lit">"group"</span>).avg()

<span class="cmt">// Compute the max age and average salary, group by specific grouping sets.</span>
ds.groupingSets(<span class="std">Seq</span>($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>), <span class="std">Seq</span>()), $<span class="lit">"department"</span>, $<span class="lit">"group"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>4.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#head" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="head(n:Int):Array[T]" class="anchorToMember"></a><a id="head(Int):Array[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#head(n:Int):Array[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">head</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns the first <code>n</code> rows.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the first <code>n</code> rows.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  this method should only be used if the resulting array is expected to be small, as all the
  data is loaded into the driver's memory.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#hint" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="hint(name:String,parameters:Any*):DS[T]" class="anchorToMember"></a><a id="hint(String,Any*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#hint(name:String,parameters:Any*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">hint</span><span class="params">(<span name="name">name: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="parameters">parameters: <span name="scala.Any" class="extype">Any</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Specifies some hint on the current Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Specifies some hint on the current Dataset. As an example, the following code specifies that
one of the plan can be broadcasted:</p><pre>df1.join(df2.hint(<span class="lit">"broadcast"</span>))</pre><p>the following code specifies that this dataset could be rebalanced with given number of
partitions:</p><pre>df1.hint(<span class="lit">"rebalance"</span>, <span class="num">10</span>)</pre></div><dl class="paramcmts block"><dt class="param">name</dt><dd class="cmt"><p>
  the name of the hint</p></dd><dt class="param">parameters</dt><dd class="cmt"><p>
  the parameters of the hint, all the parameters should be a <code>Column</code> or <code>Expression</code> or
  <code>Symbol</code> or could be converted into a <code>Literal</code></p></dd></dl><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.2.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#inputFiles" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="inputFiles:Array[String]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#inputFiles:Array[String]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">inputFiles</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span></span><p class="shortcomment cmt">Returns a best-effort snapshot of the files that compose this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a best-effort snapshot of the files that compose this Dataset. This method simply
asks each constituent BaseRelation for its respective files and takes the union of all
results. Depending on the source relations, this may not find all input files. Duplicates are
removed.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#intersect" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="intersect(other:DS[T]):DS[T]" class="anchorToMember"></a><a id="intersect(DS[T]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#intersect(other:DS[T]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">intersect</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing rows only in both this Dataset and another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing rows only in both this Dataset and another Dataset. This is
equivalent to <code>INTERSECT</code> in SQL.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  Equality checking is performed directly on the encoded representation of the data and thus
  is not affected by a custom <code>equals</code> function defined on <code>T</code>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#intersectAll" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="intersectAll(other:DS[T]):DS[T]" class="anchorToMember"></a><a id="intersectAll(DS[T]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#intersectAll(other:DS[T]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">intersectAll</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing rows only in both this Dataset and another Dataset while
preserving the duplicates.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing rows only in both this Dataset and another Dataset while
preserving the duplicates. This is equivalent to <code>INTERSECT ALL</code> in SQL.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.4.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  Equality checking is performed directly on the encoded representation of the data and thus
  is not affected by a custom <code>equals</code> function defined on <code>T</code>. Also as standard in SQL, this
  function resolves columns by position (not by name).</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#isEmpty" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="isEmpty:Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#isEmpty:Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">isEmpty</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><p class="shortcomment cmt">Returns true if the <code>Dataset</code> is empty.</p><div class="fullcomment"><div class="comment cmt"><p>Returns true if the <code>Dataset</code> is empty.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#isLocal" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="isLocal:Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#isLocal:Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">isLocal</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><p class="shortcomment cmt">Returns true if the <code>collect</code> and <code>take</code> methods can be run locally (without any Spark
executors).</p><div class="fullcomment"><div class="comment cmt"><p>Returns true if the <code>collect</code> and <code>take</code> methods can be run locally (without any Spark
executors).
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#isStreaming" group="streaming" fullComment="yes" data-isabs="true" visbl="pub"><a id="isStreaming:Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#isStreaming:Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">isStreaming</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><p class="shortcomment cmt">Returns true if this Dataset contains one or more sources that continuously return data as it
arrives.</p><div class="fullcomment"><div class="comment cmt"><p>Returns true if this Dataset contains one or more sources that continuously return data as it
arrives. A Dataset that reads data from a streaming source must be executed as a
<code>StreamingQuery</code> using the <code>start()</code> method in <code>DataStreamWriter</code>. Methods that return a
single answer, e.g. <code>count()</code> or <code>collect()</code>, will throw an
<a href="../AnalysisException.html" name="org.apache.spark.sql.AnalysisException" id="org.apache.spark.sql.AnalysisException" class="extype">org.apache.spark.sql.AnalysisException</a> when there is a streaming source present.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="join(right:DS[_],joinExprs:org.apache.spark.sql.Column,joinType:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="join(DS[_],Column,String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#join(right:DS[_],joinExprs:org.apache.spark.sql.Column,joinType:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span>, <span name="joinExprs">joinExprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="joinType">joinType: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Join with another <code>DataFrame</code>, using the given join expression.</p><div class="fullcomment"><div class="comment cmt"><p>Join with another <code>DataFrame</code>, using the given join expression. The following performs a full
outer join between <code>df1</code> and <code>df2</code>.</p><pre><span class="cmt">// Scala:</span>
<span class="kw">import</span> org.apache.spark.sql.functions._
df1.join(df2, $<span class="lit">"df1Key"</span> === $<span class="lit">"df2Key"</span>, <span class="lit">"outer"</span>)

<span class="cmt">// Java:</span>
<span class="kw">import</span> static org.apache.spark.sql.functions.*;
df1.join(df2, col(<span class="lit">"df1Key"</span>).equalTo(col(<span class="lit">"df2Key"</span>)), <span class="lit">"outer"</span>);</pre></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>
  Right side of the join.</p></dd><dt class="param">joinExprs</dt><dd class="cmt"><p>
  Join expression.</p></dd><dt class="param">joinType</dt><dd class="cmt"><p>
  Type of join to perform. Default <code>inner</code>. Must be one of: <code>inner</code>, <code>cross</code>, <code>outer</code>,
  <code>full</code>, <code>fullouter</code>, <code>full_outer</code>, <code>left</code>, <code>leftouter</code>, <code>left_outer</code>, <code>right</code>,
  <code>rightouter</code>, <code>right_outer</code>, <code>semi</code>, <code>leftsemi</code>, <code>left_semi</code>, <code>anti</code>, <code>leftanti</code>,
  <code>left_anti</code>.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="join(right:DS[_],usingColumns:Seq[String],joinType:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="join(DS[_],Seq[String],String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#join(right:DS[_],usingColumns:Seq[String],joinType:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span>, <span name="usingColumns">usingColumns: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>, <span name="joinType">joinType: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Scala-specific) Equi-join with another <code>DataFrame</code> using the given columns.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Equi-join with another <code>DataFrame</code> using the given columns. A cross join
with a predicate is specified as an inner join. If you would explicitly like to perform a
cross join use the <code>crossJoin</code> method.</p><p>Different from other join functions, the join columns will only appear once in the output,
i.e. similar to SQL's <code>JOIN USING</code> syntax.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>
  Right side of the join operation.</p></dd><dt class="param">usingColumns</dt><dd class="cmt"><p>
  Names of the columns to join on. This columns must exist on both sides.</p></dd><dt class="param">joinType</dt><dd class="cmt"><p>
  Type of join to perform. Default <code>inner</code>. Must be one of: <code>inner</code>, <code>cross</code>, <code>outer</code>,
  <code>full</code>, <code>fullouter</code>, <code>full_outer</code>, <code>left</code>, <code>leftouter</code>, <code>left_outer</code>, <code>right</code>,
  <code>rightouter</code>, <code>right_outer</code>, <code>semi</code>, <code>leftsemi</code>, <code>left_semi</code>, <code>anti</code>, <code>leftanti</code>,
  <code>left_anti</code>.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  If you perform a self-join using this function without aliasing the input <code>DataFrame</code>s, you
  will NOT be able to reference any columns after the join, since there is no way to
  disambiguate which side of the join you would like to reference.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="join(right:DS[_]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="join(DS[_]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#join(right:DS[_]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Join with another <code>DataFrame</code>.</p><div class="fullcomment"><div class="comment cmt"><p>Join with another <code>DataFrame</code>.</p><p>Behaves as an INNER JOIN and requires a subsequent join predicate.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>
  Right side of the join operation.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#joinWith" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="joinWith[U](other:DS[U],condition:org.apache.spark.sql.Column,joinType:String):DS[(T,U)]" class="anchorToMember"></a><a id="joinWith[U](DS[U],Column,String):DS[(T,U)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#joinWith[U](other:DS[U],condition:org.apache.spark.sql.Column,joinType:String):DS[(T,U)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">joinWith</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.joinWith.U" class="extype">U</span>]</span>, <span name="condition">condition: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="joinType">joinType: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[(<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.joinWith.U" class="extype">U</span>)]</span></span><p class="shortcomment cmt">Joins this Dataset returning a <code>Tuple2</code> for each pair where <code>condition</code> evaluates to true.</p><div class="fullcomment"><div class="comment cmt"><p>Joins this Dataset returning a <code>Tuple2</code> for each pair where <code>condition</code> evaluates to true.</p><p>This is similar to the relation <code>join</code> function with one important difference in the result
schema. Since <code>joinWith</code> preserves objects present on either side of the join, the result
schema is similarly nested into a tuple under the column names <code>_1</code> and <code>_2</code>.</p><p>This type of join can be useful both for preserving type-safety with the original object
types as well as working with relational data where either side of the join has column names
in common.
</p></div><dl class="paramcmts block"><dt class="param">other</dt><dd class="cmt"><p>
  Right side of the join.</p></dd><dt class="param">condition</dt><dd class="cmt"><p>
  Join expression.</p></dd><dt class="param">joinType</dt><dd class="cmt"><p>
  Type of join to perform. Default <code>inner</code>. Must be one of: <code>inner</code>, <code>cross</code>, <code>outer</code>,
  <code>full</code>, <code>fullouter</code>,<code>full_outer</code>, <code>left</code>, <code>leftouter</code>, <code>left_outer</code>, <code>right</code>, <code>rightouter</code>,
  <code>right_outer</code>.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#limit" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="limit(n:Int):DS[T]" class="anchorToMember"></a><a id="limit(Int):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#limit(n:Int):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">limit</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset by taking the first <code>n</code> rows.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by taking the first <code>n</code> rows. The difference between this function and
<code>head</code> is that <code>head</code> is an action and returns an array (by triggering query execution) while
<code>limit</code> returns a new Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#map" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="map[U](func:org.apache.spark.api.java.function.MapFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):DS[U]" class="anchorToMember"></a><a id="map[U](MapFunction[T,U],Encoder[U]):DS[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#map[U](func:org.apache.spark.api.java.function.MapFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):DS[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">map</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="func">func: <a href="../../api/java/function/MapFunction.html" name="org.apache.spark.api.java.function.MapFunction" id="org.apache.spark.api.java.function.MapFunction" class="extype">MapFunction</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.map.U" class="extype">U</span>]</span>, <span name="encoder">encoder: <a href="../Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.api.Dataset.map.U" class="extype">U</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.map.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Java-specific) Returns a new Dataset that contains the result of applying <code>func</code> to each
element.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Returns a new Dataset that contains the result of applying <code>func</code> to each
element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#map" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="map[U](func:T=&gt;U)(implicitevidence$4:org.apache.spark.sql.Encoder[U]):DS[U]" class="anchorToMember"></a><a id="map[U]((T)=&gt;U)(Encoder[U]):DS[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#map[U](func:T=&gt;U)(implicitevidence$4:org.apache.spark.sql.Encoder[U]):DS[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">map</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="func">func: (<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>) =&gt; <span name="org.apache.spark.sql.api.Dataset.map.U" class="extype">U</span></span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <a href="../Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.api.Dataset.map.U" class="extype">U</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.map.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset that contains the result of applying <code>func</code> to each
element.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset that contains the result of applying <code>func</code> to each
element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#mapPartitions" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="mapPartitions[U](func:Iterator[T]=&gt;Iterator[U])(implicitevidence$5:org.apache.spark.sql.Encoder[U]):DS[U]" class="anchorToMember"></a><a id="mapPartitions[U]((Iterator[T])=&gt;Iterator[U])(Encoder[U]):DS[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#mapPartitions[U](func:Iterator[T]=&gt;Iterator[U])(implicitevidence$5:org.apache.spark.sql.Encoder[U]):DS[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">mapPartitions</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="func">func: (<span name="scala.Iterator" class="extype">Iterator</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]) =&gt; <span name="scala.Iterator" class="extype">Iterator</span>[<span name="org.apache.spark.sql.api.Dataset.mapPartitions.U" class="extype">U</span>]</span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <a href="../Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.api.Dataset.mapPartitions.U" class="extype">U</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.mapPartitions.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset that contains the result of applying <code>func</code> to each
partition.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset that contains the result of applying <code>func</code> to each
partition.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#mergeInto" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="mergeInto(table:String,condition:org.apache.spark.sql.Column):org.apache.spark.sql.MergeIntoWriter[T]" class="anchorToMember"></a><a id="mergeInto(String,Column):MergeIntoWriter[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#mergeInto(table:String,condition:org.apache.spark.sql.Column):org.apache.spark.sql.MergeIntoWriter[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">mergeInto</span><span class="params">(<span name="table">table: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="condition">condition: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <a href="../MergeIntoWriter.html" name="org.apache.spark.sql.MergeIntoWriter" id="org.apache.spark.sql.MergeIntoWriter" class="extype">MergeIntoWriter</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Merges a set of updates, insertions, and deletions based on a source table into a target
table.</p><div class="fullcomment"><div class="comment cmt"><p>Merges a set of updates, insertions, and deletions based on a source table into a target
table.</p><p>Scala Examples:</p><pre>spark.table(<span class="lit">"source"</span>)
  .mergeInto(<span class="lit">"target"</span>, $<span class="lit">"source.id"</span> === $<span class="lit">"target.id"</span>)
  .whenMatched($<span class="lit">"salary"</span> === <span class="num">100</span>)
  .delete()
  .whenNotMatched()
  .insertAll()
  .whenNotMatchedBySource($<span class="lit">"salary"</span> === <span class="num">100</span>)
  .update(<span class="std">Map</span>(
    <span class="lit">"salary"</span> -&gt; lit(<span class="num">200</span>)
  ))
  .merge()</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>4.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#metadataColumn" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="metadataColumn(colName:String):org.apache.spark.sql.Column" class="anchorToMember"></a><a id="metadataColumn(String):Column" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#metadataColumn(colName:String):org.apache.spark.sql.Column" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">metadataColumn</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span></span><p class="shortcomment cmt">Selects a metadata column based on its logical column name, and returns it as a
<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Selects a metadata column based on its logical column name, and returns it as a
<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a>.</p><p>A metadata column can be accessed this way even if the underlying data source defines a data
column with a conflicting name.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.5.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#na" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="na:org.apache.spark.sql.api.DataFrameNaFunctions[DS]" class="anchorToMember"></a><a id="na:DataFrameNaFunctions[DS]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#na:org.apache.spark.sql.api.DataFrameNaFunctions[DS]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">na</span><span class="result">: <a href="DataFrameNaFunctions.html" name="org.apache.spark.sql.api.DataFrameNaFunctions" id="org.apache.spark.sql.api.DataFrameNaFunctions" class="extype">DataFrameNaFunctions</a>[<span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>]</span></span><p class="shortcomment cmt">Returns a <a href="DataFrameNaFunctions.html" name="org.apache.spark.sql.api.DataFrameNaFunctions" id="org.apache.spark.sql.api.DataFrameNaFunctions" class="extype">DataFrameNaFunctions</a> for working with missing data.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a <a href="DataFrameNaFunctions.html" name="org.apache.spark.sql.api.DataFrameNaFunctions" id="org.apache.spark.sql.api.DataFrameNaFunctions" class="extype">DataFrameNaFunctions</a> for working with missing data.</p><pre><span class="cmt">// Dropping rows containing any null values.</span>
ds.na.drop()</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#observe" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="observe(observation:org.apache.spark.sql.Observation,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):DS[T]" class="anchorToMember"></a><a id="observe(Observation,Column,Column*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#observe(observation:org.apache.spark.sql.Observation,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">observe</span><span class="params">(<span name="observation">observation: <a href="../Observation.html" name="org.apache.spark.sql.Observation" id="org.apache.spark.sql.Observation" class="extype">Observation</a></span>, <span name="expr">expr: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="exprs">exprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Observe (named) metrics through an <code>org.apache.spark.sql.Observation</code> instance.</p><div class="fullcomment"><div class="comment cmt"><p>Observe (named) metrics through an <code>org.apache.spark.sql.Observation</code> instance. This method
does not support streaming datasets.</p><p>A user can retrieve the metrics by accessing <code>org.apache.spark.sql.Observation.get</code>.</p><pre><span class="cmt">// Observe row count (rows) and highest id (maxid) in the Dataset while writing it</span>
<span class="kw">val</span> observation = Observation(<span class="lit">"my_metrics"</span>)
<span class="kw">val</span> observed_ds = ds.observe(observation, count(lit(<span class="num">1</span>)).as(<span class="lit">"rows"</span>), max($<span class="lit">"id"</span>).as(<span class="lit">"maxid"</span>))
observed_ds.write.parquet(<span class="lit">"ds.parquet"</span>)
<span class="kw">val</span> metrics = observation.get</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.3.0</p></dd><dt>Exceptions thrown</dt><dd><span class="cmt"><p><span name="IllegalArgumentException" class="extype"><code>IllegalArgumentException</code></span> 
  If this is a streaming Dataset (this.isStreaming == true)</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#observe" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="observe(name:String,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):DS[T]" class="anchorToMember"></a><a id="observe(String,Column,Column*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#observe(name:String,expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">observe</span><span class="params">(<span name="name">name: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="expr">expr: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="exprs">exprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Define (named) metrics to observe on the Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Define (named) metrics to observe on the Dataset. This method returns an 'observed' Dataset
that returns the same result as the input, with the following guarantees:</p><ul><li>It will
compute the defined aggregates (metrics) on all the data that is flowing through the Dataset
at that point.</li><li>It will report the value of the defined aggregate columns as soon as
we reach a completion point. A completion point is either the end of a query (batch mode) or
the end of a streaming epoch. The value of the aggregates only reflects the data processed
since the previous completion point.  Please note that continuous execution is
currently not supported.</li></ul><p>The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or
more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that
contain references to the input Dataset's columns must always be wrapped in an aggregate
function.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#offset" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="offset(n:Int):DS[T]" class="anchorToMember"></a><a id="offset(Int):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#offset(n:Int):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">offset</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset by skipping the first <code>n</code> rows.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by skipping the first <code>n</code> rows.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#persist" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="persist(newLevel:org.apache.spark.storage.StorageLevel):DS[T]" class="anchorToMember"></a><a id="persist(StorageLevel):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#persist(newLevel:org.apache.spark.storage.StorageLevel):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">persist</span><span class="params">(<span name="newLevel">newLevel: <a href="../../storage/StorageLevel.html" name="org.apache.spark.storage.StorageLevel" id="org.apache.spark.storage.StorageLevel" class="extype">StorageLevel</a></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Persist this Dataset with the given storage level.</p><div class="fullcomment"><div class="comment cmt"><p>Persist this Dataset with the given storage level.
</p></div><dl class="paramcmts block"><dt class="param">newLevel</dt><dd class="cmt"><p>
  One of: <code>MEMORY_ONLY</code>, <code>MEMORY_AND_DISK</code>, <code>MEMORY_ONLY_SER</code>, <code>MEMORY_AND_DISK_SER</code>,
  <code>DISK_ONLY</code>, <code>MEMORY_ONLY_2</code>, <code>MEMORY_AND_DISK_2</code>, etc.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#persist" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="persist():DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#persist():DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">persist</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Persist this Dataset with the default storage level (<code>MEMORY_AND_DISK</code>).</p><div class="fullcomment"><div class="comment cmt"><p>Persist this Dataset with the default storage level (<code>MEMORY_AND_DISK</code>).
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#randomSplit" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="randomSplit(weights:Array[Double]):Array[_&lt;:DS[T]]" class="anchorToMember"></a><a id="randomSplit(Array[Double]):Array[_&lt;:DS[T]]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#randomSplit(weights:Array[Double]):Array[_&lt;:DS[T]]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">randomSplit</span><span class="params">(<span name="weights">weights: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Double" class="extype">Double</span>]</span>)</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[_ &lt;: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]]</span></span><p class="shortcomment cmt">Randomly splits this Dataset with the provided weights.</p><div class="fullcomment"><div class="comment cmt"><p>Randomly splits this Dataset with the provided weights.
</p></div><dl class="paramcmts block"><dt class="param">weights</dt><dd class="cmt"><p>
  weights for splits, will be normalized if they don't sum to 1.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#randomSplit" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="randomSplit(weights:Array[Double],seed:Long):Array[_&lt;:DS[T]]" class="anchorToMember"></a><a id="randomSplit(Array[Double],Long):Array[_&lt;:DS[T]]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#randomSplit(weights:Array[Double],seed:Long):Array[_&lt;:DS[T]]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">randomSplit</span><span class="params">(<span name="weights">weights: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Double" class="extype">Double</span>]</span>, <span name="seed">seed: <span name="scala.Long" class="extype">Long</span></span>)</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[_ &lt;: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]]</span></span><p class="shortcomment cmt">Randomly splits this Dataset with the provided weights.</p><div class="fullcomment"><div class="comment cmt"><p>Randomly splits this Dataset with the provided weights.
</p></div><dl class="paramcmts block"><dt class="param">weights</dt><dd class="cmt"><p>
  weights for splits, will be normalized if they don't sum to 1.</p></dd><dt class="param">seed</dt><dd class="cmt"><p>
  Seed for sampling.
For Java API, use <a href="#randomSplitAsList(weights:Array[Double],seed:Long):java.util.List[_&lt;:DS[T]]" name="org.apache.spark.sql.api.Dataset#randomSplitAsList" id="org.apache.spark.sql.api.Dataset#randomSplitAsList" class="extmbr">randomSplitAsList</a>.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#randomSplitAsList" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="randomSplitAsList(weights:Array[Double],seed:Long):java.util.List[_&lt;:DS[T]]" class="anchorToMember"></a><a id="randomSplitAsList(Array[Double],Long):List[_&lt;:DS[T]]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#randomSplitAsList(weights:Array[Double],seed:Long):java.util.List[_&lt;:DS[T]]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">randomSplitAsList</span><span class="params">(<span name="weights">weights: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Double" class="extype">Double</span>]</span>, <span name="seed">seed: <span name="scala.Long" class="extype">Long</span></span>)</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/List.html#java.util.List" name="java.util.List" id="java.util.List" class="extype">List</a>[_ &lt;: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]]</span></span><p class="shortcomment cmt">Returns a Java list that contains randomly split Dataset with the provided weights.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a Java list that contains randomly split Dataset with the provided weights.
</p></div><dl class="paramcmts block"><dt class="param">weights</dt><dd class="cmt"><p>
  weights for splits, will be normalized if they don't sum to 1.</p></dd><dt class="param">seed</dt><dd class="cmt"><p>
  Seed for sampling.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#reduce" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="reduce(func:(T,T)=&gt;T):T" class="anchorToMember"></a><a id="reduce((T,T)=&gt;T):T" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#reduce(func:(T,T)=&gt;T):T" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">reduce</span><span class="params">(<span name="func">func: (<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>) =&gt; <span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span></span></span><p class="shortcomment cmt">(Scala-specific) Reduces the elements of this Dataset using the specified binary function.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Reduces the elements of this Dataset using the specified binary function.
The given <code>func</code> must be commutative and associative or the result may be non-deterministic.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#repartition" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="repartition(numPartitions:Int):DS[T]" class="anchorToMember"></a><a id="repartition(Int):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#repartition(numPartitions:Int):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartition</span><span class="params">(<span name="numPartitions">numPartitions: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset that has exactly <code>numPartitions</code> partitions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset that has exactly <code>numPartitions</code> partitions.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#repartitionByExpression" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="prt"><a id="repartitionByExpression(numPartitions:Option[Int],partitionExprs:Seq[org.apache.spark.sql.Column]):DS[T]" class="anchorToMember"></a><a id="repartitionByExpression(Option[Int],Seq[Column]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#repartitionByExpression(numPartitions:Option[Int],partitionExprs:Seq[org.apache.spark.sql.Column]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartitionByExpression</span><span class="params">(<span name="numPartitions">numPartitions: <span name="scala.Option" class="extype">Option</span>[<span name="scala.Int" class="extype">Int</span>]</span>, <span name="partitionExprs">partitionExprs: <span name="scala.Seq" class="extype">Seq</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><div class="fullcomment"><dl class="attributes block"><dt>Attributes</dt><dd>protected </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#repartitionByRange" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="prt"><a id="repartitionByRange(numPartitions:Option[Int],partitionExprs:Seq[org.apache.spark.sql.Column]):DS[T]" class="anchorToMember"></a><a id="repartitionByRange(Option[Int],Seq[Column]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#repartitionByRange(numPartitions:Option[Int],partitionExprs:Seq[org.apache.spark.sql.Column]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartitionByRange</span><span class="params">(<span name="numPartitions">numPartitions: <span name="scala.Option" class="extype">Option</span>[<span name="scala.Int" class="extype">Int</span>]</span>, <span name="partitionExprs">partitionExprs: <span name="scala.Seq" class="extype">Seq</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><div class="fullcomment"><dl class="attributes block"><dt>Attributes</dt><dd>protected </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#rollup" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="rollup(cols:org.apache.spark.sql.Column*):Dataset.this.RGD" class="anchorToMember"></a><a id="rollup(Column*):RGD" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#rollup(cols:org.apache.spark.sql.Column*):Dataset.this.RGD" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">rollup</span><span class="params">(<span name="cols">cols: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <a href="#RGD&lt;:org.apache.spark.sql.api.RelationalGroupedDataset[DS]" name="org.apache.spark.sql.api.Dataset.RGD" id="org.apache.spark.sql.api.Dataset.RGD" class="extmbr">RGD</a></span></span><p class="shortcomment cmt">Create a multi-dimensional rollup for the current Dataset using the specified columns, so we
can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Create a multi-dimensional rollup for the current Dataset using the specified columns, so we
can run aggregation on them. See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.api.RelationalGroupedDataset" id="org.apache.spark.sql.api.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate
functions.</p><pre><span class="cmt">// Compute the average for all numeric columns rolled up by department and group.</span>
ds.rollup($<span class="lit">"department"</span>, $<span class="lit">"group"</span>).avg()

<span class="cmt">// Compute the max age and average salary, rolled up by department and gender.</span>
ds.rollup($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sameSemantics" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="sameSemantics(other:DS[T]):Boolean" class="anchorToMember"></a><a id="sameSemantics(DS[T]):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sameSemantics(other:DS[T]):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sameSemantics</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><p class="shortcomment cmt">Returns <code>true</code> when the logical query plans inside both <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>s are equal and therefore
return same results.</p><div class="fullcomment"><div class="comment cmt"><p>Returns <code>true</code> when the logical query plans inside both <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>s are equal and therefore
return same results.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@DeveloperApi</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.1.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  The equality comparison here is simplified by tolerating the cosmetic differences such as
  attribute names.</p></span>, <span class="cmt"><p>
  This API can compare both <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>s very fast but can still return <code>false</code> on the
  <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> that return the same results, for instance, from different plans. Such false
  negative semantic can be useful when caching as an example.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sample" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="sample(withReplacement:Boolean,fraction:Double,seed:Long):DS[T]" class="anchorToMember"></a><a id="sample(Boolean,Double,Long):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sample(withReplacement:Boolean,fraction:Double,seed:Long):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sample</span><span class="params">(<span name="withReplacement">withReplacement: <span name="scala.Boolean" class="extype">Boolean</span></span>, <span name="fraction">fraction: <span name="scala.Double" class="extype">Double</span></span>, <span name="seed">seed: <span name="scala.Long" class="extype">Long</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> by sampling a fraction of rows, using a user-supplied seed.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> by sampling a fraction of rows, using a user-supplied seed.
</p></div><dl class="paramcmts block"><dt class="param">withReplacement</dt><dd class="cmt"><p>
  Sample with replacement or not.</p></dd><dt class="param">fraction</dt><dd class="cmt"><p>
  Fraction of rows to generate, range [0.0, 1.0].</p></dd><dt class="param">seed</dt><dd class="cmt"><p>
  Seed for sampling.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  This is NOT guaranteed to provide exactly the fraction of the count of the given
  <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#schema" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="schema:org.apache.spark.sql.types.StructType" class="anchorToMember"></a><a id="schema:StructType" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#schema:org.apache.spark.sql.types.StructType" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">schema</span><span class="result">: <a href="../types/StructType.html" name="org.apache.spark.sql.types.StructType" id="org.apache.spark.sql.types.StructType" class="extype">StructType</a></span></span><p class="shortcomment cmt">Returns the schema of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the schema of this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#select" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="select[U1](c1:org.apache.spark.sql.TypedColumn[T,U1]):DS[U1]" class="anchorToMember"></a><a id="select[U1](TypedColumn[T,U1]):DS[U1]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#select[U1](c1:org.apache.spark.sql.TypedColumn[T,U1]):DS[U1]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="tparams">[<span name="U1">U1</span>]</span><span class="params">(<span name="c1">c1: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U1" class="extype">U1</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.select.U1" class="extype">U1</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset by computing the given <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> expression for
each element.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by computing the given <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> expression for
each element.</p><pre><span class="kw">val</span> ds = <span class="std">Seq</span>(<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>).toDS()
<span class="kw">val</span> newDS = ds.select(expr(<span class="lit">"value + 1"</span>).as[<span class="std">Int</span>])</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#select" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="select(cols:org.apache.spark.sql.Column*):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="select(Column*):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#select(cols:org.apache.spark.sql.Column*):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="params">(<span name="cols">cols: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Selects a set of column based expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Selects a set of column based expressions.</p><pre>ds.select($<span class="lit">"colA"</span>, $<span class="lit">"colB"</span> + <span class="num">1</span>)</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#selectUntyped" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="prt"><a id="selectUntyped(columns:org.apache.spark.sql.TypedColumn[_,_]*):DS[_]" class="anchorToMember"></a><a id="selectUntyped(TypedColumn[_,_]*):DS[_]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#selectUntyped(columns:org.apache.spark.sql.TypedColumn[_,_]*):DS[_]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">selectUntyped</span><span class="params">(<span name="columns">columns: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[_, _]*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span></span><p class="shortcomment cmt">Internal helper function for building typed selects that return tuples.</p><div class="fullcomment"><div class="comment cmt"><p>Internal helper function for building typed selects that return tuples. For simplicity and
code reuse, we do this without the help of the type system and then use helper functions that
cast appropriately for the user facing interface.
</p></div><dl class="attributes block"><dt>Attributes</dt><dd>protected </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#semanticHash" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="semanticHash():Int" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#semanticHash():Int" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">semanticHash</span><span class="params">()</span><span class="result">: <span name="scala.Int" class="extype">Int</span></span></span><p class="shortcomment cmt">Returns a <code>hashCode</code> of the logical query plan against this <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a <code>hashCode</code> of the logical query plan against this <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@DeveloperApi</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.1.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  Unlike the standard <code>hashCode</code>, the hash is calculated against the query plan simplified by
  tolerating the cosmetic differences such as attribute names.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#show" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="show(numRows:Int,truncate:Int,vertical:Boolean):Unit" class="anchorToMember"></a><a id="show(Int,Int,Boolean):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#show(numRows:Int,truncate:Int,vertical:Boolean):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">(<span name="numRows">numRows: <span name="scala.Int" class="extype">Int</span></span>, <span name="truncate">truncate: <span name="scala.Int" class="extype">Int</span></span>, <span name="vertical">vertical: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the Dataset in a tabular form. For example:</p><pre>year  month AVG('Adj Close) MAX('Adj Close)
<span class="num">1980</span>  <span class="num">12</span>    <span class="num">0.503218</span>        <span class="num">0.595103</span>
<span class="num">1981</span>  <span class="num">01</span>    <span class="num">0.523289</span>        <span class="num">0.570307</span>
<span class="num">1982</span>  <span class="num">02</span>    <span class="num">0.436504</span>        <span class="num">0.475256</span>
<span class="num">1983</span>  <span class="num">03</span>    <span class="num">0.410516</span>        <span class="num">0.442194</span>
<span class="num">1984</span>  <span class="num">04</span>    <span class="num">0.450090</span>        <span class="num">0.483521</span></pre><p>If <code>vertical</code> enabled, this command prints output rows vertically (one line per column
value)?</p><pre>-RECORD <span class="num">0</span>-------------------
 year            | <span class="num">1980</span>
 month           | <span class="num">12</span>
 AVG('Adj Close) | <span class="num">0.503218</span>
 AVG('Adj Close) | <span class="num">0.595103</span>
-RECORD <span class="num">1</span>-------------------
 year            | <span class="num">1981</span>
 month           | <span class="num">01</span>
 AVG('Adj Close) | <span class="num">0.523289</span>
 AVG('Adj Close) | <span class="num">0.570307</span>
-RECORD <span class="num">2</span>-------------------
 year            | <span class="num">1982</span>
 month           | <span class="num">02</span>
 AVG('Adj Close) | <span class="num">0.436504</span>
 AVG('Adj Close) | <span class="num">0.475256</span>
-RECORD <span class="num">3</span>-------------------
 year            | <span class="num">1983</span>
 month           | <span class="num">03</span>
 AVG('Adj Close) | <span class="num">0.410516</span>
 AVG('Adj Close) | <span class="num">0.442194</span>
-RECORD <span class="num">4</span>-------------------
 year            | <span class="num">1984</span>
 month           | <span class="num">04</span>
 AVG('Adj Close) | <span class="num">0.450090</span>
 AVG('Adj Close) | <span class="num">0.483521</span></pre></div><dl class="paramcmts block"><dt class="param">numRows</dt><dd class="cmt"><p>
  Number of rows to show</p></dd><dt class="param">truncate</dt><dd class="cmt"><p>
  If set to more than 0, truncates strings to <code>truncate</code> characters and all cells will be
  aligned right.</p></dd><dt class="param">vertical</dt><dd class="cmt"><p>
  If set to true, prints output rows vertically (one line per column value).</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#show" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="show(numRows:Int,truncate:Boolean):Unit" class="anchorToMember"></a><a id="show(Int,Boolean):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#show(numRows:Int,truncate:Boolean):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">(<span name="numRows">numRows: <span name="scala.Int" class="extype">Int</span></span>, <span name="truncate">truncate: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the Dataset in a tabular form. For example:</p><pre>year  month AVG('Adj Close) MAX('Adj Close)
<span class="num">1980</span>  <span class="num">12</span>    <span class="num">0.503218</span>        <span class="num">0.595103</span>
<span class="num">1981</span>  <span class="num">01</span>    <span class="num">0.523289</span>        <span class="num">0.570307</span>
<span class="num">1982</span>  <span class="num">02</span>    <span class="num">0.436504</span>        <span class="num">0.475256</span>
<span class="num">1983</span>  <span class="num">03</span>    <span class="num">0.410516</span>        <span class="num">0.442194</span>
<span class="num">1984</span>  <span class="num">04</span>    <span class="num">0.450090</span>        <span class="num">0.483521</span></pre></div><dl class="paramcmts block"><dt class="param">numRows</dt><dd class="cmt"><p>
  Number of rows to show</p></dd><dt class="param">truncate</dt><dd class="cmt"><p>
  Whether truncate long strings. If true, strings more than 20 characters will be truncated
  and all cells will be aligned right</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sortInternal" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="prt"><a id="sortInternal(global:Boolean,sortExprs:Seq[org.apache.spark.sql.Column]):DS[T]" class="anchorToMember"></a><a id="sortInternal(Boolean,Seq[Column]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sortInternal(global:Boolean,sortExprs:Seq[org.apache.spark.sql.Column]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sortInternal</span><span class="params">(<span name="global">global: <span name="scala.Boolean" class="extype">Boolean</span></span>, <span name="sortExprs">sortExprs: <span name="scala.Seq" class="extype">Seq</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><div class="fullcomment"><dl class="attributes block"><dt>Attributes</dt><dd>protected </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sparkSession" group="Ungrouped" fullComment="no" data-isabs="true" visbl="pub"><a id="sparkSession:org.apache.spark.sql.api.SparkSession[DS]" class="anchorToMember"></a><a id="sparkSession:SparkSession[DS]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sparkSession:org.apache.spark.sql.api.SparkSession[DS]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sparkSession</span><span class="result">: <a href="SparkSession.html" name="org.apache.spark.sql.api.SparkSession" id="org.apache.spark.sql.api.SparkSession" class="extype">SparkSession</a>[<span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>]</span></span></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#stat" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="stat:org.apache.spark.sql.api.DataFrameStatFunctions[DS]" class="anchorToMember"></a><a id="stat:DataFrameStatFunctions[DS]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#stat:org.apache.spark.sql.api.DataFrameStatFunctions[DS]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">stat</span><span class="result">: <a href="DataFrameStatFunctions.html" name="org.apache.spark.sql.api.DataFrameStatFunctions" id="org.apache.spark.sql.api.DataFrameStatFunctions" class="extype">DataFrameStatFunctions</a>[<span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>]</span></span><p class="shortcomment cmt">Returns a <a href="DataFrameStatFunctions.html" name="org.apache.spark.sql.api.DataFrameStatFunctions" id="org.apache.spark.sql.api.DataFrameStatFunctions" class="extype">DataFrameStatFunctions</a> for working statistic functions support.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a <a href="DataFrameStatFunctions.html" name="org.apache.spark.sql.api.DataFrameStatFunctions" id="org.apache.spark.sql.api.DataFrameStatFunctions" class="extype">DataFrameStatFunctions</a> for working statistic functions support.</p><pre><span class="cmt">// Finding frequent items in column with name 'a'.</span>
ds.stat.freqItems(<span class="std">Seq</span>(<span class="lit">"a"</span>))</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#storageLevel" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="storageLevel:org.apache.spark.storage.StorageLevel" class="anchorToMember"></a><a id="storageLevel:StorageLevel" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#storageLevel:org.apache.spark.storage.StorageLevel" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">storageLevel</span><span class="result">: <a href="../../storage/StorageLevel.html" name="org.apache.spark.storage.StorageLevel" id="org.apache.spark.storage.StorageLevel" class="extype">StorageLevel</a></span></span><p class="shortcomment cmt">Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.</p><div class="fullcomment"><div class="comment cmt"><p>Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.1.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#summary" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="summary(statistics:String*):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="summary(String*):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#summary(statistics:String*):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">summary</span><span class="params">(<span name="statistics">statistics: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Computes specified statistics for numeric and string columns.</p><div class="fullcomment"><div class="comment cmt"><p>Computes specified statistics for numeric and string columns. Available statistics are:</p><ul><li>count</li><li>mean</li><li>stddev</li><li>min</li><li>max</li><li>arbitrary
approximate percentiles specified as a percentage (e.g. 75%)</li><li>count_distinct</li><li>approx_count_distinct</li></ul><p>If no statistics are given, this function computes count, mean, stddev, min, approximate
quartiles (percentiles at 25%, 50%, and 75%), and max.</p><p>This function is meant for exploratory data analysis, as we make no guarantee about the
backward compatibility of the schema of the resulting Dataset. If you want to
programmatically compute summary statistics, use the <code>agg</code> function instead.</p><pre>ds.summary().show()

<span class="cmt">// output:</span>
<span class="cmt">// summary age   height</span>
<span class="cmt">// count   10.0  10.0</span>
<span class="cmt">// mean    53.3  178.05</span>
<span class="cmt">// stddev  11.6  15.7</span>
<span class="cmt">// min     18.0  163.0</span>
<span class="cmt">// 25%     24.0  176.0</span>
<span class="cmt">// 50%     24.0  176.0</span>
<span class="cmt">// 75%     32.0  180.0</span>
<span class="cmt">// max     92.0  192.0</span></pre><pre>ds.summary(<span class="lit">"count"</span>, <span class="lit">"min"</span>, <span class="lit">"25%"</span>, <span class="lit">"75%"</span>, <span class="lit">"max"</span>).show()

<span class="cmt">// output:</span>
<span class="cmt">// summary age   height</span>
<span class="cmt">// count   10.0  10.0</span>
<span class="cmt">// min     18.0  163.0</span>
<span class="cmt">// 25%     24.0  176.0</span>
<span class="cmt">// 75%     32.0  180.0</span>
<span class="cmt">// max     92.0  192.0</span></pre><p>To do a summary for specific columns first select them:</p><pre>ds.select(<span class="lit">"age"</span>, <span class="lit">"height"</span>).summary().show()</pre><p>Specify statistics to output custom summaries:</p><pre>ds.summary(<span class="lit">"count"</span>, <span class="lit">"count_distinct"</span>).show()</pre><p>The distinct count isn't included by default.</p><p>You can also run approximate distinct counts which are faster:</p><pre>ds.summary(<span class="lit">"count"</span>, <span class="lit">"approx_count_distinct"</span>).show()</pre><p>See also <a href="#describe(cols:String*):DS[org.apache.spark.sql.Row]" name="org.apache.spark.sql.api.Dataset#describe" id="org.apache.spark.sql.api.Dataset#describe" class="extmbr">describe</a> for basic statistics.
</p></div><dl class="paramcmts block"><dt class="param">statistics</dt><dd class="cmt"><p>
  Statistics from above list to be computed.</p></dd></dl><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#tail" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="tail(n:Int):Array[T]" class="anchorToMember"></a><a id="tail(Int):Array[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#tail(n:Int):Array[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">tail</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns the last <code>n</code> rows in the Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the last <code>n</code> rows in the Dataset.</p><p>Running tail requires moving data into the application's driver process, and doing so with a
very large <code>n</code> can crash the driver process with OutOfMemoryError.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#to" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="to(schema:org.apache.spark.sql.types.StructType):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="to(StructType):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#to(schema:org.apache.spark.sql.types.StructType):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">to</span><span class="params">(<span name="schema">schema: <a href="../types/StructType.html" name="org.apache.spark.sql.types.StructType" id="org.apache.spark.sql.types.StructType" class="extype">StructType</a></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Returns a new DataFrame where each row is reconciled to match the specified schema.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new DataFrame where each row is reconciled to match the specified schema. Spark
will:</p><ul><li>Reorder columns and/or inner fields by name to match the specified
schema.</li><li>Project away columns and/or inner fields that are not needed by the
specified schema. Missing columns and/or inner fields (present in the specified schema but
not input DataFrame) lead to failures.</li><li>Cast the columns and/or inner fields to match
the data types in the specified schema, if the types are compatible, e.g., numeric to numeric
(error if overflows), but not string to int.</li><li>Carry over the metadata from the
specified schema, while the columns and/or inner fields still keep their own metadata if not
overwritten by the specified schema.</li><li>Fail if the nullability is not compatible. For
example, the column and/or inner field is nullable but the specified schema requires them to
be not nullable.
</li></ul></div><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#toDF" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="toDF(colNames:String*):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="toDF(String*):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#toDF(colNames:String*):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toDF</span><span class="params">(<span name="colNames">colNames: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Converts this strongly typed collection of data to generic <code>DataFrame</code> with columns renamed.</p><div class="fullcomment"><div class="comment cmt"><p>Converts this strongly typed collection of data to generic <code>DataFrame</code> with columns renamed.
This can be quite convenient in conversion from an RDD of tuples into a <code>DataFrame</code> with
meaningful names. For example:</p><pre><span class="kw">val</span> rdd: RDD[(<span class="std">Int</span>, <span class="std">String</span>)] = ...
rdd.toDF()  <span class="cmt">// this implicit conversion creates a DataFrame with column name `_1` and `_2`</span>
rdd.toDF(<span class="lit">"id"</span>, <span class="lit">"name"</span>)  <span class="cmt">// this creates a DataFrame with column name "id" and "name"</span></pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#toDF" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="toDF():DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="toDF():DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#toDF():DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toDF</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Converts this strongly typed collection of data to generic Dataframe.</p><div class="fullcomment"><div class="comment cmt"><p>Converts this strongly typed collection of data to generic Dataframe. In contrast to the
strongly typed objects that Dataset operations work on, a Dataframe returns generic
<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">org.apache.spark.sql.Row</a> objects that allow fields to be accessed by ordinal or name.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#toJSON" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="toJSON:DS[String]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#toJSON:DS[String]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toJSON</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="scala.Predef.String" class="extype">String</span>]</span></span><p class="shortcomment cmt">Returns the content of the Dataset as a Dataset of JSON strings.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the content of the Dataset as a Dataset of JSON strings.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#toLocalIterator" group="action" fullComment="yes" data-isabs="true" visbl="pub"><a id="toLocalIterator():java.util.Iterator[T]" class="anchorToMember"></a><a id="toLocalIterator():Iterator[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#toLocalIterator():java.util.Iterator[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toLocalIterator</span><span class="params">()</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Iterator.html#java.util.Iterator" name="java.util.Iterator" id="java.util.Iterator" class="extype">Iterator</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns an iterator that contains all rows in this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns an iterator that contains all rows in this Dataset.</p><p>The iterator will consume as much memory as the largest partition in this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  this results in multiple Spark jobs, and if the input Dataset is the result of a wide
  transformation (e.g. join with different partitioners), to avoid recomputing the input
  Dataset should be cached first.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#transpose" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="transpose():DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="transpose():DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#transpose():DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">transpose</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Transposes a DataFrame, switching rows to columns.</p><div class="fullcomment"><div class="comment cmt"><p>Transposes a DataFrame, switching rows to columns. This function transforms the DataFrame
such that the values in the first column become the new columns of the DataFrame.</p><p>This is equivalent to calling <code>Dataset#transpose(Column)</code> where <code>indexColumn</code> is set to the
first column.</p><p>Please note:</p><ul><li>All columns except the index column must share a least common data type. Unless they are
    the same data type, all columns are cast to the nearest common data type.</li><li>The name of the column into which the original column names are transposed defaults to
    "key".</li><li>Non-"key" column names for the transposed table are ordered in ascending order.
</li></ul></div><dl class="attributes block"><dt>Since</dt><dd><p>4.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#transpose" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="transpose(indexColumn:org.apache.spark.sql.Column):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="transpose(Column):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#transpose(indexColumn:org.apache.spark.sql.Column):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">transpose</span><span class="params">(<span name="indexColumn">indexColumn: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Transposes a DataFrame such that the values in the specified index column become the new
columns of the DataFrame.</p><div class="fullcomment"><div class="comment cmt"><p>Transposes a DataFrame such that the values in the specified index column become the new
columns of the DataFrame.</p><p>Please note:</p><ul><li>All columns except the index column must share a least common data type. Unless they are
    the same data type, all columns are cast to the nearest common data type.</li><li>The name of the column into which the original column names are transposed defaults to
    "key".</li><li>null values in the index column are excluded from the column names for the transposed
    table, which are ordered in ascending order.</li></ul><pre><span class="kw">val</span> df = <span class="std">Seq</span>((<span class="lit">"A"</span>, <span class="num">1</span>, <span class="num">2</span>), (<span class="lit">"B"</span>, <span class="num">3</span>, <span class="num">4</span>)).toDF(<span class="lit">"id"</span>, <span class="lit">"val1"</span>, <span class="lit">"val2"</span>)
df.show()
<span class="cmt">// output:</span>
<span class="cmt">// +---+----+----+</span>
<span class="cmt">// | id|val1|val2|</span>
<span class="cmt">// +---+----+----+</span>
<span class="cmt">// |  A|   1|   2|</span>
<span class="cmt">// |  B|   3|   4|</span>
<span class="cmt">// +---+----+----+</span>

df.transpose($<span class="lit">"id"</span>).show()
<span class="cmt">// output:</span>
<span class="cmt">// +----+---+---+</span>
<span class="cmt">// | key|  A|  B|</span>
<span class="cmt">// +----+---+---+</span>
<span class="cmt">// |val1|  1|  3|</span>
<span class="cmt">// |val2|  2|  4|</span>
<span class="cmt">// +----+---+---+</span>
<span class="cmt">// schema:</span>
<span class="cmt">// root</span>
<span class="cmt">//  |-- key: string (nullable = false)</span>
<span class="cmt">//  |-- A: integer (nullable = true)</span>
<span class="cmt">//  |-- B: integer (nullable = true)</span>

df.transpose().show()
<span class="cmt">// output:</span>
<span class="cmt">// +----+---+---+</span>
<span class="cmt">// | key|  A|  B|</span>
<span class="cmt">// +----+---+---+</span>
<span class="cmt">// |val1|  1|  3|</span>
<span class="cmt">// |val2|  2|  4|</span>
<span class="cmt">// +----+---+---+</span>
<span class="cmt">// schema:</span>
<span class="cmt">// root</span>
<span class="cmt">//  |-- key: string (nullable = false)</span>
<span class="cmt">//  |-- A: integer (nullable = true)</span>
<span class="cmt">//  |-- B: integer (nullable = true)</span></pre></div><dl class="paramcmts block"><dt class="param">indexColumn</dt><dd class="cmt"><p>
  The single column that will be treated as the index for the transpose operation. This
  column will be used to pivot the data, transforming the DataFrame such that the values of
  the indexColumn become the new columns in the transposed DataFrame.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>4.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#union" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="union(other:DS[T]):DS[T]" class="anchorToMember"></a><a id="union(DS[T]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#union(other:DS[T]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">union</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><p>This is equivalent to <code>UNION ALL</code> in SQL. To do a SQL-style set union (that does
deduplication of elements), use this function followed by a <a href="#distinct():DS[T]" name="org.apache.spark.sql.api.Dataset#distinct" id="org.apache.spark.sql.api.Dataset#distinct" class="extmbr">distinct</a>.</p><p>Also as standard in SQL, this function resolves columns by position (not by name):</p><pre><span class="kw">val</span> df1 = <span class="std">Seq</span>((<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>)).toDF(<span class="lit">"col0"</span>, <span class="lit">"col1"</span>, <span class="lit">"col2"</span>)
<span class="kw">val</span> df2 = <span class="std">Seq</span>((<span class="num">4</span>, <span class="num">5</span>, <span class="num">6</span>)).toDF(<span class="lit">"col1"</span>, <span class="lit">"col2"</span>, <span class="lit">"col0"</span>)
df1.union(df2).show

<span class="cmt">// output:</span>
<span class="cmt">// +----+----+----+</span>
<span class="cmt">// |col0|col1|col2|</span>
<span class="cmt">// +----+----+----+</span>
<span class="cmt">// |   1|   2|   3|</span>
<span class="cmt">// |   4|   5|   6|</span>
<span class="cmt">// +----+----+----+</span></pre><p>Notice that the column positions in the schema aren't necessarily matched with the fields in
the strongly typed objects in a Dataset. This function resolves columns by their positions in
the schema, not the fields in the strongly typed objects. Use <a href="#unionByName(other:DS[T]):DS[T]" name="org.apache.spark.sql.api.Dataset#unionByName" id="org.apache.spark.sql.api.Dataset#unionByName" class="extmbr">unionByName</a> to resolve
columns by field name in the typed objects.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#unionByName" group="typedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="unionByName(other:DS[T],allowMissingColumns:Boolean):DS[T]" class="anchorToMember"></a><a id="unionByName(DS[T],Boolean):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#unionByName(other:DS[T],allowMissingColumns:Boolean):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unionByName</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>, <span name="allowMissingColumns">allowMissingColumns: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><p>The difference between this function and <a href="#union(other:DS[T]):DS[T]" name="org.apache.spark.sql.api.Dataset#union" id="org.apache.spark.sql.api.Dataset#union" class="extmbr">union</a> is that this function resolves columns by
name (not by position).</p><p>When the parameter <code>allowMissingColumns</code> is <code>true</code>, the set of column names in this and other
<code>Dataset</code> can differ; missing columns will be filled with null. Further, the missing columns
of this <code>Dataset</code> will be added at the end in the schema of the union result:</p><pre><span class="kw">val</span> df1 = <span class="std">Seq</span>((<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>)).toDF(<span class="lit">"col0"</span>, <span class="lit">"col1"</span>, <span class="lit">"col2"</span>)
<span class="kw">val</span> df2 = <span class="std">Seq</span>((<span class="num">4</span>, <span class="num">5</span>, <span class="num">6</span>)).toDF(<span class="lit">"col1"</span>, <span class="lit">"col0"</span>, <span class="lit">"col3"</span>)
df1.unionByName(df2, <span class="kw">true</span>).show

<span class="cmt">// output: "col3" is missing at left df1 and added at the end of schema.</span>
<span class="cmt">// +----+----+----+----+</span>
<span class="cmt">// |col0|col1|col2|col3|</span>
<span class="cmt">// +----+----+----+----+</span>
<span class="cmt">// |   1|   2|   3|NULL|</span>
<span class="cmt">// |   5|   4|NULL|   6|</span>
<span class="cmt">// +----+----+----+----+</span>

df2.unionByName(df1, <span class="kw">true</span>).show

<span class="cmt">// output: "col2" is missing at left df2 and added at the end of schema.</span>
<span class="cmt">// +----+----+----+----+</span>
<span class="cmt">// |col1|col0|col3|col2|</span>
<span class="cmt">// +----+----+----+----+</span>
<span class="cmt">// |   4|   5|   6|NULL|</span>
<span class="cmt">// |   2|   1|NULL|   3|</span>
<span class="cmt">// +----+----+----+----+</span></pre><p>Note that this supports nested columns in struct and array types. With <code>allowMissingColumns</code>,
missing nested columns of struct columns with the same name will also be filled with null
values and added to the end of struct. Nested columns in map types are not currently
supported.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.1.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#unpersist" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="unpersist():DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#unpersist():DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unpersist</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</p><div class="fullcomment"><div class="comment cmt"><p>Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. This
will not un-persist any cached data that is built upon this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#unpersist" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="unpersist(blocking:Boolean):DS[T]" class="anchorToMember"></a><a id="unpersist(Boolean):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#unpersist(blocking:Boolean):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unpersist</span><span class="params">(<span name="blocking">blocking: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.</p><div class="fullcomment"><div class="comment cmt"><p>Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk. This
will not un-persist any cached data that is built upon this Dataset.
</p></div><dl class="paramcmts block"><dt class="param">blocking</dt><dd class="cmt"><p>
  Whether to block until all blocks are deleted.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#unpivot" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="unpivot(ids:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="unpivot(Array[Column],String,String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#unpivot(ids:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unpivot</span><span class="params">(<span name="ids">ids: <span name="scala.Array" class="extype">Array</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="variableColumnName">variableColumnName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="valueColumnName">valueColumnName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns
set.</p><div class="fullcomment"><div class="comment cmt"><p>Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns
set. This is the reverse to <code>groupBy(...).pivot(...).agg(...)</code>, except for the aggregation,
which cannot be reversed.
</p></div><dl class="paramcmts block"><dt class="param">ids</dt><dd class="cmt"><p>
  Id columns</p></dd><dt class="param">variableColumnName</dt><dd class="cmt"><p>
  Name of the variable column</p></dd><dt class="param">valueColumnName</dt><dd class="cmt"><p>
  Name of the value column</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd><dt>See also</dt><dd><span class="cmt"><p>
  <code>org.apache.spark.sql.Dataset.unpivot(Array, Array, String, String)</code>
This is equivalent to calling <code>Dataset#unpivot(Array, Array, String, String)</code> where <code>values</code>
is set to all non-id columns that exist in the DataFrame.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#unpivot" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="unpivot(ids:Array[org.apache.spark.sql.Column],values:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="unpivot(Array[Column],Array[Column],String,String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#unpivot(ids:Array[org.apache.spark.sql.Column],values:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unpivot</span><span class="params">(<span name="ids">ids: <span name="scala.Array" class="extype">Array</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="values">values: <span name="scala.Array" class="extype">Array</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="variableColumnName">variableColumnName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="valueColumnName">valueColumnName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns
set.</p><div class="fullcomment"><div class="comment cmt"><p>Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns
set. This is the reverse to <code>groupBy(...).pivot(...).agg(...)</code>, except for the aggregation,
which cannot be reversed.</p><p>This function is useful to massage a DataFrame into a format where some columns are
identifier columns ("ids"), while all other columns ("values") are "unpivoted" to the rows,
leaving just two non-id columns, named as given by <code>variableColumnName</code> and
<code>valueColumnName</code>.</p><pre><span class="kw">val</span> df = <span class="std">Seq</span>((<span class="num">1</span>, <span class="num">11</span>, <span class="num">12</span>L), (<span class="num">2</span>, <span class="num">21</span>, <span class="num">22</span>L)).toDF(<span class="lit">"id"</span>, <span class="lit">"int"</span>, <span class="lit">"long"</span>)
df.show()
<span class="cmt">// output:</span>
<span class="cmt">// +---+---+----+</span>
<span class="cmt">// | id|int|long|</span>
<span class="cmt">// +---+---+----+</span>
<span class="cmt">// |  1| 11|  12|</span>
<span class="cmt">// |  2| 21|  22|</span>
<span class="cmt">// +---+---+----+</span>

df.unpivot(<span class="std">Array</span>($<span class="lit">"id"</span>), <span class="std">Array</span>($<span class="lit">"int"</span>, $<span class="lit">"long"</span>), <span class="lit">"variable"</span>, <span class="lit">"value"</span>).show()
<span class="cmt">// output:</span>
<span class="cmt">// +---+--------+-----+</span>
<span class="cmt">// | id|variable|value|</span>
<span class="cmt">// +---+--------+-----+</span>
<span class="cmt">// |  1|     int|   11|</span>
<span class="cmt">// |  1|    long|   12|</span>
<span class="cmt">// |  2|     int|   21|</span>
<span class="cmt">// |  2|    long|   22|</span>
<span class="cmt">// +---+--------+-----+</span>
<span class="cmt">// schema:</span>
<span class="cmt">//root</span>
<span class="cmt">// |-- id: integer (nullable = false)</span>
<span class="cmt">// |-- variable: string (nullable = false)</span>
<span class="cmt">// |-- value: long (nullable = true)</span></pre><p>When no "id" columns are given, the unpivoted DataFrame consists of only the "variable" and
"value" columns.</p><p>All "value" columns must share a least common data type. Unless they are the same data type,
all "value" columns are cast to the nearest common data type. For instance, types
<code>IntegerType</code> and <code>LongType</code> are cast to <code>LongType</code>, while <code>IntegerType</code> and <code>StringType</code> do
not have a common data type and <code>unpivot</code> fails with an <code>AnalysisException</code>.
</p></div><dl class="paramcmts block"><dt class="param">ids</dt><dd class="cmt"><p>
  Id columns</p></dd><dt class="param">values</dt><dd class="cmt"><p>
  Value columns to unpivot</p></dd><dt class="param">variableColumnName</dt><dd class="cmt"><p>
  Name of the variable column</p></dd><dt class="param">valueColumnName</dt><dd class="cmt"><p>
  Name of the value column</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#withColumns" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="prt"><a id="withColumns(colNames:Seq[String],cols:Seq[org.apache.spark.sql.Column]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="withColumns(Seq[String],Seq[Column]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#withColumns(colNames:Seq[String],cols:Seq[org.apache.spark.sql.Column]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumns</span><span class="params">(<span name="colNames">colNames: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>, <span name="cols">cols: <span name="scala.Seq" class="extype">Seq</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Returns a new Dataset by adding columns or replacing the existing columns that has the same
names.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by adding columns or replacing the existing columns that has the same
names.
</p></div><dl class="attributes block"><dt>Attributes</dt><dd>protected </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#withColumnsRenamed" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="prt"><a id="withColumnsRenamed(colNames:Seq[String],newColNames:Seq[String]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="withColumnsRenamed(Seq[String],Seq[String]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#withColumnsRenamed(colNames:Seq[String],newColNames:Seq[String]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumnsRenamed</span><span class="params">(<span name="colNames">colNames: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>, <span name="newColNames">newColNames: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><div class="fullcomment"><dl class="attributes block"><dt>Attributes</dt><dd>protected </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#withMetadata" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="withMetadata(columnName:String,metadata:org.apache.spark.sql.types.Metadata):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="withMetadata(String,Metadata):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#withMetadata(columnName:String,metadata:org.apache.spark.sql.types.Metadata):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withMetadata</span><span class="params">(<span name="columnName">columnName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="metadata">metadata: <a href="../types/Metadata.html" name="org.apache.spark.sql.types.Metadata" id="org.apache.spark.sql.types.Metadata" class="extype">Metadata</a></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Returns a new Dataset by updating an existing column with metadata.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by updating an existing column with metadata.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#withWatermark" group="streaming" fullComment="yes" data-isabs="true" visbl="pub"><a id="withWatermark(eventTime:String,delayThreshold:String):DS[T]" class="anchorToMember"></a><a id="withWatermark(String,String):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#withWatermark(eventTime:String,delayThreshold:String):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withWatermark</span><span class="params">(<span name="eventTime">eventTime: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="delayThreshold">delayThreshold: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Defines an event time watermark for this <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Defines an event time watermark for this <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>. A watermark tracks a point in time
before which we assume no more late data is going to arrive.</p><p>Spark will use this watermark for several purposes:</p><ul><li>To know when a given time window
aggregation can be finalized and thus can be emitted when using output modes that do not
allow updates.</li><li>To minimize the amount of state that we need to keep for on-going
aggregations, <code>mapGroupsWithState</code> and <code>dropDuplicates</code> operators.  The current
watermark is computed by looking at the <code>MAX(eventTime)</code> seen across all of the partitions in
the query minus a user specified <code>delayThreshold</code>. Due to the cost of coordinating this value
across partitions, the actual watermark used is only guaranteed to be at least
<code>delayThreshold</code> behind the actual event time. In some cases we may still process records
that arrive more than <code>delayThreshold</code> late.
</li></ul></div><dl class="paramcmts block"><dt class="param">eventTime</dt><dd class="cmt"><p>
  the name of the column that contains the event time of the row.</p></dd><dt class="param">delayThreshold</dt><dd class="cmt"><p>
  the minimum delay to wait to data to arrive late, relative to the latest record that has
  been processed in the form of an interval (e.g. "1 minute" or "5 hours"). NOTE: This should
  not be negative.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.1.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#write" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="write:org.apache.spark.sql.DataFrameWriter[T]" class="anchorToMember"></a><a id="write:DataFrameWriter[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#write:org.apache.spark.sql.DataFrameWriter[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">write</span><span class="result">: <a href="../DataFrameWriter.html" name="org.apache.spark.sql.DataFrameWriter" id="org.apache.spark.sql.DataFrameWriter" class="extype">DataFrameWriter</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Interface for saving the content of the non-streaming Dataset out into external storage.</p><div class="fullcomment"><div class="comment cmt"><p>Interface for saving the content of the non-streaming Dataset out into external storage.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#writeTo" group="basic" fullComment="yes" data-isabs="true" visbl="pub"><a id="writeTo(table:String):org.apache.spark.sql.DataFrameWriterV2[T]" class="anchorToMember"></a><a id="writeTo(String):DataFrameWriterV2[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#writeTo(table:String):org.apache.spark.sql.DataFrameWriterV2[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">writeTo</span><span class="params">(<span name="table">table: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="../DataFrameWriterV2.html" name="org.apache.spark.sql.DataFrameWriterV2" id="org.apache.spark.sql.DataFrameWriterV2" class="extype">DataFrameWriterV2</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Create a write configuration builder for v2 sources.</p><div class="fullcomment"><div class="comment cmt"><p>Create a write configuration builder for v2 sources.</p><p>This builder is used to configure and execute write operations. For example, to append to an
existing table, run:</p><pre>df.writeTo(<span class="lit">"catalog.db.table"</span>).append()</pre><p>This can also be used to create or replace existing tables:</p><pre>df.writeTo(<span class="lit">"catalog.db.table"</span>).partitionedBy($<span class="lit">"col"</span>).createOrReplace()</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#explode" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="explode[A,B](inputColumn:String,outputColumn:String)(f:A=&gt;IterableOnce[B])(implicitevidence$3:reflect.runtime.universe.TypeTag[B]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="explode[A,B](String,String)((A)=&gt;IterableOnce[B])(scala.reflect.api.JavaUniverse.TypeTag[B]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#explode[A,B](inputColumn:String,outputColumn:String)(f:A=&gt;IterableOnce[B])(implicitevidence$3:reflect.runtime.universe.TypeTag[B]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name deprecated" title="Deprecated: (Since version 2.0.0) use flatMap() or select() with functions.explode() instead">explode</span><span class="tparams">[<span name="A">A</span>, <span name="B">B</span>]</span><span class="params">(<span name="inputColumn">inputColumn: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="outputColumn">outputColumn: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="params">(<span name="f">f: (<span name="org.apache.spark.sql.api.Dataset.explode.A" class="extype">A</span>) =&gt; <span name="scala.IterableOnce" class="extype">IterableOnce</span>[<span name="org.apache.spark.sql.api.Dataset.explode.B" class="extype">B</span>]</span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <span name="scala.reflect.api.TypeTags.TypeTag" class="extype">scala.reflect.api.JavaUniverse.TypeTag</span>[<span name="org.apache.spark.sql.api.Dataset.explode.B" class="extype">B</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset where a single column has been expanded to zero or
more rows by the provided function.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset where a single column has been expanded to zero or
more rows by the provided function. This is similar to a <code>LATERAL VIEW</code> in HiveQL. All
columns of the input row are implicitly joined with each value that is output by the
function.</p><p>Given that this is deprecated, as an alternative, you can explode columns either using
<code>functions.explode()</code>:</p><pre>ds.select(explode(split($<span class="lit">"words"</span>, <span class="lit">" "</span>)).as(<span class="lit">"word"</span>))</pre><p>or <code>flatMap()</code>:</p><pre>ds.flatMap(_.words.split(<span class="lit">" "</span>))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@deprecated</span> </dd><dt>Deprecated</dt><dd class="cmt"><p><i>(Since version 2.0.0)</i> use flatMap() or select() with functions.explode() instead</p></dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#explode" group="untypedrel" fullComment="yes" data-isabs="true" visbl="pub"><a id="explode[A&lt;:Product](input:org.apache.spark.sql.Column*)(f:org.apache.spark.sql.Row=&gt;IterableOnce[A])(implicitevidence$2:reflect.runtime.universe.TypeTag[A]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="explode[A&lt;:Product](Column*)((Row)=&gt;IterableOnce[A])(scala.reflect.api.JavaUniverse.TypeTag[A]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#explode[A&lt;:Product](input:org.apache.spark.sql.Column*)(f:org.apache.spark.sql.Row=&gt;IterableOnce[A])(implicitevidence$2:reflect.runtime.universe.TypeTag[A]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">def</span></span> <span class="symbol"><span class="name deprecated" title="Deprecated: (Since version 2.0.0) use flatMap() or select() with functions.explode() instead">explode</span><span class="tparams">[<span name="A">A &lt;: <span name="scala.Product" class="extype">Product</span></span>]</span><span class="params">(<span name="input">input: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="params">(<span name="f">f: (<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>) =&gt; <span name="scala.IterableOnce" class="extype">IterableOnce</span>[<span name="org.apache.spark.sql.api.Dataset.explode.A" class="extype">A</span>]</span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <span name="scala.reflect.api.TypeTags.TypeTag" class="extype">scala.reflect.api.JavaUniverse.TypeTag</span>[<span name="org.apache.spark.sql.api.Dataset.explode.A" class="extype">A</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset where each row has been expanded to zero or more rows
by the provided function.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset where each row has been expanded to zero or more rows
by the provided function. This is similar to a <code>LATERAL VIEW</code> in HiveQL. The columns of the
input row are implicitly joined with each row that is output by the function.</p><p>Given that this is deprecated, as an alternative, you can explode columns either using
<code>functions.explode()</code> or <code>flatMap()</code>. The following example uses these alternatives to count
the number of books that contain a given word:</p><pre><span class="kw">case</span> <span class="kw">class</span> Book(title: <span class="std">String</span>, words: <span class="std">String</span>)
<span class="kw">val</span> ds: DS[Book]

<span class="kw">val</span> allWords = ds.select($<span class="lit">"title"</span>, explode(split($<span class="lit">"words"</span>, <span class="lit">" "</span>)).as(<span class="lit">"word"</span>))

<span class="kw">val</span> bookCountPerWord = allWords.groupBy(<span class="lit">"word"</span>).agg(count_distinct(<span class="lit">"title"</span>))</pre><p>Using <code>flatMap()</code> this can similarly be exploded as:</p><pre>ds.flatMap(_.words.split(<span class="lit">" "</span>))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@deprecated</span> </dd><dt>Deprecated</dt><dd class="cmt"><p><i>(Since version 2.0.0)</i> use flatMap() or select() with functions.explode() instead</p></dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li></ol></div><div class="values members"><h3>Concrete Value Members</h3><ol><li class="indented0 " name="scala.AnyRef#!=" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="!=(x$1:Any):Boolean" class="anchorToMember"></a><a id="!=(Any):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#!=(x$1:Any):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name" title="gt4s: $bang$eq">!=</span><span class="params">(<span name="arg0">arg0: <span name="scala.Any" class="extype">Any</span></span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div></li><li class="indented0 " name="scala.AnyRef###" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="##:Int" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html###:Int" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name" title="gt4s: $hash$hash">##</span><span class="result">: <span name="scala.Int" class="extype">Int</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div></li><li class="indented0 " name="scala.AnyRef#==" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="==(x$1:Any):Boolean" class="anchorToMember"></a><a id="==(Any):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#==(x$1:Any):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name" title="gt4s: $eq$eq">==</span><span class="params">(<span name="arg0">arg0: <span name="scala.Any" class="extype">Any</span></span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#agg" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="agg(expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="agg(Column,Column*):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#agg(expr:org.apache.spark.sql.Column,exprs:org.apache.spark.sql.Column*):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">agg</span><span class="params">(<span name="expr">expr: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>, <span name="exprs">exprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Aggregates on the entire Dataset without groups.</p><div class="fullcomment"><div class="comment cmt"><p>Aggregates on the entire Dataset without groups.</p><pre><span class="cmt">// ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span>
ds.agg(max($<span class="lit">"age"</span>), avg($<span class="lit">"salary"</span>))
ds.groupBy().agg(max($<span class="lit">"age"</span>), avg($<span class="lit">"salary"</span>))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#agg" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="agg(exprs:java.util.Map[String,String]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="agg(Map[String,String]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#agg(exprs:java.util.Map[String,String]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">agg</span><span class="params">(<span name="exprs">exprs: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Map.html#java.util.Map" name="java.util.Map" id="java.util.Map" class="extype">Map</a>[<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Java-specific) Aggregates on the entire Dataset without groups.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Aggregates on the entire Dataset without groups.</p><pre><span class="cmt">// ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span>
ds.agg(<span class="std">Map</span>(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>))
ds.groupBy().agg(<span class="std">Map</span>(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>))</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#agg" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="agg(exprs:Map[String,String]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="agg(Map[String,String]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#agg(exprs:Map[String,String]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">agg</span><span class="params">(<span name="exprs">exprs: <span name="scala.Predef.Map" class="extype">Map</span>[<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Scala-specific) Aggregates on the entire Dataset without groups.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Aggregates on the entire Dataset without groups.</p><pre><span class="cmt">// ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span>
ds.agg(<span class="std">Map</span>(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>))
ds.groupBy().agg(<span class="std">Map</span>(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>))</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#agg" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="agg(aggExpr:(String,String),aggExprs:(String,String)*):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="agg((String,String),(String,String)*):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#agg(aggExpr:(String,String),aggExprs:(String,String)*):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">agg</span><span class="params">(<span name="aggExpr">aggExpr: (<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>)</span>, <span name="aggExprs">aggExprs: (<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>)*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Scala-specific) Aggregates on the entire Dataset without groups.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Aggregates on the entire Dataset without groups.</p><pre><span class="cmt">// ds.agg(...) is a shorthand for ds.groupBy().agg(...)</span>
ds.agg(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>)
ds.groupBy().agg(<span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>, <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#alias" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="alias(alias:Symbol):DS[T]" class="anchorToMember"></a><a id="alias(Symbol):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#alias(alias:Symbol):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">alias</span><span class="params">(<span name="alias">alias: <span name="scala.Symbol" class="extype">Symbol</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset with an alias set.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset with an alias set. Same as <code>as</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#alias" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="alias(alias:String):DS[T]" class="anchorToMember"></a><a id="alias(String):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#alias(alias:String):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">alias</span><span class="params">(<span name="alias">alias: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with an alias set.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with an alias set. Same as <code>as</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#apply" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="apply(colName:String):org.apache.spark.sql.Column" class="anchorToMember"></a><a id="apply(String):Column" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#apply(colName:String):org.apache.spark.sql.Column" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">apply</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span></span><p class="shortcomment cmt">Selects column based on the column name and returns it as a <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a>.</p><div class="fullcomment"><div class="comment cmt"><p>Selects column based on the column name and returns it as a <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  The column name can also reference to a nested column like <code>a.b</code>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#as" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="as(alias:Symbol):DS[T]" class="anchorToMember"></a><a id="as(Symbol):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#as(alias:Symbol):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">as</span><span class="params">(<span name="alias">alias: <span name="scala.Symbol" class="extype">Symbol</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset with an alias set.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset with an alias set.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="scala.Any#asInstanceOf" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="asInstanceOf[T0]:T0" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#asInstanceOf[T0]:T0" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">asInstanceOf</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="result">: <span name="scala.Any.asInstanceOf.T0" class="extype">T0</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>Any</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#checkpoint" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="checkpoint(eager:Boolean):DS[T]" class="anchorToMember"></a><a id="checkpoint(Boolean):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#checkpoint(eager:Boolean):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">checkpoint</span><span class="params">(<span name="eager">eager: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a checkpointed version of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
logical plan of this Dataset, which is especially useful in iterative algorithms where the
plan may grow exponentially. It will be saved to files inside the checkpoint directory set
with <code>SparkContext#setCheckpointDir</code>.
</p></div><dl class="paramcmts block"><dt class="param">eager</dt><dd class="cmt"><p>
  Whether to checkpoint this dataframe immediately</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.1.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  When checkpoint is used with eager = false, the final data that is checkpointed after the
  first action may be different from the data that was used during the job due to
  non-determinism of the underlying operation and retries. If checkpoint is used to achieve
  saving a deterministic snapshot of the data, eager = true should be used. Otherwise, it is
  only deterministic after the first execution, after the checkpoint was finalized.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#checkpoint" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="checkpoint():DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#checkpoint():DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">checkpoint</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Eagerly checkpoint a Dataset and return the new Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to
truncate the logical plan of this Dataset, which is especially useful in iterative algorithms
where the plan may grow exponentially. It will be saved to files inside the checkpoint
directory set with <code>SparkContext#setCheckpointDir</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.1.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#clone" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="prt"><a id="clone():Object" class="anchorToMember"></a><a id="clone():AnyRef" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#clone():Object" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">clone</span><span class="params">()</span><span class="result">: <span name="scala.AnyRef" class="extype">AnyRef</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Attributes</dt><dd>protected[<span name="java.lang" class="extype">lang</span>] </dd><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">classOf[java.lang.CloneNotSupportedException]</span></span>)</span> <span class="name">@IntrinsicCandidate</span><span class="args">()</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#columns" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="columns:Array[String]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#columns:Array[String]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">columns</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span></span><p class="shortcomment cmt">Returns all column names as an array.</p><div class="fullcomment"><div class="comment cmt"><p>Returns all column names as an array.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#createGlobalTempView" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="createGlobalTempView(viewName:String):Unit" class="anchorToMember"></a><a id="createGlobalTempView(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#createGlobalTempView(viewName:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">createGlobalTempView</span><span class="params">(<span name="viewName">viewName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Creates a global temporary view using the given name.</p><div class="fullcomment"><div class="comment cmt"><p>Creates a global temporary view using the given name. The lifetime of this temporary view is
tied to this Spark application.</p><p>Global temporary view is cross-session. Its lifetime is the lifetime of the Spark
application, i.e. it will be automatically dropped when the application terminates. It's tied
to a system preserved database <code>global_temp</code>, and we must use the qualified name to refer a
global temp view, e.g. <code>SELECT * FROM global_temp.view1</code>.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">scala.this.throws.&lt;init&gt;$default$1[org.apache.spark.sql.AnalysisException]</span></span>)</span> </dd><dt>Since</dt><dd><p>2.1.0</p></dd><dt>Exceptions thrown</dt><dd><span class="cmt"><p><a href="../AnalysisException.html" name="org.apache.spark.sql.AnalysisException" id="org.apache.spark.sql.AnalysisException" class="extype"><code>org.apache.spark.sql.AnalysisException</code></a> 
  if the view name is invalid or already exists</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#createOrReplaceGlobalTempView" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="createOrReplaceGlobalTempView(viewName:String):Unit" class="anchorToMember"></a><a id="createOrReplaceGlobalTempView(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#createOrReplaceGlobalTempView(viewName:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">createOrReplaceGlobalTempView</span><span class="params">(<span name="viewName">viewName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Creates or replaces a global temporary view using the given name.</p><div class="fullcomment"><div class="comment cmt"><p>Creates or replaces a global temporary view using the given name. The lifetime of this
temporary view is tied to this Spark application.</p><p>Global temporary view is cross-session. Its lifetime is the lifetime of the Spark
application, i.e. it will be automatically dropped when the application terminates. It's tied
to a system preserved database <code>global_temp</code>, and we must use the qualified name to refer a
global temp view, e.g. <code>SELECT * FROM global_temp.view1</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.2.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#createOrReplaceTempView" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="createOrReplaceTempView(viewName:String):Unit" class="anchorToMember"></a><a id="createOrReplaceTempView(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#createOrReplaceTempView(viewName:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">createOrReplaceTempView</span><span class="params">(<span name="viewName">viewName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Creates a local temporary view using the given name.</p><div class="fullcomment"><div class="comment cmt"><p>Creates a local temporary view using the given name. The lifetime of this temporary view is
tied to the <code>SparkSession</code> that was used to create this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#createTempView" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="createTempView(viewName:String):Unit" class="anchorToMember"></a><a id="createTempView(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#createTempView(viewName:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">createTempView</span><span class="params">(<span name="viewName">viewName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Creates a local temporary view using the given name.</p><div class="fullcomment"><div class="comment cmt"><p>Creates a local temporary view using the given name. The lifetime of this temporary view is
tied to the <code>SparkSession</code> that was used to create this Dataset.</p><p>Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
created it, i.e. it will be automatically dropped when the session terminates. It's not tied
to any databases, i.e. we can't use <code>db1.view1</code> to reference a local temporary view.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">scala.this.throws.&lt;init&gt;$default$1[org.apache.spark.sql.AnalysisException]</span></span>)</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Exceptions thrown</dt><dd><span class="cmt"><p><a href="../AnalysisException.html" name="org.apache.spark.sql.AnalysisException" id="org.apache.spark.sql.AnalysisException" class="extype"><code>org.apache.spark.sql.AnalysisException</code></a> 
  if the view name is invalid or already exists</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#cube" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="cube(col1:String,cols:String*):Dataset.this.RGD" class="anchorToMember"></a><a id="cube(String,String*):RGD" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#cube(col1:String,cols:String*):Dataset.this.RGD" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">cube</span><span class="params">(<span name="col1">col1: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="#RGD&lt;:org.apache.spark.sql.api.RelationalGroupedDataset[DS]" name="org.apache.spark.sql.api.Dataset.RGD" id="org.apache.spark.sql.api.Dataset.RGD" class="extmbr">RGD</a></span></span><p class="shortcomment cmt">Create a multi-dimensional cube for the current Dataset using the specified columns, so we
can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Create a multi-dimensional cube for the current Dataset using the specified columns, so we
can run aggregation on them. See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.api.RelationalGroupedDataset" id="org.apache.spark.sql.api.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate
functions.</p><p>This is a variant of cube that can only group by existing columns using column names (i.e.
cannot construct expressions).</p><pre><span class="cmt">// Compute the average for all numeric columns cubed by department and group.</span>
ds.cube(<span class="lit">"department"</span>, <span class="lit">"group"</span>).avg()

<span class="cmt">// Compute the max age and average salary, cubed by department and gender.</span>
ds.cube($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#distinct" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="distinct():DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#distinct():DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">distinct</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset that contains only the unique rows from this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset that contains only the unique rows from this Dataset. This is an alias
for <code>dropDuplicates</code>.</p><p>Note that for a streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, this method returns distinct rows only once regardless
of the output mode, which the behavior may not be same with <code>DISTINCT</code> in SQL against
streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  Equality checking is performed directly on the encoded representation of the data and thus
  is not affected by a custom <code>equals</code> function defined on <code>T</code>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#drop" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="drop(col:org.apache.spark.sql.Column):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="drop(Column):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#drop(col:org.apache.spark.sql.Column):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">drop</span><span class="params">(<span name="col">col: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Returns a new Dataset with column dropped.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with column dropped.</p><p>This method can only be used to drop top level column. This version of drop accepts a
<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> rather than a name. This is a no-op if the Dataset doesn't
have a column with an equivalent expression.</p><p>Note: <code>drop(col(colName))</code> has different semantic with <code>drop(colName)</code>, please refer to
<code>Dataset#drop(colName: String)</code>.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#drop" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="drop(colName:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="drop(String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#drop(colName:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">drop</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Returns a new Dataset with a column dropped.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain column
name.</p><p>This method can only be used to drop top level columns. the colName string is treated
literally without further interpretation.</p><p>Note: <code>drop(colName)</code> has different semantic with <code>drop(col(colName))</code>, for example: 1, multi
column have the same colName:</p><pre><span class="kw">val</span> df1 = spark.range(<span class="num">0</span>, <span class="num">2</span>).withColumn(<span class="lit">"key1"</span>, lit(<span class="num">1</span>))
<span class="kw">val</span> df2 = spark.range(<span class="num">0</span>, <span class="num">2</span>).withColumn(<span class="lit">"key2"</span>, lit(<span class="num">2</span>))
<span class="kw">val</span> df3 = df1.join(df2)

df3.show
<span class="cmt">// +---+----+---+----+</span>
<span class="cmt">// | id|key1| id|key2|</span>
<span class="cmt">// +---+----+---+----+</span>
<span class="cmt">// |  0|   1|  0|   2|</span>
<span class="cmt">// |  0|   1|  1|   2|</span>
<span class="cmt">// |  1|   1|  0|   2|</span>
<span class="cmt">// |  1|   1|  1|   2|</span>
<span class="cmt">// +---+----+---+----+</span>

df3.drop(<span class="lit">"id"</span>).show()
<span class="cmt">// output: the two 'id' columns are both dropped.</span>
<span class="cmt">// |key1|key2|</span>
<span class="cmt">// +----+----+</span>
<span class="cmt">// |   1|   2|</span>
<span class="cmt">// |   1|   2|</span>
<span class="cmt">// |   1|   2|</span>
<span class="cmt">// |   1|   2|</span>
<span class="cmt">// +----+----+</span>

df3.drop(col(<span class="lit">"id"</span>)).show()
<span class="cmt">// ...AnalysisException: [AMBIGUOUS_REFERENCE] Reference `id` is ambiguous...</span></pre><p>2, colName contains special characters, like dot.</p><pre><span class="kw">val</span> df = spark.range(<span class="num">0</span>, <span class="num">2</span>).withColumn(<span class="lit">"a.b.c"</span>, lit(<span class="num">1</span>))

df.show()
<span class="cmt">// +---+-----+</span>
<span class="cmt">// | id|a.b.c|</span>
<span class="cmt">// +---+-----+</span>
<span class="cmt">// |  0|    1|</span>
<span class="cmt">// |  1|    1|</span>
<span class="cmt">// +---+-----+</span>

df.drop(<span class="lit">"a.b.c"</span>).show()
<span class="cmt">// +---+</span>
<span class="cmt">// | id|</span>
<span class="cmt">// +---+</span>
<span class="cmt">// |  0|</span>
<span class="cmt">// |  1|</span>
<span class="cmt">// +---+</span>

df.drop(col(<span class="lit">"a.b.c"</span>)).show()
<span class="cmt">// no column match the expression 'a.b.c'</span>
<span class="cmt">// +---+-----+</span>
<span class="cmt">// | id|a.b.c|</span>
<span class="cmt">// +---+-----+</span>
<span class="cmt">// |  0|    1|</span>
<span class="cmt">// |  1|    1|</span>
<span class="cmt">// +---+-----+</span></pre></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#dropDuplicates" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicates(col1:String,cols:String*):DS[T]" class="anchorToMember"></a><a id="dropDuplicates(String,String*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#dropDuplicates(col1:String,cols:String*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicates</span><span class="params">(<span name="col1">col1: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> with duplicate rows removed, considering only the subset of
columns.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> with duplicate rows removed, considering only the subset of
columns.</p><p>For a static batch <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, it just drops duplicate rows. For a streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, it
will keep all data across triggers as intermediate state to drop duplicates rows. You can use
<a href="#withWatermark(eventTime:String,delayThreshold:String):DS[T]" name="org.apache.spark.sql.api.Dataset#withWatermark" id="org.apache.spark.sql.api.Dataset#withWatermark" class="extmbr">withWatermark</a> to limit how late the duplicate data can be and system will accordingly
limit the state. In addition, too late data older than watermark will be dropped to avoid any
possibility of duplicates.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#dropDuplicates" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicates(colNames:Array[String]):DS[T]" class="anchorToMember"></a><a id="dropDuplicates(Array[String]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#dropDuplicates(colNames:Array[String]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicates</span><span class="params">(<span name="colNames">colNames: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with duplicate rows removed, considering only the subset of columns.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with duplicate rows removed, considering only the subset of columns.</p><p>For a static batch <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, it just drops duplicate rows. For a streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, it
will keep all data across triggers as intermediate state to drop duplicates rows. You can use
<a href="#withWatermark(eventTime:String,delayThreshold:String):DS[T]" name="org.apache.spark.sql.api.Dataset#withWatermark" id="org.apache.spark.sql.api.Dataset#withWatermark" class="extmbr">withWatermark</a> to limit how late the duplicate data can be and system will accordingly
limit the state. In addition, too late data older than watermark will be dropped to avoid any
possibility of duplicates.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#dropDuplicatesWithinWatermark" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicatesWithinWatermark(col1:String,cols:String*):DS[T]" class="anchorToMember"></a><a id="dropDuplicatesWithinWatermark(String,String*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#dropDuplicatesWithinWatermark(col1:String,cols:String*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicatesWithinWatermark</span><span class="params">(<span name="col1">col1: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><p>This only works with streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, and watermark for the input <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> must be
set via <a href="#withWatermark(eventTime:String,delayThreshold:String):DS[T]" name="org.apache.spark.sql.api.Dataset#withWatermark" id="org.apache.spark.sql.api.Dataset#withWatermark" class="extmbr">withWatermark</a>.</p><p>For a streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, this will keep all data across triggers as intermediate state to
drop duplicated rows. The state will be kept to guarantee the semantic, "Events are
deduplicated as long as the time distance of earliest and latest events are smaller than the
delay threshold of watermark." Users are encouraged to set the delay threshold of watermark
longer than max timestamp differences among duplicated events.</p><p>Note: too late data older than watermark will be dropped.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.5.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#dropDuplicatesWithinWatermark" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="dropDuplicatesWithinWatermark(colNames:Array[String]):DS[T]" class="anchorToMember"></a><a id="dropDuplicatesWithinWatermark(Array[String]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#dropDuplicatesWithinWatermark(colNames:Array[String]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dropDuplicatesWithinWatermark</span><span class="params">(<span name="colNames">colNames: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with duplicates rows removed, considering only the subset of columns,
within watermark.</p><p>This only works with streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, and watermark for the input <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> must be
set via <a href="#withWatermark(eventTime:String,delayThreshold:String):DS[T]" name="org.apache.spark.sql.api.Dataset#withWatermark" id="org.apache.spark.sql.api.Dataset#withWatermark" class="extmbr">withWatermark</a>.</p><p>For a streaming <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>, this will keep all data across triggers as intermediate state to
drop duplicated rows. The state will be kept to guarantee the semantic, "Events are
deduplicated as long as the time distance of earliest and latest events are smaller than the
delay threshold of watermark." Users are encouraged to set the delay threshold of watermark
longer than max timestamp differences among duplicated events.</p><p>Note: too late data older than watermark will be dropped.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.5.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#dtypes" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="dtypes:Array[(String,String)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#dtypes:Array[(String,String)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">dtypes</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[(<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>)]</span></span><p class="shortcomment cmt">Returns all column names and their data types as an array.</p><div class="fullcomment"><div class="comment cmt"><p>Returns all column names and their data types as an array.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#eq" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="eq(x$1:AnyRef):Boolean" class="anchorToMember"></a><a id="eq(AnyRef):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#eq(x$1:AnyRef):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">eq</span><span class="params">(<span name="arg0">arg0: <span name="scala.AnyRef" class="extype">AnyRef</span></span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd></dl></div></li><li class="indented0 " name="scala.AnyRef#equals" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="equals(x$1:Object):Boolean" class="anchorToMember"></a><a id="equals(AnyRef):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#equals(x$1:Object):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">equals</span><span class="params">(<span name="arg0">arg0: <span name="scala.AnyRef" class="extype">AnyRef</span></span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#explain" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="explain():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#explain():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">explain</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Prints the physical plan to the console for debugging purposes.</p><div class="fullcomment"><div class="comment cmt"><p>Prints the physical plan to the console for debugging purposes.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#explain" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="explain(extended:Boolean):Unit" class="anchorToMember"></a><a id="explain(Boolean):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#explain(extended:Boolean):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">explain</span><span class="params">(<span name="extended">extended: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Prints the plans (logical and physical) to the console for debugging purposes.</p><div class="fullcomment"><div class="comment cmt"><p>Prints the plans (logical and physical) to the console for debugging purposes.
</p></div><dl class="paramcmts block"><dt class="param">extended</dt><dd class="cmt"><p>
  default <code>false</code>. If <code>false</code>, prints only the physical plan.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#filter" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="filter(conditionExpr:String):DS[T]" class="anchorToMember"></a><a id="filter(String):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#filter(conditionExpr:String):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">filter</span><span class="params">(<span name="conditionExpr">conditionExpr: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Filters rows using the given SQL expression.</p><div class="fullcomment"><div class="comment cmt"><p>Filters rows using the given SQL expression.</p><pre>peopleDs.filter(<span class="lit">"age &gt; 15"</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#first" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="first():T" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#first():T" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">first</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span></span></span><p class="shortcomment cmt">Returns the first row.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the first row. Alias for head().
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#flatMap" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="flatMap[U](f:org.apache.spark.api.java.function.FlatMapFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):DS[U]" class="anchorToMember"></a><a id="flatMap[U](FlatMapFunction[T,U],Encoder[U]):DS[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#flatMap[U](f:org.apache.spark.api.java.function.FlatMapFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):DS[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">flatMap</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="f">f: <a href="../../api/java/function/FlatMapFunction.html" name="org.apache.spark.api.java.function.FlatMapFunction" id="org.apache.spark.api.java.function.FlatMapFunction" class="extype">FlatMapFunction</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.flatMap.U" class="extype">U</span>]</span>, <span name="encoder">encoder: <a href="../Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.api.Dataset.flatMap.U" class="extype">U</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.flatMap.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Java-specific) Returns a new Dataset by first applying a function to all elements of this
Dataset, and then flattening the results.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Returns a new Dataset by first applying a function to all elements of this
Dataset, and then flattening the results.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#flatMap" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="flatMap[U](func:T=&gt;IterableOnce[U])(implicitevidence$6:org.apache.spark.sql.Encoder[U]):DS[U]" class="anchorToMember"></a><a id="flatMap[U]((T)=&gt;IterableOnce[U])(Encoder[U]):DS[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#flatMap[U](func:T=&gt;IterableOnce[U])(implicitevidence$6:org.apache.spark.sql.Encoder[U]):DS[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">flatMap</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="func">func: (<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>) =&gt; <span name="scala.IterableOnce" class="extype">IterableOnce</span>[<span name="org.apache.spark.sql.api.Dataset.flatMap.U" class="extype">U</span>]</span>)</span><span class="params">(<span class="implicit">implicit </span><span name="arg0">arg0: <a href="../Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.api.Dataset.flatMap.U" class="extype">U</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.flatMap.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset by first applying a function to all elements of this
Dataset, and then flattening the results.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset by first applying a function to all elements of this
Dataset, and then flattening the results.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#foreach" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="foreach(func:org.apache.spark.api.java.function.ForeachFunction[T]):Unit" class="anchorToMember"></a><a id="foreach(ForeachFunction[T]):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#foreach(func:org.apache.spark.api.java.function.ForeachFunction[T]):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">foreach</span><span class="params">(<span name="func">func: <a href="../../api/java/function/ForeachFunction.html" name="org.apache.spark.api.java.function.ForeachFunction" id="org.apache.spark.api.java.function.ForeachFunction" class="extype">ForeachFunction</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">(Java-specific) Runs <code>func</code> on each element of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Runs <code>func</code> on each element of this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#foreach" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="foreach(f:T=&gt;Unit):Unit" class="anchorToMember"></a><a id="foreach((T)=&gt;Unit):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#foreach(f:T=&gt;Unit):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">foreach</span><span class="params">(<span name="f">f: (<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>) =&gt; <span name="scala.Unit" class="extype">Unit</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Applies a function <code>f</code> to all rows.</p><div class="fullcomment"><div class="comment cmt"><p>Applies a function <code>f</code> to all rows.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#foreachPartition" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="foreachPartition(func:org.apache.spark.api.java.function.ForeachPartitionFunction[T]):Unit" class="anchorToMember"></a><a id="foreachPartition(ForeachPartitionFunction[T]):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#foreachPartition(func:org.apache.spark.api.java.function.ForeachPartitionFunction[T]):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">foreachPartition</span><span class="params">(<span name="func">func: <a href="../../api/java/function/ForeachPartitionFunction.html" name="org.apache.spark.api.java.function.ForeachPartitionFunction" id="org.apache.spark.api.java.function.ForeachPartitionFunction" class="extype">ForeachPartitionFunction</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">(Java-specific) Runs <code>func</code> on each partition of this Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Runs <code>func</code> on each partition of this Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#getClass" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="getClass():Class[_]" class="anchorToMember"></a><a id="getClass():Class[_&lt;:AnyRef]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#getClass():Class[_]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">getClass</span><span class="params">()</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/lang/Class.html#java.lang.Class" name="java.lang.Class" id="java.lang.Class" class="extype">Class</a>[_ &lt;: <span name="scala.AnyRef" class="extype">AnyRef</span>]</span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd><dt>Annotations</dt><dd><span class="name">@IntrinsicCandidate</span><span class="args">()</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#groupBy" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="groupBy(col1:String,cols:String*):Dataset.this.RGD" class="anchorToMember"></a><a id="groupBy(String,String*):RGD" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#groupBy(col1:String,cols:String*):Dataset.this.RGD" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">groupBy</span><span class="params">(<span name="col1">col1: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="#RGD&lt;:org.apache.spark.sql.api.RelationalGroupedDataset[DS]" name="org.apache.spark.sql.api.Dataset.RGD" id="org.apache.spark.sql.api.Dataset.RGD" class="extmbr">RGD</a></span></span><p class="shortcomment cmt">Groups the Dataset using the specified columns, so that we can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Groups the Dataset using the specified columns, so that we can run aggregation on them. See
<a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.api.RelationalGroupedDataset" id="org.apache.spark.sql.api.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate functions.</p><p>This is a variant of groupBy that can only group by existing columns using column names (i.e.
cannot construct expressions).</p><pre><span class="cmt">// Compute the average for all numeric columns grouped by department.</span>
ds.groupBy(<span class="lit">"department"</span>).avg()

<span class="cmt">// Compute the max age and average salary, grouped by department and gender.</span>
ds.groupBy($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#hashCode" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="hashCode():Int" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#hashCode():Int" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">hashCode</span><span class="params">()</span><span class="result">: <span name="scala.Int" class="extype">Int</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd><dt>Annotations</dt><dd><span class="name">@IntrinsicCandidate</span><span class="args">()</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#head" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="head():T" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#head():T" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">head</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span></span></span><p class="shortcomment cmt">Returns the first row.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the first row.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="scala.Any#isInstanceOf" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="isInstanceOf[T0]:Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#isInstanceOf[T0]:Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">isInstanceOf</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>Any</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:DS[_],joinExprs:org.apache.spark.sql.Column):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="join(DS[_],Column):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#join(right:DS[_],joinExprs:org.apache.spark.sql.Column):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span>, <span name="joinExprs">joinExprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Inner join with another <code>DataFrame</code>, using the given join expression.</p><div class="fullcomment"><div class="comment cmt"><p>Inner join with another <code>DataFrame</code>, using the given join expression.</p><pre><span class="cmt">// The following two are equivalent:</span>
df1.join(df2, $<span class="lit">"df1Key"</span> === $<span class="lit">"df2Key"</span>)
df1.join(df2).where($<span class="lit">"df1Key"</span> === $<span class="lit">"df2Key"</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:DS[_],usingColumns:Array[String],joinType:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="join(DS[_],Array[String],String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#join(right:DS[_],usingColumns:Array[String],joinType:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span>, <span name="usingColumns">usingColumns: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>, <span name="joinType">joinType: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Java-specific) Equi-join with another <code>DataFrame</code> using the given columns.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Equi-join with another <code>DataFrame</code> using the given columns. See the
Scala-specific overload for more details.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>
  Right side of the join operation.</p></dd><dt class="param">usingColumns</dt><dd class="cmt"><p>
  Names of the columns to join on. This columns must exist on both sides.</p></dd><dt class="param">joinType</dt><dd class="cmt"><p>
  Type of join to perform. Default <code>inner</code>. Must be one of: <code>inner</code>, <code>cross</code>, <code>outer</code>,
  <code>full</code>, <code>fullouter</code>, <code>full_outer</code>, <code>left</code>, <code>leftouter</code>, <code>left_outer</code>, <code>right</code>,
  <code>rightouter</code>, <code>right_outer</code>, <code>semi</code>, <code>leftsemi</code>, <code>left_semi</code>, <code>anti</code>, <code>leftanti</code>,
  <code>left_anti</code>.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:DS[_],usingColumn:String,joinType:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="join(DS[_],String,String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#join(right:DS[_],usingColumn:String,joinType:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span>, <span name="usingColumn">usingColumn: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="joinType">joinType: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Equi-join with another <code>DataFrame</code> using the given column.</p><div class="fullcomment"><div class="comment cmt"><p>Equi-join with another <code>DataFrame</code> using the given column. A cross join with a predicate is
specified as an inner join. If you would explicitly like to perform a cross join use the
<code>crossJoin</code> method.</p><p>Different from other join functions, the join column will only appear once in the output,
i.e. similar to SQL's <code>JOIN USING</code> syntax.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>
  Right side of the join operation.</p></dd><dt class="param">usingColumn</dt><dd class="cmt"><p>
  Name of the column to join on. This column must exist on both sides.</p></dd><dt class="param">joinType</dt><dd class="cmt"><p>
  Type of join to perform. Default <code>inner</code>. Must be one of: <code>inner</code>, <code>cross</code>, <code>outer</code>,
  <code>full</code>, <code>fullouter</code>, <code>full_outer</code>, <code>left</code>, <code>leftouter</code>, <code>left_outer</code>, <code>right</code>,
  <code>rightouter</code>, <code>right_outer</code>, <code>semi</code>, <code>leftsemi</code>, <code>left_semi</code>, <code>anti</code>, <code>leftanti</code>,
  <code>left_anti</code>.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  If you perform a self-join using this function without aliasing the input <code>DataFrame</code>s, you
  will NOT be able to reference any columns after the join, since there is no way to
  disambiguate which side of the join you would like to reference.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:DS[_],usingColumns:Seq[String]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="join(DS[_],Seq[String]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#join(right:DS[_],usingColumns:Seq[String]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span>, <span name="usingColumns">usingColumns: <span name="scala.Seq" class="extype">Seq</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Scala-specific) Inner equi-join with another <code>DataFrame</code> using the given columns.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Inner equi-join with another <code>DataFrame</code> using the given columns.</p><p>Different from other join functions, the join columns will only appear once in the output,
i.e. similar to SQL's <code>JOIN USING</code> syntax.</p><pre><span class="cmt">// Joining df1 and df2 using the columns "user_id" and "user_name"</span>
df1.join(df2, <span class="std">Seq</span>(<span class="lit">"user_id"</span>, <span class="lit">"user_name"</span>))</pre></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>
  Right side of the join operation.</p></dd><dt class="param">usingColumns</dt><dd class="cmt"><p>
  Names of the columns to join on. This columns must exist on both sides.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  If you perform a self-join using this function without aliasing the input <code>DataFrame</code>s, you
  will NOT be able to reference any columns after the join, since there is no way to
  disambiguate which side of the join you would like to reference.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:DS[_],usingColumns:Array[String]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="join(DS[_],Array[String]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#join(right:DS[_],usingColumns:Array[String]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span>, <span name="usingColumns">usingColumns: <span name="scala.Array" class="extype">Array</span>[<span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Java-specific) Inner equi-join with another <code>DataFrame</code> using the given columns.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Inner equi-join with another <code>DataFrame</code> using the given columns. See the
Scala-specific overload for more details.
</p></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>
  Right side of the join operation.</p></dd><dt class="param">usingColumns</dt><dd class="cmt"><p>
  Names of the columns to join on. This columns must exist on both sides.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#join" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="join(right:DS[_],usingColumn:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="join(DS[_],String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#join(right:DS[_],usingColumn:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">join</span><span class="params">(<span name="right">right: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[_]</span>, <span name="usingColumn">usingColumn: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Inner equi-join with another <code>DataFrame</code> using the given column.</p><div class="fullcomment"><div class="comment cmt"><p>Inner equi-join with another <code>DataFrame</code> using the given column.</p><p>Different from other join functions, the join column will only appear once in the output,
i.e. similar to SQL's <code>JOIN USING</code> syntax.</p><pre><span class="cmt">// Joining df1 and df2 using the column "user_id"</span>
df1.join(df2, <span class="lit">"user_id"</span>)</pre></div><dl class="paramcmts block"><dt class="param">right</dt><dd class="cmt"><p>
  Right side of the join operation.</p></dd><dt class="param">usingColumn</dt><dd class="cmt"><p>
  Name of the column to join on. This column must exist on both sides.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  If you perform a self-join using this function without aliasing the input <code>DataFrame</code>s, you
  will NOT be able to reference any columns after the join, since there is no way to
  disambiguate which side of the join you would like to reference.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#joinWith" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="joinWith[U](other:DS[U],condition:org.apache.spark.sql.Column):DS[(T,U)]" class="anchorToMember"></a><a id="joinWith[U](DS[U],Column):DS[(T,U)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#joinWith[U](other:DS[U],condition:org.apache.spark.sql.Column):DS[(T,U)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">joinWith</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.joinWith.U" class="extype">U</span>]</span>, <span name="condition">condition: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[(<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.joinWith.U" class="extype">U</span>)]</span></span><p class="shortcomment cmt">Using inner equi-join to join this Dataset returning a <code>Tuple2</code> for each pair where
<code>condition</code> evaluates to true.</p><div class="fullcomment"><div class="comment cmt"><p>Using inner equi-join to join this Dataset returning a <code>Tuple2</code> for each pair where
<code>condition</code> evaluates to true.
</p></div><dl class="paramcmts block"><dt class="param">other</dt><dd class="cmt"><p>
  Right side of the join.</p></dd><dt class="param">condition</dt><dd class="cmt"><p>
  Join expression.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#localCheckpoint" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="localCheckpoint(eager:Boolean):DS[T]" class="anchorToMember"></a><a id="localCheckpoint(Boolean):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#localCheckpoint(eager:Boolean):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">localCheckpoint</span><span class="params">(<span name="eager">eager: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Locally checkpoints a Dataset and return the new Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to
truncate the logical plan of this Dataset, which is especially useful in iterative algorithms
where the plan may grow exponentially. Local checkpoints are written to executor storage and
despite potentially faster they are unreliable and may compromise job completion.
</p></div><dl class="paramcmts block"><dt class="param">eager</dt><dd class="cmt"><p>
  Whether to checkpoint this dataframe immediately</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  When checkpoint is used with eager = false, the final data that is checkpointed after the
  first action may be different from the data that was used during the job due to
  non-determinism of the underlying operation and retries. If checkpoint is used to achieve
  saving a deterministic snapshot of the data, eager = true should be used. Otherwise, it is
  only deterministic after the first execution, after the checkpoint was finalized.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#localCheckpoint" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="localCheckpoint():DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#localCheckpoint():DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">localCheckpoint</span><span class="params">()</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Eagerly locally checkpoints a Dataset and return the new Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used
to truncate the logical plan of this Dataset, which is especially useful in iterative
algorithms where the plan may grow exponentially. Local checkpoints are written to executor
storage and despite potentially faster they are unreliable and may compromise job completion.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#mapPartitions" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="mapPartitions[U](f:org.apache.spark.api.java.function.MapPartitionsFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):DS[U]" class="anchorToMember"></a><a id="mapPartitions[U](MapPartitionsFunction[T,U],Encoder[U]):DS[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#mapPartitions[U](f:org.apache.spark.api.java.function.MapPartitionsFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):DS[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">mapPartitions</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="f">f: <a href="../../api/java/function/MapPartitionsFunction.html" name="org.apache.spark.api.java.function.MapPartitionsFunction" id="org.apache.spark.api.java.function.MapPartitionsFunction" class="extype">MapPartitionsFunction</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.mapPartitions.U" class="extype">U</span>]</span>, <span name="encoder">encoder: <a href="../Encoder.html" name="org.apache.spark.sql.Encoder" id="org.apache.spark.sql.Encoder" class="extype">Encoder</a>[<span name="org.apache.spark.sql.api.Dataset.mapPartitions.U" class="extype">U</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.mapPartitions.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">(Java-specific) Returns a new Dataset that contains the result of applying <code>f</code> to each
partition.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Returns a new Dataset that contains the result of applying <code>f</code> to each
partition.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#melt" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="melt(ids:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="melt(Array[Column],String,String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#melt(ids:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">melt</span><span class="params">(<span name="ids">ids: <span name="scala.Array" class="extype">Array</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="variableColumnName">variableColumnName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="valueColumnName">valueColumnName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns
set.</p><div class="fullcomment"><div class="comment cmt"><p>Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns
set. This is the reverse to <code>groupBy(...).pivot(...).agg(...)</code>, except for the aggregation,
which cannot be reversed. This is an alias for <code>unpivot</code>.
</p></div><dl class="paramcmts block"><dt class="param">ids</dt><dd class="cmt"><p>
  Id columns</p></dd><dt class="param">variableColumnName</dt><dd class="cmt"><p>
  Name of the variable column</p></dd><dt class="param">valueColumnName</dt><dd class="cmt"><p>
  Name of the value column</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd><dt>See also</dt><dd><span class="cmt"><p>
  <code>org.apache.spark.sql.Dataset.unpivot(Array, Array, String, String)</code>
This is equivalent to calling <code>Dataset#unpivot(Array, Array, String, String)</code> where <code>values</code>
is set to all non-id columns that exist in the DataFrame.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#melt" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="melt(ids:Array[org.apache.spark.sql.Column],values:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="melt(Array[Column],Array[Column],String,String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#melt(ids:Array[org.apache.spark.sql.Column],values:Array[org.apache.spark.sql.Column],variableColumnName:String,valueColumnName:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">melt</span><span class="params">(<span name="ids">ids: <span name="scala.Array" class="extype">Array</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="values">values: <span name="scala.Array" class="extype">Array</span>[<a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>, <span name="variableColumnName">variableColumnName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="valueColumnName">valueColumnName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns
set.</p><div class="fullcomment"><div class="comment cmt"><p>Unpivot a DataFrame from wide format to long format, optionally leaving identifier columns
set. This is the reverse to <code>groupBy(...).pivot(...).agg(...)</code>, except for the aggregation,
which cannot be reversed. This is an alias for <code>unpivot</code>.
</p></div><dl class="paramcmts block"><dt class="param">ids</dt><dd class="cmt"><p>
  Id columns</p></dd><dt class="param">values</dt><dd class="cmt"><p>
  Value columns to unpivot</p></dd><dt class="param">variableColumnName</dt><dd class="cmt"><p>
  Name of the variable column</p></dd><dt class="param">valueColumnName</dt><dd class="cmt"><p>
  Name of the value column</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd><dt>See also</dt><dd><span class="cmt"><p>
  <code>org.apache.spark.sql.Dataset.unpivot(Array, Array, String, String)</code></p></span></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#ne" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="ne(x$1:AnyRef):Boolean" class="anchorToMember"></a><a id="ne(AnyRef):Boolean" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#ne(x$1:AnyRef):Boolean" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">ne</span><span class="params">(<span name="arg0">arg0: <span name="scala.AnyRef" class="extype">AnyRef</span></span>)</span><span class="result">: <span name="scala.Boolean" class="extype">Boolean</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd></dl></div></li><li class="indented0 " name="scala.AnyRef#notify" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="notify():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#notify():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">notify</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@IntrinsicCandidate</span><span class="args">()</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="scala.AnyRef#notifyAll" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="notifyAll():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#notifyAll():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">notifyAll</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@IntrinsicCandidate</span><span class="args">()</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#orderBy" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="orderBy(sortExprs:org.apache.spark.sql.Column*):DS[T]" class="anchorToMember"></a><a id="orderBy(Column*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#orderBy(sortExprs:org.apache.spark.sql.Column*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">orderBy</span><span class="params">(<span name="sortExprs">sortExprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset sorted by the given expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset sorted by the given expressions. This is an alias of the <code>sort</code>
function.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#orderBy" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="orderBy(sortCol:String,sortCols:String*):DS[T]" class="anchorToMember"></a><a id="orderBy(String,String*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#orderBy(sortCol:String,sortCols:String*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">orderBy</span><span class="params">(<span name="sortCol">sortCol: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="sortCols">sortCols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset sorted by the given expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset sorted by the given expressions. This is an alias of the <code>sort</code>
function.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#printSchema" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="printSchema(level:Int):Unit" class="anchorToMember"></a><a id="printSchema(Int):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#printSchema(level:Int):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">printSchema</span><span class="params">(<span name="level">level: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Prints the schema up to the given level to the console in a nice tree format.</p><div class="fullcomment"><div class="comment cmt"><p>Prints the schema up to the given level to the console in a nice tree format.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#printSchema" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="printSchema():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#printSchema():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">printSchema</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Prints the schema to the console in a nice tree format.</p><div class="fullcomment"><div class="comment cmt"><p>Prints the schema to the console in a nice tree format.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#reduce" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="reduce(func:org.apache.spark.api.java.function.ReduceFunction[T]):T" class="anchorToMember"></a><a id="reduce(ReduceFunction[T]):T" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#reduce(func:org.apache.spark.api.java.function.ReduceFunction[T]):T" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">reduce</span><span class="params">(<span name="func">func: <a href="../../api/java/function/ReduceFunction.html" name="org.apache.spark.api.java.function.ReduceFunction" id="org.apache.spark.api.java.function.ReduceFunction" class="extype">ReduceFunction</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span></span></span><p class="shortcomment cmt">(Java-specific) Reduces the elements of this Dataset using the specified binary function.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Reduces the elements of this Dataset using the specified binary function. The
given <code>func</code> must be commutative and associative or the result may be non-deterministic.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#repartition" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="repartition(partitionExprs:org.apache.spark.sql.Column*):DS[T]" class="anchorToMember"></a><a id="repartition(Column*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#repartition(partitionExprs:org.apache.spark.sql.Column*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartition</span><span class="params">(<span name="partitionExprs">partitionExprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset partitioned by the given partitioning expressions, using
<code>spark.sql.shuffle.partitions</code> as number of partitions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset partitioned by the given partitioning expressions, using
<code>spark.sql.shuffle.partitions</code> as number of partitions. The resulting Dataset is hash
partitioned.</p><p>This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#repartition" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="repartition(numPartitions:Int,partitionExprs:org.apache.spark.sql.Column*):DS[T]" class="anchorToMember"></a><a id="repartition(Int,Column*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#repartition(numPartitions:Int,partitionExprs:org.apache.spark.sql.Column*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartition</span><span class="params">(<span name="numPartitions">numPartitions: <span name="scala.Int" class="extype">Int</span></span>, <span name="partitionExprs">partitionExprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset partitioned by the given partitioning expressions into <code>numPartitions</code>.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset partitioned by the given partitioning expressions into <code>numPartitions</code>.
The resulting Dataset is hash partitioned.</p><p>This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#repartitionByRange" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="repartitionByRange(partitionExprs:org.apache.spark.sql.Column*):DS[T]" class="anchorToMember"></a><a id="repartitionByRange(Column*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#repartitionByRange(partitionExprs:org.apache.spark.sql.Column*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartitionByRange</span><span class="params">(<span name="partitionExprs">partitionExprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset partitioned by the given partitioning expressions, using
<code>spark.sql.shuffle.partitions</code> as number of partitions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset partitioned by the given partitioning expressions, using
<code>spark.sql.shuffle.partitions</code> as number of partitions. The resulting Dataset is range
partitioned.</p><p>At least one partition-by expression must be specified. When no explicit sort order is
specified, "ascending nulls first" is assumed. Note, the rows are not sorted in each
partition of the resulting Dataset.</p><p>Note that due to performance reasons this method uses sampling to estimate the ranges. Hence,
the output may not be consistent, since sampling can return different values. The sample size
can be controlled by the config <code>spark.sql.execution.rangeExchange.sampleSizePerPartition</code>.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#repartitionByRange" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="repartitionByRange(numPartitions:Int,partitionExprs:org.apache.spark.sql.Column*):DS[T]" class="anchorToMember"></a><a id="repartitionByRange(Int,Column*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#repartitionByRange(numPartitions:Int,partitionExprs:org.apache.spark.sql.Column*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">repartitionByRange</span><span class="params">(<span name="numPartitions">numPartitions: <span name="scala.Int" class="extype">Int</span></span>, <span name="partitionExprs">partitionExprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset partitioned by the given partitioning expressions into <code>numPartitions</code>.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset partitioned by the given partitioning expressions into <code>numPartitions</code>.
The resulting Dataset is range partitioned.</p><p>At least one partition-by expression must be specified. When no explicit sort order is
specified, "ascending nulls first" is assumed. Note, the rows are not sorted in each
partition of the resulting Dataset.</p><p>Note that due to performance reasons this method uses sampling to estimate the ranges. Hence,
the output may not be consistent, since sampling can return different values. The sample size
can be controlled by the config <code>spark.sql.execution.rangeExchange.sampleSizePerPartition</code>.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#rollup" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="rollup(col1:String,cols:String*):Dataset.this.RGD" class="anchorToMember"></a><a id="rollup(String,String*):RGD" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#rollup(col1:String,cols:String*):Dataset.this.RGD" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">rollup</span><span class="params">(<span name="col1">col1: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <a href="#RGD&lt;:org.apache.spark.sql.api.RelationalGroupedDataset[DS]" name="org.apache.spark.sql.api.Dataset.RGD" id="org.apache.spark.sql.api.Dataset.RGD" class="extmbr">RGD</a></span></span><p class="shortcomment cmt">Create a multi-dimensional rollup for the current Dataset using the specified columns, so we
can run aggregation on them.</p><div class="fullcomment"><div class="comment cmt"><p>Create a multi-dimensional rollup for the current Dataset using the specified columns, so we
can run aggregation on them. See <a href="RelationalGroupedDataset.html" name="org.apache.spark.sql.api.RelationalGroupedDataset" id="org.apache.spark.sql.api.RelationalGroupedDataset" class="extype">RelationalGroupedDataset</a> for all the available aggregate
functions.</p><p>This is a variant of rollup that can only group by existing columns using column names (i.e.
cannot construct expressions).</p><pre><span class="cmt">// Compute the average for all numeric columns rolled up by department and group.</span>
ds.rollup(<span class="lit">"department"</span>, <span class="lit">"group"</span>).avg()

<span class="cmt">// Compute the max age and average salary, rolled up by department and gender.</span>
ds.rollup($<span class="lit">"department"</span>, $<span class="lit">"gender"</span>).agg(<span class="std">Map</span>(
  <span class="lit">"salary"</span> -&gt; <span class="lit">"avg"</span>,
  <span class="lit">"age"</span> -&gt; <span class="lit">"max"</span>
))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sample" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sample(withReplacement:Boolean,fraction:Double):DS[T]" class="anchorToMember"></a><a id="sample(Boolean,Double):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sample(withReplacement:Boolean,fraction:Double):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sample</span><span class="params">(<span name="withReplacement">withReplacement: <span name="scala.Boolean" class="extype">Boolean</span></span>, <span name="fraction">fraction: <span name="scala.Double" class="extype">Double</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> by sampling a fraction of rows, using a random seed.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> by sampling a fraction of rows, using a random seed.
</p></div><dl class="paramcmts block"><dt class="param">withReplacement</dt><dd class="cmt"><p>
  Sample with replacement or not.</p></dd><dt class="param">fraction</dt><dd class="cmt"><p>
  Fraction of rows to generate, range [0.0, 1.0].</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  This is NOT guaranteed to provide exactly the fraction of the total count of the given
  <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sample" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sample(fraction:Double):DS[T]" class="anchorToMember"></a><a id="sample(Double):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sample(fraction:Double):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sample</span><span class="params">(<span name="fraction">fraction: <span name="scala.Double" class="extype">Double</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> by sampling a fraction of rows (without replacement), using a
random seed.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> by sampling a fraction of rows (without replacement), using a
random seed.
</p></div><dl class="paramcmts block"><dt class="param">fraction</dt><dd class="cmt"><p>
  Fraction of rows to generate, range [0.0, 1.0].</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  This is NOT guaranteed to provide exactly the fraction of the count of the given
  <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sample" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sample(fraction:Double,seed:Long):DS[T]" class="anchorToMember"></a><a id="sample(Double,Long):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sample(fraction:Double,seed:Long):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sample</span><span class="params">(<span name="fraction">fraction: <span name="scala.Double" class="extype">Double</span></span>, <span name="seed">seed: <span name="scala.Long" class="extype">Long</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> by sampling a fraction of rows (without replacement), using a
user-supplied seed.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a> by sampling a fraction of rows (without replacement), using a
user-supplied seed.
</p></div><dl class="paramcmts block"><dt class="param">fraction</dt><dd class="cmt"><p>
  Fraction of rows to generate, range [0.0, 1.0].</p></dd><dt class="param">seed</dt><dd class="cmt"><p>
  Seed for sampling.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  This is NOT guaranteed to provide exactly the fraction of the count of the given
  <a href="" name="org.apache.spark.sql.api.Dataset" id="org.apache.spark.sql.api.Dataset" class="extype">Dataset</a>.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#select" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select[U1,U2,U3,U4,U5](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3],c4:org.apache.spark.sql.TypedColumn[T,U4],c5:org.apache.spark.sql.TypedColumn[T,U5]):DS[(U1,U2,U3,U4,U5)]" class="anchorToMember"></a><a id="select[U1,U2,U3,U4,U5](TypedColumn[T,U1],TypedColumn[T,U2],TypedColumn[T,U3],TypedColumn[T,U4],TypedColumn[T,U5]):DS[(U1,U2,U3,U4,U5)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#select[U1,U2,U3,U4,U5](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3],c4:org.apache.spark.sql.TypedColumn[T,U4],c5:org.apache.spark.sql.TypedColumn[T,U5]):DS[(U1,U2,U3,U4,U5)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="tparams">[<span name="U1">U1</span>, <span name="U2">U2</span>, <span name="U3">U3</span>, <span name="U4">U4</span>, <span name="U5">U5</span>]</span><span class="params">(<span name="c1">c1: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U1" class="extype">U1</span>]</span>, <span name="c2">c2: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U2" class="extype">U2</span>]</span>, <span name="c3">c3: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U3" class="extype">U3</span>]</span>, <span name="c4">c4: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U4" class="extype">U4</span>]</span>, <span name="c5">c5: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U5" class="extype">U5</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[(<span name="org.apache.spark.sql.api.Dataset.select.U1" class="extype">U1</span>, <span name="org.apache.spark.sql.api.Dataset.select.U2" class="extype">U2</span>, <span name="org.apache.spark.sql.api.Dataset.select.U3" class="extype">U3</span>, <span name="org.apache.spark.sql.api.Dataset.select.U4" class="extype">U4</span>, <span name="org.apache.spark.sql.api.Dataset.select.U5" class="extype">U5</span>)]</span></span><p class="shortcomment cmt">Returns a new Dataset by computing the given <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> expressions for
each element.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by computing the given <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> expressions for
each element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#select" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select[U1,U2,U3,U4](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3],c4:org.apache.spark.sql.TypedColumn[T,U4]):DS[(U1,U2,U3,U4)]" class="anchorToMember"></a><a id="select[U1,U2,U3,U4](TypedColumn[T,U1],TypedColumn[T,U2],TypedColumn[T,U3],TypedColumn[T,U4]):DS[(U1,U2,U3,U4)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#select[U1,U2,U3,U4](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3],c4:org.apache.spark.sql.TypedColumn[T,U4]):DS[(U1,U2,U3,U4)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="tparams">[<span name="U1">U1</span>, <span name="U2">U2</span>, <span name="U3">U3</span>, <span name="U4">U4</span>]</span><span class="params">(<span name="c1">c1: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U1" class="extype">U1</span>]</span>, <span name="c2">c2: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U2" class="extype">U2</span>]</span>, <span name="c3">c3: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U3" class="extype">U3</span>]</span>, <span name="c4">c4: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U4" class="extype">U4</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[(<span name="org.apache.spark.sql.api.Dataset.select.U1" class="extype">U1</span>, <span name="org.apache.spark.sql.api.Dataset.select.U2" class="extype">U2</span>, <span name="org.apache.spark.sql.api.Dataset.select.U3" class="extype">U3</span>, <span name="org.apache.spark.sql.api.Dataset.select.U4" class="extype">U4</span>)]</span></span><p class="shortcomment cmt">Returns a new Dataset by computing the given <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> expressions for
each element.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by computing the given <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> expressions for
each element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#select" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select[U1,U2,U3](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3]):DS[(U1,U2,U3)]" class="anchorToMember"></a><a id="select[U1,U2,U3](TypedColumn[T,U1],TypedColumn[T,U2],TypedColumn[T,U3]):DS[(U1,U2,U3)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#select[U1,U2,U3](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2],c3:org.apache.spark.sql.TypedColumn[T,U3]):DS[(U1,U2,U3)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="tparams">[<span name="U1">U1</span>, <span name="U2">U2</span>, <span name="U3">U3</span>]</span><span class="params">(<span name="c1">c1: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U1" class="extype">U1</span>]</span>, <span name="c2">c2: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U2" class="extype">U2</span>]</span>, <span name="c3">c3: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U3" class="extype">U3</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[(<span name="org.apache.spark.sql.api.Dataset.select.U1" class="extype">U1</span>, <span name="org.apache.spark.sql.api.Dataset.select.U2" class="extype">U2</span>, <span name="org.apache.spark.sql.api.Dataset.select.U3" class="extype">U3</span>)]</span></span><p class="shortcomment cmt">Returns a new Dataset by computing the given <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> expressions for
each element.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by computing the given <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> expressions for
each element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#select" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select[U1,U2](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2]):DS[(U1,U2)]" class="anchorToMember"></a><a id="select[U1,U2](TypedColumn[T,U1],TypedColumn[T,U2]):DS[(U1,U2)]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#select[U1,U2](c1:org.apache.spark.sql.TypedColumn[T,U1],c2:org.apache.spark.sql.TypedColumn[T,U2]):DS[(U1,U2)]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="tparams">[<span name="U1">U1</span>, <span name="U2">U2</span>]</span><span class="params">(<span name="c1">c1: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U1" class="extype">U1</span>]</span>, <span name="c2">c2: <a href="../TypedColumn.html" name="org.apache.spark.sql.TypedColumn" id="org.apache.spark.sql.TypedColumn" class="extype">TypedColumn</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>, <span name="org.apache.spark.sql.api.Dataset.select.U2" class="extype">U2</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[(<span name="org.apache.spark.sql.api.Dataset.select.U1" class="extype">U1</span>, <span name="org.apache.spark.sql.api.Dataset.select.U2" class="extype">U2</span>)]</span></span><p class="shortcomment cmt">Returns a new Dataset by computing the given <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> expressions for
each element.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by computing the given <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">org.apache.spark.sql.Column</a> expressions for
each element.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#select" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="select(col:String,cols:String*):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="select(String,String*):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#select(col:String,cols:String*):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">select</span><span class="params">(<span name="col">col: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="cols">cols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Selects a set of columns.</p><div class="fullcomment"><div class="comment cmt"><p>Selects a set of columns. This is a variant of <code>select</code> that can only select existing columns
using column names (i.e. cannot construct expressions).</p><pre><span class="cmt">// The following two are equivalent:</span>
ds.select(<span class="lit">"colA"</span>, <span class="lit">"colB"</span>)
ds.select($<span class="lit">"colA"</span>, $<span class="lit">"colB"</span>)</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#selectExpr" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="selectExpr(exprs:String*):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="selectExpr(String*):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#selectExpr(exprs:String*):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">selectExpr</span><span class="params">(<span name="exprs">exprs: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Selects a set of SQL expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Selects a set of SQL expressions. This is a variant of <code>select</code> that accepts SQL expressions.</p><pre><span class="cmt">// The following are equivalent:</span>
ds.selectExpr(<span class="lit">"colA"</span>, <span class="lit">"colB as newName"</span>, <span class="lit">"abs(colC)"</span>)
ds.select(expr(<span class="lit">"colA"</span>), expr(<span class="lit">"colB as newName"</span>), expr(<span class="lit">"abs(colC)"</span>))</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#show" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="show(numRows:Int,truncate:Int):Unit" class="anchorToMember"></a><a id="show(Int,Int):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#show(numRows:Int,truncate:Int):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">(<span name="numRows">numRows: <span name="scala.Int" class="extype">Int</span></span>, <span name="truncate">truncate: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the Dataset in a tabular form. For example:</p><pre>year  month AVG('Adj Close) MAX('Adj Close)
<span class="num">1980</span>  <span class="num">12</span>    <span class="num">0.503218</span>        <span class="num">0.595103</span>
<span class="num">1981</span>  <span class="num">01</span>    <span class="num">0.523289</span>        <span class="num">0.570307</span>
<span class="num">1982</span>  <span class="num">02</span>    <span class="num">0.436504</span>        <span class="num">0.475256</span>
<span class="num">1983</span>  <span class="num">03</span>    <span class="num">0.410516</span>        <span class="num">0.442194</span>
<span class="num">1984</span>  <span class="num">04</span>    <span class="num">0.450090</span>        <span class="num">0.483521</span></pre></div><dl class="paramcmts block"><dt class="param">numRows</dt><dd class="cmt"><p>
  Number of rows to show</p></dd><dt class="param">truncate</dt><dd class="cmt"><p>
  If set to more than 0, truncates strings to <code>truncate</code> characters and all cells will be
  aligned right.</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#show" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="show(truncate:Boolean):Unit" class="anchorToMember"></a><a id="show(Boolean):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#show(truncate:Boolean):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">(<span name="truncate">truncate: <span name="scala.Boolean" class="extype">Boolean</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the top 20 rows of Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the top 20 rows of Dataset in a tabular form.
</p></div><dl class="paramcmts block"><dt class="param">truncate</dt><dd class="cmt"><p>
  Whether truncate long strings. If true, strings more than 20 characters will be truncated
  and all cells will be aligned right</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#show" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="show():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#show():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the top 20 rows of Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters will
be truncated, and all cells will be aligned right.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#show" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="show(numRows:Int):Unit" class="anchorToMember"></a><a id="show(Int):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#show(numRows:Int):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">show</span><span class="params">(<span name="numRows">numRows: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Displays the Dataset in a tabular form.</p><div class="fullcomment"><div class="comment cmt"><p>Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
and all cells will be aligned right. For example:</p><pre>year  month AVG('Adj Close) MAX('Adj Close)
<span class="num">1980</span>  <span class="num">12</span>    <span class="num">0.503218</span>        <span class="num">0.595103</span>
<span class="num">1981</span>  <span class="num">01</span>    <span class="num">0.523289</span>        <span class="num">0.570307</span>
<span class="num">1982</span>  <span class="num">02</span>    <span class="num">0.436504</span>        <span class="num">0.475256</span>
<span class="num">1983</span>  <span class="num">03</span>    <span class="num">0.410516</span>        <span class="num">0.442194</span>
<span class="num">1984</span>  <span class="num">04</span>    <span class="num">0.450090</span>        <span class="num">0.483521</span></pre></div><dl class="paramcmts block"><dt class="param">numRows</dt><dd class="cmt"><p>
  Number of rows to show</p></dd></dl><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sort" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sort(sortExprs:org.apache.spark.sql.Column*):DS[T]" class="anchorToMember"></a><a id="sort(Column*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sort(sortExprs:org.apache.spark.sql.Column*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sort</span><span class="params">(<span name="sortExprs">sortExprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset sorted by the given expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset sorted by the given expressions. For example:</p><pre>ds.sort($<span class="lit">"col1"</span>, $<span class="lit">"col2"</span>.desc)</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sort" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sort(sortCol:String,sortCols:String*):DS[T]" class="anchorToMember"></a><a id="sort(String,String*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sort(sortCol:String,sortCols:String*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sort</span><span class="params">(<span name="sortCol">sortCol: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="sortCols">sortCols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset sorted by the specified column, all in ascending order.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset sorted by the specified column, all in ascending order.</p><pre><span class="cmt">// The following 3 are equivalent</span>
ds.sort(<span class="lit">"sortcol"</span>)
ds.sort($<span class="lit">"sortcol"</span>)
ds.sort($<span class="lit">"sortcol"</span>.asc)</pre></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sortWithinPartitions" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sortWithinPartitions(sortExprs:org.apache.spark.sql.Column*):DS[T]" class="anchorToMember"></a><a id="sortWithinPartitions(Column*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sortWithinPartitions(sortExprs:org.apache.spark.sql.Column*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sortWithinPartitions</span><span class="params">(<span name="sortExprs">sortExprs: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with each partition sorted by the given expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with each partition sorted by the given expressions.</p><p>This is the same operation as "SORT BY" in SQL (Hive QL).
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#sortWithinPartitions" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="sortWithinPartitions(sortCol:String,sortCols:String*):DS[T]" class="anchorToMember"></a><a id="sortWithinPartitions(String,String*):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#sortWithinPartitions(sortCol:String,sortCols:String*):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">sortWithinPartitions</span><span class="params">(<span name="sortCol">sortCol: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="sortCols">sortCols: <span name="scala.Predef.String" class="extype">String</span>*</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset with each partition sorted by the given expressions.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with each partition sorted by the given expressions.</p><p>This is the same operation as "SORT BY" in SQL (Hive QL).
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@varargs</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#synchronized" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="synchronized[T0](x$1:=&gt;T0):T0" class="anchorToMember"></a><a id="synchronized[T0](=&gt;T0):T0" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#synchronized[T0](x$1:=&gt;T0):T0" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">synchronized</span><span class="tparams">[<span name="T0">T0</span>]</span><span class="params">(<span name="arg0">arg0: =&gt; <span name="java.lang.AnyRef.synchronized.T0" class="extype">T0</span></span>)</span><span class="result">: <span name="java.lang.AnyRef.synchronized.T0" class="extype">T0</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#take" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="take(n:Int):Array[T]" class="anchorToMember"></a><a id="take(Int):Array[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#take(n:Int):Array[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">take</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Array" class="extype">Array</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns the first <code>n</code> rows in the Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the first <code>n</code> rows in the Dataset.</p><p>Running take requires moving data into the application's driver process, and doing so with a
very large <code>n</code> can crash the driver process with OutOfMemoryError.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#takeAsList" group="action" fullComment="yes" data-isabs="false" visbl="pub"><a id="takeAsList(n:Int):java.util.List[T]" class="anchorToMember"></a><a id="takeAsList(Int):List[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#takeAsList(n:Int):java.util.List[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">takeAsList</span><span class="params">(<span name="n">n: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/List.html#java.util.List" name="java.util.List" id="java.util.List" class="extype">List</a>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns the first <code>n</code> rows in the Dataset as a list.</p><div class="fullcomment"><div class="comment cmt"><p>Returns the first <code>n</code> rows in the Dataset as a list.</p><p>Running take requires moving data into the application's driver process, and doing so with a
very large <code>n</code> can crash the driver process with OutOfMemoryError.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#toString" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="toString():String" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#toString():String" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">toString</span><span class="params">()</span><span class="result">: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/lang/String.html#java.lang.String" name="java.lang.String" id="java.lang.String" class="extype">String</a></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef → Any</dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#transform" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="transform[U](t:DS[T]=&gt;DS[U]):DS[U]" class="anchorToMember"></a><a id="transform[U]((DS[T])=&gt;DS[U]):DS[U]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#transform[U](t:DS[T]=&gt;DS[U]):DS[U]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">transform</span><span class="tparams">[<span name="U">U</span>]</span><span class="params">(<span name="t">t: (<span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]) =&gt; <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.transform.U" class="extype">U</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.transform.U" class="extype">U</span>]</span></span><p class="shortcomment cmt">Concise syntax for chaining custom transformations.</p><div class="fullcomment"><div class="comment cmt"><p>Concise syntax for chaining custom transformations.</p><pre><span class="kw">def</span> featurize(ds: DS[T]): DS[U] = ...

ds
  .transform(featurize)
  .transform(...)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#unionAll" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="unionAll(other:DS[T]):DS[T]" class="anchorToMember"></a><a id="unionAll(DS[T]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#unionAll(other:DS[T]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unionAll</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing union of rows in this Dataset and another Dataset. This is
an alias for <code>union</code>.</p><p>This is equivalent to <code>UNION ALL</code> in SQL. To do a SQL-style set union (that does
deduplication of elements), use this function followed by a <a href="#distinct():DS[T]" name="org.apache.spark.sql.api.Dataset#distinct" id="org.apache.spark.sql.api.Dataset#distinct" class="extmbr">distinct</a>.</p><p>Also as standard in SQL, this function resolves columns by position (not by name).
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#unionByName" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="unionByName(other:DS[T]):DS[T]" class="anchorToMember"></a><a id="unionByName(DS[T]):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#unionByName(other:DS[T]):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">unionByName</span><span class="params">(<span name="other">other: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset containing union of rows in this Dataset and another Dataset.</p><p>This is different from both <code>UNION ALL</code> and <code>UNION DISTINCT</code> in SQL. To do a SQL-style set
union (that does deduplication of elements), use this function followed by a <a href="#distinct():DS[T]" name="org.apache.spark.sql.api.Dataset#distinct" id="org.apache.spark.sql.api.Dataset#distinct" class="extmbr">distinct</a>.</p><p>The difference between this function and <a href="#union(other:DS[T]):DS[T]" name="org.apache.spark.sql.api.Dataset#union" id="org.apache.spark.sql.api.Dataset#union" class="extmbr">union</a> is that this function resolves columns by
name (not by position):</p><pre><span class="kw">val</span> df1 = <span class="std">Seq</span>((<span class="num">1</span>, <span class="num">2</span>, <span class="num">3</span>)).toDF(<span class="lit">"col0"</span>, <span class="lit">"col1"</span>, <span class="lit">"col2"</span>)
<span class="kw">val</span> df2 = <span class="std">Seq</span>((<span class="num">4</span>, <span class="num">5</span>, <span class="num">6</span>)).toDF(<span class="lit">"col1"</span>, <span class="lit">"col2"</span>, <span class="lit">"col0"</span>)
df1.unionByName(df2).show

<span class="cmt">// output:</span>
<span class="cmt">// +----+----+----+</span>
<span class="cmt">// |col0|col1|col2|</span>
<span class="cmt">// +----+----+----+</span>
<span class="cmt">// |   1|   2|   3|</span>
<span class="cmt">// |   6|   4|   5|</span>
<span class="cmt">// +----+----+----+</span></pre><p>Note that this supports nested columns in struct and array types. Nested columns in map types
are not currently supported.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.3.0</p></dd></dl></div></li><li class="indented0 " name="scala.AnyRef#wait" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="wait(x$1:Long,x$2:Int):Unit" class="anchorToMember"></a><a id="wait(Long,Int):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#wait(x$1:Long,x$2:Int):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">wait</span><span class="params">(<span name="arg0">arg0: <span name="scala.Long" class="extype">Long</span></span>, <span name="arg1">arg1: <span name="scala.Int" class="extype">Int</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">classOf[java.lang.InterruptedException]</span></span>)</span> </dd></dl></div></li><li class="indented0 " name="scala.AnyRef#wait" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="wait(x$1:Long):Unit" class="anchorToMember"></a><a id="wait(Long):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#wait(x$1:Long):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">wait</span><span class="params">(<span name="arg0">arg0: <span name="scala.Long" class="extype">Long</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">classOf[java.lang.InterruptedException]</span></span>)</span> <span class="name">@native</span><span class="args">()</span> </dd></dl></div></li><li class="indented0 " name="scala.AnyRef#wait" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="wait():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#wait():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">def</span></span> <span class="symbol"><span class="name">wait</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">classOf[java.lang.InterruptedException]</span></span>)</span> </dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#where" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="where(conditionExpr:String):DS[T]" class="anchorToMember"></a><a id="where(String):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#where(conditionExpr:String):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">where</span><span class="params">(<span name="conditionExpr">conditionExpr: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Filters rows using the given SQL expression.</p><div class="fullcomment"><div class="comment cmt"><p>Filters rows using the given SQL expression.</p><pre>peopleDs.where(<span class="lit">"age &gt; 15"</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#where" group="typedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="where(condition:org.apache.spark.sql.Column):DS[T]" class="anchorToMember"></a><a id="where(Column):DS[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#where(condition:org.apache.spark.sql.Column):DS[T]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">where</span><span class="params">(<span name="condition">condition: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<span name="org.apache.spark.sql.api.Dataset.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">Filters rows using the given condition.</p><div class="fullcomment"><div class="comment cmt"><p>Filters rows using the given condition. This is an alias for <code>filter</code>.</p><pre><span class="cmt">// The following are equivalent:</span>
peopleDs.filter($<span class="lit">"age"</span> &gt; <span class="num">15</span>)
peopleDs.where($<span class="lit">"age"</span> &gt; <span class="num">15</span>)</pre></div><dl class="attributes block"><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#withColumn" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumn(colName:String,col:org.apache.spark.sql.Column):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="withColumn(String,Column):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#withColumn(colName:String,col:org.apache.spark.sql.Column):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumn</span><span class="params">(<span name="colName">colName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="col">col: <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Returns a new Dataset by adding a column or replacing the existing column that has the same
name.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset by adding a column or replacing the existing column that has the same
name.</p><p><code>column</code>'s expression must only refer to attributes supplied by this Dataset. It is an error
to add a column that refers to some other Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd><dt>Note</dt><dd><span class="cmt"><p>
  this method introduces a projection internally. Therefore, calling it multiple times, for
  instance, via loops in order to add multiple columns can generate big plans which can cause
  performance issues and even <code>StackOverflowException</code>. To avoid this, use <code>select</code> with the
  multiple columns at once.</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#withColumnRenamed" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumnRenamed(existingName:String,newName:String):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="withColumnRenamed(String,String):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#withColumnRenamed(existingName:String,newName:String):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumnRenamed</span><span class="params">(<span name="existingName">existingName: <span name="scala.Predef.String" class="extype">String</span></span>, <span name="newName">newName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">Returns a new Dataset with a column renamed.</p><div class="fullcomment"><div class="comment cmt"><p>Returns a new Dataset with a column renamed. This is a no-op if schema doesn't contain
existingName.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>2.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#withColumns" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumns(colsMap:java.util.Map[String,org.apache.spark.sql.Column]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="withColumns(Map[String,Column]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#withColumns(colsMap:java.util.Map[String,org.apache.spark.sql.Column]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumns</span><span class="params">(<span name="colsMap">colsMap: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Map.html#java.util.Map" name="java.util.Map" id="java.util.Map" class="extype">Map</a>[<span name="scala.Predef.String" class="extype">String</span>, <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Java-specific) Returns a new Dataset by adding columns or replacing the existing columns
that has the same names.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Returns a new Dataset by adding columns or replacing the existing columns
that has the same names.</p><p><code>colsMap</code> is a map of column name and column, the column must only refer to attribute
supplied by this Dataset. It is an error to add columns that refers to some other Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#withColumns" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumns(colsMap:Map[String,org.apache.spark.sql.Column]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="withColumns(Map[String,Column]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#withColumns(colsMap:Map[String,org.apache.spark.sql.Column]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumns</span><span class="params">(<span name="colsMap">colsMap: <span name="scala.Predef.Map" class="extype">Map</span>[<span name="scala.Predef.String" class="extype">String</span>, <a href="../Column.html" name="org.apache.spark.sql.Column" id="org.apache.spark.sql.Column" class="extype">Column</a>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns
that has the same names.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset by adding columns or replacing the existing columns
that has the same names.</p><p><code>colsMap</code> is a map of column name and column, the column must only refer to attributes
supplied by this Dataset. It is an error to add columns that refers to some other Dataset.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#withColumnsRenamed" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumnsRenamed(colsMap:java.util.Map[String,String]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="withColumnsRenamed(Map[String,String]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#withColumnsRenamed(colsMap:java.util.Map[String,String]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumnsRenamed</span><span class="params">(<span name="colsMap">colsMap: <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Map.html#java.util.Map" name="java.util.Map" id="java.util.Map" class="extype">Map</a>[<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Java-specific) Returns a new Dataset with a columns renamed.</p><div class="fullcomment"><div class="comment cmt"><p>(Java-specific) Returns a new Dataset with a columns renamed. This is a no-op if schema
doesn't contain existingName.</p><p><code>colsMap</code> is a map of existing column name and new column name.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#withColumnsRenamed" group="untypedrel" fullComment="yes" data-isabs="false" visbl="pub"><a id="withColumnsRenamed(colsMap:Map[String,String]):DS[org.apache.spark.sql.Row]" class="anchorToMember"></a><a id="withColumnsRenamed(Map[String,String]):DS[Row]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#withColumnsRenamed(colsMap:Map[String,String]):DS[org.apache.spark.sql.Row]" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name">withColumnsRenamed</span><span class="params">(<span name="colsMap">colsMap: <span name="scala.Predef.Map" class="extype">Map</span>[<span name="scala.Predef.String" class="extype">String</span>, <span name="scala.Predef.String" class="extype">String</span>]</span>)</span><span class="result">: <span name="org.apache.spark.sql.api.Dataset.DS" class="extype">DS</span>[<a href="../Row.html" name="org.apache.spark.sql.Row" id="org.apache.spark.sql.Row" class="extype">Row</a>]</span></span><p class="shortcomment cmt">(Scala-specific) Returns a new Dataset with a columns renamed.</p><div class="fullcomment"><div class="comment cmt"><p>(Scala-specific) Returns a new Dataset with a columns renamed. This is a no-op if schema
doesn't contain existingName.</p><p><code>colsMap</code> is a map of existing column name and new column name.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="defval">scala.this.throws.&lt;init&gt;$default$1[org.apache.spark.sql.AnalysisException]</span></span>)</span> </dd><dt>Since</dt><dd><p>3.4.0</p></dd><dt>Exceptions thrown</dt><dd><span class="cmt"><p><a href="../AnalysisException.html" name="org.apache.spark.sql.AnalysisException" id="org.apache.spark.sql.AnalysisException" class="extype"><code>org.apache.spark.sql.AnalysisException</code></a> 
  if there are duplicate names in resulting projection</p></span></dd></dl></div></li></ol></div><div class="values members"><h3>Deprecated Value Members</h3><ol><li class="indented0 " name="scala.AnyRef#finalize" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="prt"><a id="finalize():Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#finalize():Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name deprecated" title="Deprecated: (Since version 9)">finalize</span><span class="params">()</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><div class="fullcomment"><dl class="attributes block"><dt>Attributes</dt><dd>protected[<span name="java.lang" class="extype">lang</span>] </dd><dt>Definition Classes</dt><dd>AnyRef</dd><dt>Annotations</dt><dd><span class="name">@throws</span><span class="args">(<span><span class="symbol">classOf[java.lang.Throwable]</span></span>)</span> <span class="name">@Deprecated</span> </dd><dt>Deprecated</dt><dd class="cmt"><p><i>(Since version 9)</i></p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.api.Dataset#registerTempTable" group="basic" fullComment="yes" data-isabs="false" visbl="pub"><a id="registerTempTable(tableName:String):Unit" class="anchorToMember"></a><a id="registerTempTable(String):Unit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../org/apache/spark/sql/api/Dataset.html#registerTempTable(tableName:String):Unit" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">def</span></span> <span class="symbol"><span class="name deprecated" title="Deprecated: (Since version 2.0.0) Use createOrReplaceTempView(viewName) instead.">registerTempTable</span><span class="params">(<span name="tableName">tableName: <span name="scala.Predef.String" class="extype">String</span></span>)</span><span class="result">: <span name="scala.Unit" class="extype">Unit</span></span></span><p class="shortcomment cmt">Registers this Dataset as a temporary table using the given name.</p><div class="fullcomment"><div class="comment cmt"><p>Registers this Dataset as a temporary table using the given name. The lifetime of this
temporary table is tied to the <code>SparkSession</code> that was used to create this Dataset.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@deprecated</span> </dd><dt>Deprecated</dt><dd class="cmt"><p><i>(Since version 2.0.0)</i> Use createOrReplaceTempView(viewName) instead.</p></dd><dt>Since</dt><dd><p>1.6.0</p></dd></dl></div></li></ol></div></div><div id="inheritedMembers"><div name="java.io.Serializable" class="parent"><h3>Inherited from <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/io/Serializable.html#java.io.Serializable" name="java.io.Serializable" id="java.io.Serializable" class="extype">Serializable</a></h3></div><div name="scala.AnyRef" class="parent"><h3>Inherited from <span name="scala.AnyRef" class="extype">AnyRef</span></h3></div><div name="scala.Any" class="parent"><h3>Inherited from <span name="scala.Any" class="extype">Any</span></h3></div></div><div id="groupedMembers"><div name="action" class="group"><h3>Actions</h3></div><div name="basic" class="group"><h3>Basic Dataset functions</h3></div><div name="streaming" class="group"><h3>streaming</h3></div><div name="typedrel" class="group"><h3>Typed transformations</h3></div><div name="untypedrel" class="group"><h3>Untyped transformations</h3></div><div name="Ungrouped" class="group"><h3>Ungrouped</h3></div></div></div><div id="tooltip"></div><div id="footer"></div></body></div></div></div></body></html>
