<!DOCTYPE html ><html><head><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" name="viewport"/><title>Spark 4.0.0-preview2 ScalaDoc  - org.apache.spark.sql.connector.read.streaming</title><meta content="Spark 4.0.0 - preview2 ScalaDoc - org.apache.spark.sql.connector.read.streaming" name="description"/><meta content="Spark 4.0.0 preview2 ScalaDoc org.apache.spark.sql.connector.read.streaming" name="keywords"/><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><link href="../../../../../../../lib/index.css" media="screen" type="text/css" rel="stylesheet"/><link href="../../../../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet"/><link href="../../../../../../../lib/print.css" media="print" type="text/css" rel="stylesheet"/><link href="../../../../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css"/><script type="text/javascript" src="../../../../../../../lib/jquery.min.js"></script><script type="text/javascript" src="../../../../../../../lib/index.js"></script><script type="text/javascript" src="../../../../../../../index.js"></script><script type="text/javascript" src="../../../../../../../lib/scheduler.js"></script><script type="text/javascript" src="../../../../../../../lib/template.js"></script><script type="text/javascript">/* this variable can be used by the JS to determine the path to the root document */
var toRoot = '../../../../../../../';</script></head><body><div id="search"><span id="doc-title">Spark 4.0.0-preview2 ScalaDoc<span id="doc-version"></span></span> <span class="close-results"><span class="left">&lt;</span> Back</span><div id="textfilter"><span class="input"><input autocapitalize="none" placeholder="Search" id="index-input" type="text" accesskey="/"/><i class="clear material-icons"></i><i id="search-icon" class="material-icons"></i></span></div></div><div id="search-results"><div id="search-progress"><div id="progress-fill"></div></div><div id="results-content"><div id="entity-results"></div><div id="member-results"></div></div></div><div id="content-scroll-container" style="-webkit-overflow-scrolling: touch;"><div id="content-container" style="-webkit-overflow-scrolling: touch;"><div id="subpackage-spacer"><div id="packages"><h1>Packages</h1><ul><li class="indented0 " name="_root_.root" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="_root_" class="anchorToMember"></a><a id="root:_root_" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../../../../../index.html" title=""><span class="name">root</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../../../../../index.html" name="_root_" id="_root_" class="extype">root</a></dd></dl></div></li><li class="indented1 " name="_root_.org" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="org" class="anchorToMember"></a><a id="org:org" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../../../../index.html" title=""><span class="name">org</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../../../../../index.html" name="_root_" id="_root_" class="extype">root</a></dd></dl></div></li><li class="indented2 " name="org.apache" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="apache" class="anchorToMember"></a><a id="apache:apache" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../../../index.html" title=""><span class="name">apache</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../../../../index.html" name="org" id="org" class="extype">org</a></dd></dl></div></li><li class="indented3 " name="org.apache.spark" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="spark" class="anchorToMember"></a><a id="spark:spark" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../../index.html" title="Core Spark functionality."><span class="name">spark</span></a></span><p class="shortcomment cmt">Core Spark functionality.</p><div class="fullcomment"><div class="comment cmt"><p>Core Spark functionality. <a href="../../../../SparkContext.html" name="org.apache.spark.SparkContext" id="org.apache.spark.SparkContext" class="extype">org.apache.spark.SparkContext</a> serves as the main entry point to
Spark, while <a href="../../../../rdd/RDD.html" name="org.apache.spark.rdd.RDD" id="org.apache.spark.rdd.RDD" class="extype">org.apache.spark.rdd.RDD</a> is the data type representing a distributed collection,
and provides most parallel operations.</p><p>In addition, <a href="../../../../rdd/PairRDDFunctions.html" name="org.apache.spark.rdd.PairRDDFunctions" id="org.apache.spark.rdd.PairRDDFunctions" class="extype">org.apache.spark.rdd.PairRDDFunctions</a> contains operations available only on RDDs
of key-value pairs, such as <code>groupByKey</code> and <code>join</code>; <a href="../../../../rdd/DoubleRDDFunctions.html" name="org.apache.spark.rdd.DoubleRDDFunctions" id="org.apache.spark.rdd.DoubleRDDFunctions" class="extype">org.apache.spark.rdd.DoubleRDDFunctions</a>
contains operations available only on RDDs of Doubles; and
<a href="../../../../rdd/SequenceFileRDDFunctions.html" name="org.apache.spark.rdd.SequenceFileRDDFunctions" id="org.apache.spark.rdd.SequenceFileRDDFunctions" class="extype">org.apache.spark.rdd.SequenceFileRDDFunctions</a> contains operations available on RDDs that can
be saved as SequenceFiles. These operations are automatically available on any RDD of the right
type (e.g. RDD[(Int, Int)] through implicit conversions.</p><p>Java programmers should reference the <a href="../../../../api/java/index.html" name="org.apache.spark.api.java" id="org.apache.spark.api.java" class="extype">org.apache.spark.api.java</a> package
for Spark programming APIs in Java.</p><p>Classes and methods marked with <span class="experimental badge" style="float: none;">
Experimental</span> are user-facing features which have not been officially adopted by the
Spark project. These are subject to change or removal in minor releases.</p><p>Classes and methods marked with <span class="developer badge" style="float: none;">
Developer API</span> are intended for advanced users want to extend Spark through lower
level interfaces. These are subject to changes or removal in minor releases.
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../../../index.html" name="org.apache" id="org.apache" class="extype">apache</a></dd></dl></div></li><li class="indented4 " name="org.apache.spark.sql" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="sql" class="anchorToMember"></a><a id="sql:sql" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../../index.html" title="Allows the execution of relational queries, including those expressed in SQL using Spark."><span class="name">sql</span></a></span><p class="shortcomment cmt">Allows the execution of relational queries, including those expressed in SQL using Spark.</p><div class="fullcomment"><div class="comment cmt"><p>Allows the execution of relational queries, including those expressed in SQL using Spark.
</p></div><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../../index.html" name="org.apache.spark" id="org.apache.spark" class="extype">spark</a></dd></dl></div></li><li class="indented5 " name="org.apache.spark.sql.connector" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="connector" class="anchorToMember"></a><a id="connector:connector" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../../index.html" title=""><span class="name">connector</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../../index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a></dd></dl></div></li><li class="indented6 " name="org.apache.spark.sql.connector.read" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="read" class="anchorToMember"></a><a id="read:read" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../index.html" title=""><span class="name">read</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../../index.html" name="org.apache.spark.sql.connector" id="org.apache.spark.sql.connector" class="extype">connector</a></dd></dl></div></li><li class="indented7 " name="org.apache.spark.sql.connector.read.colstats" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="colstats" class="anchorToMember"></a><a id="colstats:colstats" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/colstats/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../colstats/index.html" title=""><span class="name">colstats</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../index.html" name="org.apache.spark.sql.connector.read" id="org.apache.spark.sql.connector.read" class="extype">read</a></dd></dl></div></li><li class="indented7 " name="org.apache.spark.sql.connector.read.partitioning" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="partitioning" class="anchorToMember"></a><a id="partitioning:partitioning" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/partitioning/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><a href="../partitioning/index.html" title=""><span class="name">partitioning</span></a></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../index.html" name="org.apache.spark.sql.connector.read" id="org.apache.spark.sql.connector.read" class="extype">read</a></dd></dl></div></li><li class="indented7 current" name="org.apache.spark.sql.connector.read.streaming" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="streaming" class="anchorToMember"></a><a id="streaming:streaming" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/index.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><span class="name">streaming</span></span><div class="fullcomment"><dl class="attributes block"><dt>Definition Classes</dt><dd><a href="../index.html" name="org.apache.spark.sql.connector.read" id="org.apache.spark.sql.connector.read" class="extype">read</a></dd></dl></div></li><li class="current-entities indented7"><span class="separator"></span> <a href="AcceptsLatestSeenOffset.html" title="Indicates that the source accepts the latest seen offset, which requires streaming execution to provide the latest seen offset when restarting the streaming query from checkpoint." class="trait"></a><a href="AcceptsLatestSeenOffset.html" title="Indicates that the source accepts the latest seen offset, which requires streaming execution to provide the latest seen offset when restarting the streaming query from checkpoint.">AcceptsLatestSeenOffset</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="CompositeReadLimit.html" title="/** Represents a ReadLimit where the MicroBatchStream should scan approximately given maximum number of rows with at least the given minimum number of rows." class="class"></a><a href="CompositeReadLimit.html" title="/** Represents a ReadLimit where the MicroBatchStream should scan approximately given maximum number of rows with at least the given minimum number of rows.">CompositeReadLimit</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ContinuousPartitionReader.html" title="A variation on PartitionReader for use with continuous streaming processing." class="trait"></a><a href="ContinuousPartitionReader.html" title="A variation on PartitionReader for use with continuous streaming processing.">ContinuousPartitionReader</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ContinuousPartitionReaderFactory.html" title="A variation on PartitionReaderFactory that returns ContinuousPartitionReader instead of PartitionReader." class="trait"></a><a href="ContinuousPartitionReaderFactory.html" title="A variation on PartitionReaderFactory that returns ContinuousPartitionReader instead of PartitionReader.">ContinuousPartitionReaderFactory</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ContinuousStream.html" title="A SparkDataStream for streaming queries with continuous mode." class="trait"></a><a href="ContinuousStream.html" title="A SparkDataStream for streaming queries with continuous mode.">ContinuousStream</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="MicroBatchStream.html" title="A SparkDataStream for streaming queries with micro-batch mode." class="trait"></a><a href="MicroBatchStream.html" title="A SparkDataStream for streaming queries with micro-batch mode.">MicroBatchStream</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="Offset.html" title="An abstract representation of progress through a MicroBatchStream or ContinuousStream." class="class"></a><a href="Offset.html" title="An abstract representation of progress through a MicroBatchStream or ContinuousStream.">Offset</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="PartitionOffset.html" title="Used for per-partition offsets in continuous processing." class="trait"></a><a href="PartitionOffset.html" title="Used for per-partition offsets in continuous processing.">PartitionOffset</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ReadAllAvailable.html" title="Represents a ReadLimit where the MicroBatchStream must scan all the data available at the streaming source." class="class"></a><a href="ReadAllAvailable.html" title="Represents a ReadLimit where the MicroBatchStream must scan all the data available at the streaming source.">ReadAllAvailable</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ReadLimit.html" title="Interface representing limits on how much to read from a MicroBatchStream when it implements SupportsAdmissionControl." class="trait"></a><a href="ReadLimit.html" title="Interface representing limits on how much to read from a MicroBatchStream when it implements SupportsAdmissionControl.">ReadLimit</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ReadMaxBytes.html" title="Represents a ReadLimit where the MicroBatchStream should scan files which total size doesn't go beyond a given maximum total size." class="class"></a><a href="ReadMaxBytes.html" title="Represents a ReadLimit where the MicroBatchStream should scan files which total size doesn't go beyond a given maximum total size.">ReadMaxBytes</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ReadMaxFiles.html" title="Represents a ReadLimit where the MicroBatchStream should scan approximately the given maximum number of files." class="class"></a><a href="ReadMaxFiles.html" title="Represents a ReadLimit where the MicroBatchStream should scan approximately the given maximum number of files.">ReadMaxFiles</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ReadMaxRows.html" title="Represents a ReadLimit where the MicroBatchStream should scan approximately the given maximum number of rows." class="class"></a><a href="ReadMaxRows.html" title="Represents a ReadLimit where the MicroBatchStream should scan approximately the given maximum number of rows.">ReadMaxRows</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ReadMinRows.html" title="Represents a ReadLimit where the MicroBatchStream should scan approximately at least the given minimum number of rows." class="class"></a><a href="ReadMinRows.html" title="Represents a ReadLimit where the MicroBatchStream should scan approximately at least the given minimum number of rows.">ReadMinRows</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ReportsSinkMetrics.html" title="A mix-in interface for streaming sinks to signal that they can report metrics." class="trait"></a><a href="ReportsSinkMetrics.html" title="A mix-in interface for streaming sinks to signal that they can report metrics.">ReportsSinkMetrics</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="ReportsSourceMetrics.html" title="A mix-in interface for SparkDataStream streaming sources to signal that they can report metrics." class="trait"></a><a href="ReportsSourceMetrics.html" title="A mix-in interface for SparkDataStream streaming sources to signal that they can report metrics.">ReportsSourceMetrics</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="SparkDataStream.html" title="The base interface representing a readable data stream in a Spark streaming query." class="trait"></a><a href="SparkDataStream.html" title="The base interface representing a readable data stream in a Spark streaming query.">SparkDataStream</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="SupportsAdmissionControl.html" title="A mix-in interface for SparkDataStream streaming sources to signal that they can control the rate of data ingested into the system." class="trait"></a><a href="SupportsAdmissionControl.html" title="A mix-in interface for SparkDataStream streaming sources to signal that they can control the rate of data ingested into the system.">SupportsAdmissionControl</a></li><li class="current-entities indented7"><span class="separator"></span> <a href="SupportsTriggerAvailableNow.html" title="An interface for streaming sources that supports running in Trigger.AvailableNow mode, which will process all the available data at the beginning of the query in (possibly) multiple batches." class="trait"></a><a href="SupportsTriggerAvailableNow.html" title="An interface for streaming sources that supports running in Trigger.AvailableNow mode, which will process all the available data at the beginning of the query in (possibly) multiple batches.">SupportsTriggerAvailableNow</a></li></ul></div></div><div id="content"><body class="package value"><div id="definition"><div class="big-circle package">p</div><p id="owner"><a href="../../../../../../index.html" name="org" id="org" class="extype">org</a>.<a href="../../../../../index.html" name="org.apache" id="org.apache" class="extype">apache</a>.<a href="../../../../index.html" name="org.apache.spark" id="org.apache.spark" class="extype">spark</a>.<a href="../../../index.html" name="org.apache.spark.sql" id="org.apache.spark.sql" class="extype">sql</a>.<a href="../../index.html" name="org.apache.spark.sql.connector" id="org.apache.spark.sql.connector" class="extype">connector</a>.<a href="../index.html" name="org.apache.spark.sql.connector.read" id="org.apache.spark.sql.connector.read" class="extype">read</a></p><h1>streaming<span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/index.html" title="Permalink"><i class="material-icons"></i></a></span></h1></div><h4 id="signature" class="signature"><span class="modifier_kind"><span class="modifier"></span> <span class="kind">package</span></span> <span class="symbol"><span class="name">streaming</span></span></h4><div id="comment" class="fullcommenttop"></div><div id="template"><div id="allMembers"><div id="types" class="types members"><h3>Type Members</h3><ol><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.AcceptsLatestSeenOffset" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="AcceptsLatestSeenOffsetextendsSparkDataStream" class="anchorToMember"></a><a id="AcceptsLatestSeenOffset:AcceptsLatestSeenOffset" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/AcceptsLatestSeenOffset.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="AcceptsLatestSeenOffset.html" title="Indicates that the source accepts the latest seen offset, which requires streaming execution to provide the latest seen offset when restarting the streaming query from checkpoint."><span class="name">AcceptsLatestSeenOffset</span></a><span class="result"> extends <a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></span></span><p class="shortcomment cmt">Indicates that the source accepts the latest seen offset, which requires streaming execution
to provide the latest seen offset when restarting the streaming query from checkpoint.</p><div class="fullcomment"><div class="comment cmt"><p>Indicates that the source accepts the latest seen offset, which requires streaming execution
to provide the latest seen offset when restarting the streaming query from checkpoint.</p><p>Note that this interface aims to only support DSv2 streaming sources. Spark may throw error
if the interface is implemented along with DSv1 streaming sources.</p><p>The callback method will be called once per run.
</p></div><dl class="attributes block"><dt>Since</dt><dd><p>3.3.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.CompositeReadLimit" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="CompositeReadLimitextendsReadLimit" class="anchorToMember"></a><a id="CompositeReadLimit:CompositeReadLimit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/CompositeReadLimit.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">class</span></span> <span class="symbol"><a href="CompositeReadLimit.html" title="/** Represents a ReadLimit where the MicroBatchStream should scan approximately given maximum number of rows with at least the given minimum number of rows."><span class="name">CompositeReadLimit</span></a><span class="result"> extends <a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></span></span><p class="shortcomment cmt"> /**
Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> should scan approximately
given maximum number of rows with at least the given minimum number of rows.</p><div class="fullcomment"><div class="comment cmt"><p> /**
Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> should scan approximately
given maximum number of rows with at least the given minimum number of rows.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.2.0</p></dd><dt>See also</dt><dd><span class="cmt"><p>SupportsAdmissionControl#latestOffset(Offset, ReadLimit)</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="ContinuousPartitionReader[T]extendsPartitionReader[T]" class="anchorToMember"></a><a id="ContinuousPartitionReader[T]:ContinuousPartitionReader[T]" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ContinuousPartitionReader.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="ContinuousPartitionReader.html" title="A variation on PartitionReader for use with continuous streaming processing."><span class="name">ContinuousPartitionReader</span></a><span class="tparams">[<span name="T">T</span>]</span><span class="result"> extends <a href="../PartitionReader.html" name="org.apache.spark.sql.connector.read.PartitionReader" id="org.apache.spark.sql.connector.read.PartitionReader" class="extype">PartitionReader</a>[<span name="org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader.T" class="extype">T</span>]</span></span><p class="shortcomment cmt">A variation on <code><a href="../PartitionReader.html" name="org.apache.spark.sql.connector.read.PartitionReader" id="org.apache.spark.sql.connector.read.PartitionReader" class="extype">PartitionReader</a></code> for use with continuous streaming processing.</p><div class="fullcomment"><div class="comment cmt"><p>A variation on <code><a href="../PartitionReader.html" name="org.apache.spark.sql.connector.read.PartitionReader" id="org.apache.spark.sql.connector.read.PartitionReader" class="extype">PartitionReader</a></code> for use with continuous streaming processing.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReaderFactory" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="ContinuousPartitionReaderFactoryextendsPartitionReaderFactory" class="anchorToMember"></a><a id="ContinuousPartitionReaderFactory:ContinuousPartitionReaderFactory" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ContinuousPartitionReaderFactory.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="ContinuousPartitionReaderFactory.html" title="A variation on PartitionReaderFactory that returns ContinuousPartitionReader instead of PartitionReader."><span class="name">ContinuousPartitionReaderFactory</span></a><span class="result"> extends <a href="../PartitionReaderFactory.html" name="org.apache.spark.sql.connector.read.PartitionReaderFactory" id="org.apache.spark.sql.connector.read.PartitionReaderFactory" class="extype">PartitionReaderFactory</a></span></span><p class="shortcomment cmt">A variation on <code><a href="../PartitionReaderFactory.html" name="org.apache.spark.sql.connector.read.PartitionReaderFactory" id="org.apache.spark.sql.connector.read.PartitionReaderFactory" class="extype">PartitionReaderFactory</a></code> that returns <code><a href="ContinuousPartitionReader.html" name="org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader" id="org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader" class="extype">ContinuousPartitionReader</a></code>
instead of <code><a href="../PartitionReader.html" name="org.apache.spark.sql.connector.read.PartitionReader" id="org.apache.spark.sql.connector.read.PartitionReader" class="extype">PartitionReader</a></code>.</p><div class="fullcomment"><div class="comment cmt"><p>A variation on <code><a href="../PartitionReaderFactory.html" name="org.apache.spark.sql.connector.read.PartitionReaderFactory" id="org.apache.spark.sql.connector.read.PartitionReaderFactory" class="extype">PartitionReaderFactory</a></code> that returns <code><a href="ContinuousPartitionReader.html" name="org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader" id="org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader" class="extype">ContinuousPartitionReader</a></code>
instead of <code><a href="../PartitionReader.html" name="org.apache.spark.sql.connector.read.PartitionReader" id="org.apache.spark.sql.connector.read.PartitionReader" class="extype">PartitionReader</a></code>. It's used for continuous streaming processing.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ContinuousStream" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="ContinuousStreamextendsSparkDataStream" class="anchorToMember"></a><a id="ContinuousStream:ContinuousStream" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ContinuousStream.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="ContinuousStream.html" title="A SparkDataStream for streaming queries with continuous mode."><span class="name">ContinuousStream</span></a><span class="result"> extends <a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></span></span><p class="shortcomment cmt">A <code><a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></code> for streaming queries with continuous mode.</p><div class="fullcomment"><div class="comment cmt"><p>A <code><a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></code> for streaming queries with continuous mode.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="MicroBatchStreamextendsSparkDataStream" class="anchorToMember"></a><a id="MicroBatchStream:MicroBatchStream" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/MicroBatchStream.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="MicroBatchStream.html" title="A SparkDataStream for streaming queries with micro-batch mode."><span class="name">MicroBatchStream</span></a><span class="result"> extends <a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></span></span><p class="shortcomment cmt">A <code><a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></code> for streaming queries with micro-batch mode.</p><div class="fullcomment"><div class="comment cmt"><p>A <code><a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></code> for streaming queries with micro-batch mode.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.Offset" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="OffsetextendsObject" class="anchorToMember"></a><a id="Offset:Offset" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/Offset.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">abstract </span> <span class="kind">class</span></span> <span class="symbol"><a href="Offset.html" title="An abstract representation of progress through a MicroBatchStream or ContinuousStream."><span class="name">Offset</span></a><span class="result"> extends <span name="scala.AnyRef" class="extype">AnyRef</span></span></span><p class="shortcomment cmt">An abstract representation of progress through a <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> or
<code><a href="ContinuousStream.html" name="org.apache.spark.sql.connector.read.streaming.ContinuousStream" id="org.apache.spark.sql.connector.read.streaming.ContinuousStream" class="extype">ContinuousStream</a></code>.</p><div class="fullcomment"><div class="comment cmt"><p>An abstract representation of progress through a <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> or
<code><a href="ContinuousStream.html" name="org.apache.spark.sql.connector.read.streaming.ContinuousStream" id="org.apache.spark.sql.connector.read.streaming.ContinuousStream" class="extype">ContinuousStream</a></code>.</p><p>During execution, offsets provided by the data source implementation will be logged and used as
restart checkpoints. Each source should provide an offset implementation which the source can use
to reconstruct a position in the stream up to which data has been seen/processed.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.PartitionOffset" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="PartitionOffsetextendsSerializable" class="anchorToMember"></a><a id="PartitionOffset:PartitionOffset" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/PartitionOffset.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="PartitionOffset.html" title="Used for per-partition offsets in continuous processing."><span class="name">PartitionOffset</span></a><span class="result"> extends <a href="https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/io/Serializable.html#java.io.Serializable" name="java.io.Serializable" id="java.io.Serializable" class="extype">Serializable</a></span></span><p class="shortcomment cmt">Used for per-partition offsets in continuous processing.</p><div class="fullcomment"><div class="comment cmt"><p>Used for per-partition offsets in continuous processing. ContinuousReader implementations will
provide a method to merge these into a global Offset.</p><p>These offsets must be serializable.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ReadAllAvailable" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="ReadAllAvailableextendsReadLimit" class="anchorToMember"></a><a id="ReadAllAvailable:ReadAllAvailable" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ReadAllAvailable.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">class</span></span> <span class="symbol"><a href="ReadAllAvailable.html" title="Represents a ReadLimit where the MicroBatchStream must scan all the data available at the streaming source."><span class="name">ReadAllAvailable</span></a><span class="result"> extends <a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></span></span><p class="shortcomment cmt">Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> must scan all the data
available at the streaming source.</p><div class="fullcomment"><div class="comment cmt"><p>Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> must scan all the data
available at the streaming source. This is meant to be a hard specification as being able
to return all available data is necessary for <code>Trigger.Once()</code> to work correctly.
If a source is unable to scan all available data, then it must throw an error.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd><dt>See also</dt><dd><span class="cmt"><p>SupportsAdmissionControl#latestOffset(Offset, ReadLimit)</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ReadLimit" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="ReadLimitextendsObject" class="anchorToMember"></a><a id="ReadLimit:ReadLimit" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ReadLimit.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="ReadLimit.html" title="Interface representing limits on how much to read from a MicroBatchStream when it implements SupportsAdmissionControl."><span class="name">ReadLimit</span></a><span class="result"> extends <span name="scala.AnyRef" class="extype">AnyRef</span></span></span><p class="shortcomment cmt">Interface representing limits on how much to read from a <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> when it
implements <code><a href="SupportsAdmissionControl.html" name="org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl" id="org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl" class="extype">SupportsAdmissionControl</a></code>.</p><div class="fullcomment"><div class="comment cmt"><p>Interface representing limits on how much to read from a <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> when it
implements <code><a href="SupportsAdmissionControl.html" name="org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl" id="org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl" class="extype">SupportsAdmissionControl</a></code>. There are several child interfaces representing
various kinds of limits.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd><dt>See also</dt><dd><span class="cmt"><p>SupportsAdmissionControl#latestOffset(Offset, ReadLimit)</p></span><span class="cmt"><p>ReadAllAvailable</p></span><span class="cmt"><p>ReadMaxRows</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ReadMaxBytes" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="ReadMaxBytesextendsReadLimit" class="anchorToMember"></a><a id="ReadMaxBytes:ReadMaxBytes" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ReadMaxBytes.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">class</span></span> <span class="symbol"><a href="ReadMaxBytes.html" title="Represents a ReadLimit where the MicroBatchStream should scan files which total size doesn't go beyond a given maximum total size."><span class="name">ReadMaxBytes</span></a><span class="result"> extends <a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></span></span><p class="shortcomment cmt">Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> should scan files which total
size doesn't go beyond a given maximum total size.</p><div class="fullcomment"><div class="comment cmt"><p>Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> should scan files which total
size doesn't go beyond a given maximum total size. Always reads at least one file so a stream
can make progress and not get stuck on a file larger than a given maximum.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>4.0.0</p></dd><dt>See also</dt><dd><span class="cmt"><p>SupportsAdmissionControl#latestOffset(Offset, ReadLimit)</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ReadMaxFiles" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="ReadMaxFilesextendsReadLimit" class="anchorToMember"></a><a id="ReadMaxFiles:ReadMaxFiles" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ReadMaxFiles.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">class</span></span> <span class="symbol"><a href="ReadMaxFiles.html" title="Represents a ReadLimit where the MicroBatchStream should scan approximately the given maximum number of files."><span class="name">ReadMaxFiles</span></a><span class="result"> extends <a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></span></span><p class="shortcomment cmt">Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> should scan approximately the
given maximum number of files.</p><div class="fullcomment"><div class="comment cmt"><p>Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> should scan approximately the
given maximum number of files.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd><dt>See also</dt><dd><span class="cmt"><p>SupportsAdmissionControl#latestOffset(Offset, ReadLimit)</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ReadMaxRows" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="ReadMaxRowsextendsReadLimit" class="anchorToMember"></a><a id="ReadMaxRows:ReadMaxRows" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ReadMaxRows.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">class</span></span> <span class="symbol"><a href="ReadMaxRows.html" title="Represents a ReadLimit where the MicroBatchStream should scan approximately the given maximum number of rows."><span class="name">ReadMaxRows</span></a><span class="result"> extends <a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></span></span><p class="shortcomment cmt">Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> should scan approximately the
given maximum number of rows.</p><div class="fullcomment"><div class="comment cmt"><p>Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> should scan approximately the
given maximum number of rows.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd><dt>See also</dt><dd><span class="cmt"><p>SupportsAdmissionControl#latestOffset(Offset, ReadLimit)</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ReadMinRows" group="Ungrouped" fullComment="yes" data-isabs="false" visbl="pub"><a id="ReadMinRowsextendsReadLimit" class="anchorToMember"></a><a id="ReadMinRows:ReadMinRows" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ReadMinRows.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier">final </span> <span class="kind">class</span></span> <span class="symbol"><a href="ReadMinRows.html" title="Represents a ReadLimit where the MicroBatchStream should scan approximately at least the given minimum number of rows."><span class="name">ReadMinRows</span></a><span class="result"> extends <a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></span></span><p class="shortcomment cmt">Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> should scan approximately
at least the given minimum number of rows.</p><div class="fullcomment"><div class="comment cmt"><p>Represents a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code> where the <code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> should scan approximately
at least the given minimum number of rows.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.2.0</p></dd><dt>See also</dt><dd><span class="cmt"><p>SupportsAdmissionControl#latestOffset(Offset, ReadLimit)</p></span></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ReportsSinkMetrics" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="ReportsSinkMetricsextendsObject" class="anchorToMember"></a><a id="ReportsSinkMetrics:ReportsSinkMetrics" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ReportsSinkMetrics.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="ReportsSinkMetrics.html" title="A mix-in interface for streaming sinks to signal that they can report metrics."><span class="name">ReportsSinkMetrics</span></a><span class="result"> extends <span name="scala.AnyRef" class="extype">AnyRef</span></span></span><p class="shortcomment cmt">A mix-in interface for streaming sinks to signal that they can report
metrics.</p><div class="fullcomment"><div class="comment cmt"><p>A mix-in interface for streaming sinks to signal that they can report
metrics.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.4.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.ReportsSourceMetrics" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="ReportsSourceMetricsextendsSparkDataStream" class="anchorToMember"></a><a id="ReportsSourceMetrics:ReportsSourceMetrics" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/ReportsSourceMetrics.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="ReportsSourceMetrics.html" title="A mix-in interface for SparkDataStream streaming sources to signal that they can report metrics."><span class="name">ReportsSourceMetrics</span></a><span class="result"> extends <a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></span></span><p class="shortcomment cmt">A mix-in interface for <code><a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></code> streaming sources to signal that they can report
metrics.</p><div class="fullcomment"><div class="comment cmt"><p>A mix-in interface for <code><a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></code> streaming sources to signal that they can report
metrics.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.2.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="SparkDataStreamextendsObject" class="anchorToMember"></a><a id="SparkDataStream:SparkDataStream" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/SparkDataStream.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="SparkDataStream.html" title="The base interface representing a readable data stream in a Spark streaming query."><span class="name">SparkDataStream</span></a><span class="result"> extends <span name="scala.AnyRef" class="extype">AnyRef</span></span></span><p class="shortcomment cmt">The base interface representing a readable data stream in a Spark streaming query.</p><div class="fullcomment"><div class="comment cmt"><p>The base interface representing a readable data stream in a Spark streaming query. It's
responsible to manage the offsets of the streaming source in the streaming query.</p><p>Data sources should implement concrete data stream interfaces:
<code><a href="MicroBatchStream.html" name="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" id="org.apache.spark.sql.connector.read.streaming.MicroBatchStream" class="extype">MicroBatchStream</a></code> and <code><a href="ContinuousStream.html" name="org.apache.spark.sql.connector.read.streaming.ContinuousStream" id="org.apache.spark.sql.connector.read.streaming.ContinuousStream" class="extype">ContinuousStream</a></code>.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="SupportsAdmissionControlextendsSparkDataStream" class="anchorToMember"></a><a id="SupportsAdmissionControl:SupportsAdmissionControl" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/SupportsAdmissionControl.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="SupportsAdmissionControl.html" title="A mix-in interface for SparkDataStream streaming sources to signal that they can control the rate of data ingested into the system."><span class="name">SupportsAdmissionControl</span></a><span class="result"> extends <a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></span></span><p class="shortcomment cmt">A mix-in interface for <code><a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></code> streaming sources to signal that they can control
the rate of data ingested into the system.</p><div class="fullcomment"><div class="comment cmt"><p>A mix-in interface for <code><a href="SparkDataStream.html" name="org.apache.spark.sql.connector.read.streaming.SparkDataStream" id="org.apache.spark.sql.connector.read.streaming.SparkDataStream" class="extype">SparkDataStream</a></code> streaming sources to signal that they can control
the rate of data ingested into the system. These rate limits can come implicitly from the
contract of triggers, e.g. Trigger.Once() requires that a micro-batch process all data
available to the system at the start of the micro-batch. Alternatively, sources can decide to
limit ingest through data source options.</p><p>Through this interface, a MicroBatchStream should be able to return the next offset that it will
process until given a <code><a href="ReadLimit.html" name="org.apache.spark.sql.connector.read.streaming.ReadLimit" id="org.apache.spark.sql.connector.read.streaming.ReadLimit" class="extype">ReadLimit</a></code>.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.0.0</p></dd></dl></div></li><li class="indented0 " name="org.apache.spark.sql.connector.read.streaming.SupportsTriggerAvailableNow" group="Ungrouped" fullComment="yes" data-isabs="true" visbl="pub"><a id="SupportsTriggerAvailableNowextendsSupportsAdmissionControl" class="anchorToMember"></a><a id="SupportsTriggerAvailableNow:SupportsTriggerAvailableNow" class="anchorToMember"></a> <span class="permalink"><a href="../../../../../../../org/apache/spark/sql/connector/read/streaming/SupportsTriggerAvailableNow.html" title="Permalink"><i class="material-icons"></i></a></span> <span class="modifier_kind"><span class="modifier"></span> <span class="kind">trait</span></span> <span class="symbol"><a href="SupportsTriggerAvailableNow.html" title="An interface for streaming sources that supports running in Trigger.AvailableNow mode, which will process all the available data at the beginning of the query in (possibly) multiple batches."><span class="name">SupportsTriggerAvailableNow</span></a><span class="result"> extends <a href="SupportsAdmissionControl.html" name="org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl" id="org.apache.spark.sql.connector.read.streaming.SupportsAdmissionControl" class="extype">SupportsAdmissionControl</a></span></span><p class="shortcomment cmt">An interface for streaming sources that supports running in Trigger.AvailableNow mode, which
will process all the available data at the beginning of the query in (possibly) multiple batches.</p><div class="fullcomment"><div class="comment cmt"><p>An interface for streaming sources that supports running in Trigger.AvailableNow mode, which
will process all the available data at the beginning of the query in (possibly) multiple batches.</p><p>This mode will have better scalability comparing to Trigger.Once mode.
</p></div><dl class="attributes block"><dt>Annotations</dt><dd><span class="name">@Evolving</span><span class="args">()</span> </dd><dt>Since</dt><dd><p>3.3.0</p></dd></dl></div></li></ol></div></div><div id="inheritedMembers"></div><div id="groupedMembers"><div name="Ungrouped" class="group"><h3>Ungrouped</h3></div></div></div><div id="tooltip"></div><div id="footer"></div></body></div></div></div></body></html>
