



<!DOCTYPE html>
<html class="no-js">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <title>Performance Tuning - Spark 4.0.0-preview2 Documentation</title>
        

        


        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,wght@0,400;0,500;0,700;1,400;1,500;1,700&Courier+Prime:wght@400;700&display=swap" rel="stylesheet">
        <link href="css/custom.css" rel="stylesheet">
        <script src="/js/vendor/modernizr-2.6.1-respond-1.1.0.min.js"></script>

        <link rel="stylesheet" href="css/pygments-default.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
        <link rel="stylesheet" href="css/docsearch.css">

        
        <!-- Matomo -->
        <script>
            var _paq = window._paq = window._paq || [];
            /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
            _paq.push(["disableCookies"]);
            _paq.push(['trackPageView']);
            _paq.push(['enableLinkTracking']);
            (function() {
              var u="https://analytics.apache.org/";
              _paq.push(['setTrackerUrl', u+'matomo.php']);
              _paq.push(['setSiteId', '40']);
              var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
              g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
            })();
        </script>
        <!-- End Matomo Code -->
        

    </head>
    <body class="global">
        <!-- This code is taken from http://twitter.github.com/bootstrap/examples/hero.html -->
        <nav class="navbar navbar-expand-lg navbar-dark p-0 px-4 fixed-top" style="background: #1d6890;" id="topbar">
            <div class="navbar-brand"><a href="index.html">
                <img src="https://spark.apache.org/images/spark-logo-rev.svg" width="141" height="72"/></a><span class="version">4.0.0-preview2</span>
            </div>
            <button class="navbar-toggler" type="button" data-toggle="collapse"
                    data-target="#navbarCollapse" aria-controls="navbarCollapse"
                    aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarCollapse">
                <ul class="navbar-nav me-auto">
                    <li class="nav-item"><a href="index.html" class="nav-link">Overview</a></li>

                    <li class="nav-item dropdown">
                        <a href="#" class="nav-link dropdown-toggle" id="navbarQuickStart" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Programming Guides</a>
                        <div class="dropdown-menu" aria-labelledby="navbarQuickStart">
                            <a class="dropdown-item" href="quick-start.html">Quick Start</a>
                            <a class="dropdown-item" href="rdd-programming-guide.html">RDDs, Accumulators, Broadcasts Vars</a>
                            <a class="dropdown-item" href="sql-programming-guide.html">SQL, DataFrames, and Datasets</a>
                            <a class="dropdown-item" href="streaming/index.html">Structured Streaming</a>
                            <a class="dropdown-item" href="streaming-programming-guide.html">Spark Streaming (DStreams)</a>
                            <a class="dropdown-item" href="ml-guide.html">MLlib (Machine Learning)</a>
                            <a class="dropdown-item" href="graphx-programming-guide.html">GraphX (Graph Processing)</a>
                            <a class="dropdown-item" href="sparkr.html">SparkR (R on Spark)</a>
                            <a class="dropdown-item" href="api/python/getting_started/index.html">PySpark (Python on Spark)</a>
                        </div>
                    </li>

                    <li class="nav-item dropdown">
                        <a href="#" class="nav-link dropdown-toggle" id="navbarAPIDocs" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">API Docs</a>
                        <div class="dropdown-menu" aria-labelledby="navbarAPIDocs">
                            <a class="dropdown-item" href="api/python/index.html">Python</a>
                            <a class="dropdown-item" href="api/scala/org/apache/spark/index.html">Scala</a>
                            <a class="dropdown-item" href="api/java/index.html">Java</a>
                            <a class="dropdown-item" href="api/R/index.html">R</a>
                            <a class="dropdown-item" href="api/sql/index.html">SQL, Built-in Functions</a>
                        </div>
                    </li>

                    <li class="nav-item dropdown">
                        <a href="#" class="nav-link dropdown-toggle" id="navbarDeploying" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Deploying</a>
                        <div class="dropdown-menu" aria-labelledby="navbarDeploying">
                            <a class="dropdown-item" href="cluster-overview.html">Overview</a>
                            <a class="dropdown-item" href="submitting-applications.html">Submitting Applications</a>
                            <div class="dropdown-divider"></div>
                            <a class="dropdown-item" href="spark-standalone.html">Spark Standalone</a>
                            <a class="dropdown-item" href="running-on-yarn.html">YARN</a>
                            <a class="dropdown-item" href="running-on-kubernetes.html">Kubernetes</a>
                        </div>
                    </li>

                    <li class="nav-item dropdown">
                        <a href="#" class="nav-link dropdown-toggle" id="navbarMore" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a>
                        <div class="dropdown-menu" aria-labelledby="navbarMore">
                            <a class="dropdown-item" href="configuration.html">Configuration</a>
                            <a class="dropdown-item" href="monitoring.html">Monitoring</a>
                            <a class="dropdown-item" href="tuning.html">Tuning Guide</a>
                            <a class="dropdown-item" href="job-scheduling.html">Job Scheduling</a>
                            <a class="dropdown-item" href="security.html">Security</a>
                            <a class="dropdown-item" href="hardware-provisioning.html">Hardware Provisioning</a>
                            <a class="dropdown-item" href="migration-guide.html">Migration Guide</a>
                            <div class="dropdown-divider"></div>
                            <a class="dropdown-item" href="building-spark.html">Building Spark</a>
                            <a class="dropdown-item" href="https://spark.apache.org/contributing.html">Contributing to Spark</a>
                            <a class="dropdown-item" href="https://spark.apache.org/third-party-projects.html">Third Party Projects</a>
                        </div>
                    </li>

                    <li class="nav-item">
                        <input type="text" id="docsearch-input" placeholder="Search the docsâ€¦">
                    </li>
                </ul>
                <!--<span class="navbar-text navbar-right"><span class="version-text">v4.0.0-preview2</span></span>-->
            </div>
        </nav>

        

        <div class="container">
            
                
                    <div class="left-menu-wrapper">
    <div class="left-menu">
        <h3><a href="sql-programming-guide.html">Spark SQL Guide</a></h3>
        
<ul>

    <li>
        <a href="sql-getting-started.html">
            
                Getting Started
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-data-sources.html">
            
                Data Sources
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-performance-tuning.html">
            
                Performance Tuning
            
        </a>
    </li>
    
    
        
<ul>

    <li>
        <a href="sql-performance-tuning.html#caching-data">
            
                Caching Data
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-performance-tuning.html#tuning-partitions">
            
                Tuning Partitions
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-performance-tuning.html#leveraging-statistics">
            
                Leveraging Statistics
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-performance-tuning.html#optimizing-the-join-strategy">
            
                Optimizing the Join Strategy
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-performance-tuning.html#adaptive-query-execution">
            
                Adaptive Query Execution
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-performance-tuning.html#storage-partition-join">
            
                Storage Partition Join
            
        </a>
    </li>
    
    

</ul>

    

    <li>
        <a href="sql-distributed-sql-engine.html">
            
                Distributed SQL Engine
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-pyspark-pandas-with-arrow.html">
            
                PySpark Usage Guide for Pandas with Apache Arrow
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-migration-guide.html">
            
                Migration Guide
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-ref.html">
            
                SQL Reference
            
        </a>
    </li>
    
    

    <li>
        <a href="sql-error-conditions.html">
            
                Error Conditions
            
        </a>
    </li>
    
    

</ul>

    </div>
</div>
                
                <input id="nav-trigger" class="nav-trigger" checked type="checkbox">
                <label for="nav-trigger"></label>
                <div class="content-with-sidebar mr-3" id="content">
                    
                        <h1 class="title">Performance Tuning</h1>
                    

                    <p>Spark offers many techniques for tuning the performance of DataFrame or SQL workloads. Those techniques, broadly speaking, include caching data, altering how datasets are partitioned, selecting the optimal join strategy, and providing the optimizer with additional information it can use to build more efficient execution plans.</p>

<ul id="markdown-toc">
  <li><a href="#caching-data" id="markdown-toc-caching-data">Caching Data</a></li>
  <li><a href="#tuning-partitions" id="markdown-toc-tuning-partitions">Tuning Partitions</a>    <ul>
      <li><a href="#coalesce-hints" id="markdown-toc-coalesce-hints">Coalesce Hints</a></li>
    </ul>
  </li>
  <li><a href="#leveraging-statistics" id="markdown-toc-leveraging-statistics">Leveraging Statistics</a></li>
  <li><a href="#optimizing-the-join-strategy" id="markdown-toc-optimizing-the-join-strategy">Optimizing the Join Strategy</a>    <ul>
      <li><a href="#automatically-broadcasting-joins" id="markdown-toc-automatically-broadcasting-joins">Automatically Broadcasting Joins</a></li>
      <li><a href="#join-strategy-hints" id="markdown-toc-join-strategy-hints">Join Strategy Hints</a></li>
    </ul>
  </li>
  <li><a href="#adaptive-query-execution" id="markdown-toc-adaptive-query-execution">Adaptive Query Execution</a>    <ul>
      <li><a href="#coalescing-post-shuffle-partitions" id="markdown-toc-coalescing-post-shuffle-partitions">Coalescing Post Shuffle Partitions</a></li>
      <li><a href="#splitting-skewed-shuffle-partitions" id="markdown-toc-splitting-skewed-shuffle-partitions">Splitting skewed shuffle partitions</a></li>
      <li><a href="#converting-sort-merge-join-to-broadcast-join" id="markdown-toc-converting-sort-merge-join-to-broadcast-join">Converting sort-merge join to broadcast join</a></li>
      <li><a href="#converting-sort-merge-join-to-shuffled-hash-join" id="markdown-toc-converting-sort-merge-join-to-shuffled-hash-join">Converting sort-merge join to shuffled hash join</a></li>
      <li><a href="#optimizing-skew-join" id="markdown-toc-optimizing-skew-join">Optimizing Skew Join</a></li>
      <li><a href="#advanced-customization" id="markdown-toc-advanced-customization">Advanced Customization</a></li>
    </ul>
  </li>
  <li><a href="#storage-partition-join" id="markdown-toc-storage-partition-join">Storage Partition Join</a></li>
</ul>

<h2 id="caching-data">Caching Data</h2>

<p>Spark SQL can cache tables using an in-memory columnar format by calling <code class="language-plaintext highlighter-rouge">spark.catalog.cacheTable("tableName")</code> or <code class="language-plaintext highlighter-rouge">dataFrame.cache()</code>.
Then Spark SQL will scan only required columns and will automatically tune compression to minimize
memory usage and GC pressure. You can call <code class="language-plaintext highlighter-rouge">spark.catalog.uncacheTable("tableName")</code> or <code class="language-plaintext highlighter-rouge">dataFrame.unpersist()</code> to remove the table from memory.</p>

<p>Configuration of in-memory caching can be done via <code class="language-plaintext highlighter-rouge">spark.conf.set</code> or by running
<code class="language-plaintext highlighter-rouge">SET key=value</code> commands using SQL.</p>

<table class="spark-config">
<thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
<tr>
  <td><code>spark.sql.inMemoryColumnarStorage.compressed</code></td>
  <td>true</td>
  <td>
    When set to true, Spark SQL will automatically select a compression codec for each column based
    on statistics of the data.
  </td>
  <td>1.0.1</td>
</tr>
<tr>
  <td><code>spark.sql.inMemoryColumnarStorage.batchSize</code></td>
  <td>10000</td>
  <td>
    Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization
    and compression, but risk OOMs when caching data.
  </td>
  <td>1.1.1</td>
</tr>
</table>

<h2 id="tuning-partitions">Tuning Partitions</h2>

<table class="spark-config">
  <thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
  <tr>
    <td><code>spark.sql.files.maxPartitionBytes</code></td>
    <td>134217728 (128 MB)</td>
    <td>
      The maximum number of bytes to pack into a single partition when reading files.
      This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.
    </td>
    <td>2.0.0</td>
  </tr>
  <tr>
    <td><code>spark.sql.files.openCostInBytes</code></td>
    <td>4194304 (4 MB)</td>
    <td>
      The estimated cost to open a file, measured by the number of bytes that could be scanned in the same
      time. This is used when putting multiple files into a partition. It is better to over-estimate,
      then the partitions with small files will be faster than partitions with bigger files (which is
      scheduled first). This configuration is effective only when using file-based sources such as Parquet,
      JSON and ORC.
    </td>
    <td>2.0.0</td>
  </tr>
  <tr>
    <td><code>spark.sql.files.minPartitionNum</code></td>
    <td>Default Parallelism</td>
    <td>
      The suggested (not guaranteed) minimum number of split file partitions. If not set, the default
      value is `spark.sql.leafNodeDefaultParallelism`. This configuration is effective only when using file-based
      sources such as Parquet, JSON and ORC.
    </td>
    <td>3.1.0</td>
  </tr>
  <tr>
    <td><code>spark.sql.files.maxPartitionNum</code></td>
    <td>None</td>
    <td>
      The suggested (not guaranteed) maximum number of split file partitions. If it is set,
      Spark will rescale each partition to make the number of partitions is close to this
      value if the initial number of partitions exceeds this value. This configuration is
      effective only when using file-based sources such as Parquet, JSON and ORC.
    </td>
    <td>3.5.0</td>
  </tr>
  <tr>
    <td><code>spark.sql.shuffle.partitions</code></td>
    <td>200</td>
    <td>
      Configures the number of partitions to use when shuffling data for joins or aggregations.
    </td>
    <td>1.1.0</td>
  </tr>
  <tr>
    <td><code>spark.sql.sources.parallelPartitionDiscovery.threshold</code></td>
    <td>32</td>
    <td>
      Configures the threshold to enable parallel listing for job input paths. If the number of
      input paths is larger than this threshold, Spark will list the files by using Spark distributed job.
      Otherwise, it will fallback to sequential listing. This configuration is only effective when
      using file-based data sources such as Parquet, ORC and JSON.
    </td>
    <td>1.5.0</td>
  </tr>
  <tr>
    <td><code>spark.sql.sources.parallelPartitionDiscovery.parallelism</code></td>
    <td>10000</td>
    <td>
      Configures the maximum listing parallelism for job input paths. In case the number of input
      paths is larger than this value, it will be throttled down to use this value. This configuration is only effective when using file-based data sources such as Parquet, ORC
      and JSON.
    </td>
    <td>2.1.1</td>
  </tr>
</table>

<h3 id="coalesce-hints">Coalesce Hints</h3>

<p>Coalesce hints allow Spark SQL users to control the number of output files just like
<code class="language-plaintext highlighter-rouge">coalesce</code>, <code class="language-plaintext highlighter-rouge">repartition</code> and <code class="language-plaintext highlighter-rouge">repartitionByRange</code> in the Dataset API, they can be used for performance
tuning and reducing the number of output files. The &#8220;COALESCE&#8221; hint only has a partition number as a
parameter. The &#8220;REPARTITION&#8221; hint has a partition number, columns, or both/neither of them as parameters.
The &#8220;REPARTITION_BY_RANGE&#8221; hint must have column names and a partition number is optional. The &#8220;REBALANCE&#8221;
hint has an initial partition number, columns, or both/neither of them as parameters.</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">SELECT</span> <span class="cm">/*+ COALESCE(3) */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
<span class="k">SELECT</span> <span class="cm">/*+ REPARTITION(3) */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
<span class="k">SELECT</span> <span class="cm">/*+ REPARTITION(c) */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
<span class="k">SELECT</span> <span class="cm">/*+ REPARTITION(3, c) */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
<span class="k">SELECT</span> <span class="cm">/*+ REPARTITION */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
<span class="k">SELECT</span> <span class="cm">/*+ REPARTITION_BY_RANGE(c) */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
<span class="k">SELECT</span> <span class="cm">/*+ REPARTITION_BY_RANGE(3, c) */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
<span class="k">SELECT</span> <span class="cm">/*+ REBALANCE */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
<span class="k">SELECT</span> <span class="cm">/*+ REBALANCE(3) */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
<span class="k">SELECT</span> <span class="cm">/*+ REBALANCE(c) */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
<span class="k">SELECT</span> <span class="cm">/*+ REBALANCE(3, c) */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">t</span><span class="p">;</span>
</code></pre></div></div>

<p>For more details please refer to the documentation of <a href="sql-ref-syntax-qry-select-hints.html#partitioning-hints">Partitioning Hints</a>.</p>

<h2 id="leveraging-statistics">Leveraging Statistics</h2>
<p>Apache Spark&#8217;s ability to choose the best execution plan among many possible options is determined in part by its estimates of how many rows will be output by every node in the execution plan (read, filter, join, etc.). Those estimates in turn are based on statistics that are made available to Spark in one of several ways:</p>

<ul>
  <li><strong>Data source</strong>: Statistics that Spark reads directly from the underlying data source, like the counts and min/max values in the metadata of Parquet files. These statistics are maintained by the underlying data source.</li>
  <li><strong>Catalog</strong>: Statistics that Spark reads from the catalog, like the Hive Metastore. These statistics are collected or updated whenever you run <a href="sql-ref-syntax-aux-analyze-table.html"><code class="language-plaintext highlighter-rouge">ANALYZE TABLE</code></a>.</li>
  <li><strong>Runtime</strong>: Statistics that Spark computes itself as a query is running. This is part of the <a href="#adaptive-query-execution">adaptive query execution framework</a>.</li>
</ul>

<p>Missing or inaccurate statistics will hinder Spark&#8217;s ability to select an optimal plan, and may lead to poor query performance. It&#8217;s helpful then to inspect the statistics available to Spark and the estimates it makes during query planning and execution.</p>

<ul>
  <li><strong>Data object statistics</strong>: You can inspect the statistics on a table or column with <a href="sql-ref-syntax-aux-describe-table.html"><code class="language-plaintext highlighter-rouge">DESCRIBE EXTENDED</code></a>.</li>
  <li><strong>Query plan estimates</strong>: You can inspect Spark&#8217;s cost estimates in the optimized query plan via <a href="sql-ref-syntax-qry-explain.html"><code class="language-plaintext highlighter-rouge">EXPLAIN COST</code></a> or <code class="language-plaintext highlighter-rouge">DataFrame.explain(mode="cost")</code>.</li>
  <li><strong>Runtime statistics</strong>: You can inspect these statistics in the <a href="web-ui.html#sql-tab">SQL UI</a> under the &#8220;Details&#8221; section as a query is running. Look for <code class="language-plaintext highlighter-rouge">Statistics(..., isRuntime=true)</code> in the plan.</li>
</ul>

<h2 id="optimizing-the-join-strategy">Optimizing the Join Strategy</h2>

<h3 id="automatically-broadcasting-joins">Automatically Broadcasting Joins</h3>

<table class="spark-config">
  <thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
  <tr>
    <td><code>spark.sql.autoBroadcastJoinThreshold</code></td>
    <td>10485760 (10 MB)</td>
    <td>
      Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when
      performing a join. By setting this value to -1, broadcasting can be disabled.
    </td>
    <td>1.1.0</td>
  </tr>
  <tr>
    <td><code>spark.sql.broadcastTimeout</code></td>
    <td>300</td>
    <td>
      <p>
        Timeout in seconds for the broadcast wait time in broadcast joins
      </p>
    </td>
    <td>1.3.0</td>
  </tr>
</table>

<h3 id="join-strategy-hints">Join Strategy Hints</h3>

<p>The join strategy hints, namely <code class="language-plaintext highlighter-rouge">BROADCAST</code>, <code class="language-plaintext highlighter-rouge">MERGE</code>, <code class="language-plaintext highlighter-rouge">SHUFFLE_HASH</code> and <code class="language-plaintext highlighter-rouge">SHUFFLE_REPLICATE_NL</code>,
instruct Spark to use the hinted strategy on each specified relation when joining them with another
relation. For example, when the <code class="language-plaintext highlighter-rouge">BROADCAST</code> hint is used on table &#8216;t1&#8217;, broadcast join (either
broadcast hash join or broadcast nested loop join depending on whether there is any equi-join key)
with &#8216;t1&#8217; as the build side will be prioritized by Spark even if the size of table &#8216;t1&#8217; suggested
by the statistics is above the configuration <code class="language-plaintext highlighter-rouge">spark.sql.autoBroadcastJoinThreshold</code>.</p>

<p>When different join strategy hints are specified on both sides of a join, Spark prioritizes the
<code class="language-plaintext highlighter-rouge">BROADCAST</code> hint over the <code class="language-plaintext highlighter-rouge">MERGE</code> hint over the <code class="language-plaintext highlighter-rouge">SHUFFLE_HASH</code> hint over the <code class="language-plaintext highlighter-rouge">SHUFFLE_REPLICATE_NL</code>
hint. When both sides are specified with the <code class="language-plaintext highlighter-rouge">BROADCAST</code> hint or the <code class="language-plaintext highlighter-rouge">SHUFFLE_HASH</code> hint, Spark will
pick the build side based on the join type and the sizes of the relations.</p>

<p>Note that there is no guarantee that Spark will choose the join strategy specified in the hint since
a specific strategy may not support all join types.</p>

<div class="codetabs">
<div data-lang="python">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">spark</span><span class="p">.</span><span class="n">table</span><span class="p">(</span><span class="s">"src"</span><span class="p">).</span><span class="n">join</span><span class="p">(</span><span class="n">spark</span><span class="p">.</span><span class="n">table</span><span class="p">(</span><span class="s">"records"</span><span class="p">).</span><span class="n">hint</span><span class="p">(</span><span class="s">"broadcast"</span><span class="p">),</span> <span class="s">"key"</span><span class="p">).</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>
<div data-lang="scala">
    <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">spark</span><span class="o">.</span><span class="py">table</span><span class="o">(</span><span class="s">"src"</span><span class="o">).</span><span class="py">join</span><span class="o">(</span><span class="nv">spark</span><span class="o">.</span><span class="py">table</span><span class="o">(</span><span class="s">"records"</span><span class="o">).</span><span class="py">hint</span><span class="o">(</span><span class="s">"broadcast"</span><span class="o">),</span> <span class="s">"key"</span><span class="o">).</span><span class="py">show</span><span class="o">()</span>
</code></pre></div>    </div>
  </div>
<div data-lang="java">
    <div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">spark</span><span class="o">.</span><span class="na">table</span><span class="o">(</span><span class="s">"src"</span><span class="o">).</span><span class="na">join</span><span class="o">(</span><span class="n">spark</span><span class="o">.</span><span class="na">table</span><span class="o">(</span><span class="s">"records"</span><span class="o">).</span><span class="na">hint</span><span class="o">(</span><span class="s">"broadcast"</span><span class="o">),</span> <span class="s">"key"</span><span class="o">).</span><span class="na">show</span><span class="o">();</span>
</code></pre></div>    </div>
  </div>
<div data-lang="r">
    <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">src</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sql</span><span class="p">(</span><span class="s2">"SELECT * FROM src"</span><span class="p">)</span><span class="w">
</span><span class="n">records</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sql</span><span class="p">(</span><span class="s2">"SELECT * FROM records"</span><span class="p">)</span><span class="w">
</span><span class="n">head</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">hint</span><span class="p">(</span><span class="n">records</span><span class="p">,</span><span class="w"> </span><span class="s2">"broadcast"</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="o">$</span><span class="n">key</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">records</span><span class="o">$</span><span class="n">key</span><span class="p">))</span><span class="w">
</span></code></pre></div>    </div>
  </div>
<div data-lang="SQL">
    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- We accept BROADCAST, BROADCASTJOIN and MAPJOIN for broadcast hint</span>
<span class="k">SELECT</span> <span class="cm">/*+ BROADCAST(r) */</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">records</span> <span class="n">r</span> <span class="k">JOIN</span> <span class="n">src</span> <span class="n">s</span> <span class="k">ON</span> <span class="n">r</span><span class="p">.</span><span class="k">key</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="k">key</span>
</code></pre></div>    </div>
  </div>
</div>

<p>For more details please refer to the documentation of <a href="sql-ref-syntax-qry-select-hints.html#join-hints">Join Hints</a>.</p>

<h2 id="adaptive-query-execution">Adaptive Query Execution</h2>
<p>Adaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of the runtime statistics to choose the most efficient query execution plan, which is enabled by default since Apache Spark 3.2.0. Spark SQL can turn on and off AQE by <code class="language-plaintext highlighter-rouge">spark.sql.adaptive.enabled</code> as an umbrella configuration.</p>

<table class="spark-config">
   <thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
   <tr>
     <td><code>spark.sql.adaptive.enabled</code></td>
     <td>true</td>
     <td>
     When true, enable adaptive query execution, which re-optimizes the query plan in the middle of query execution, based on accurate runtime statistics.
     </td>
     <td>1.6.0</td>
   </tr>
</table>

<h3 id="coalescing-post-shuffle-partitions">Coalescing Post Shuffle Partitions</h3>
<p>This feature coalesces the post shuffle partitions based on the map output statistics when both <code class="language-plaintext highlighter-rouge">spark.sql.adaptive.enabled</code> and <code class="language-plaintext highlighter-rouge">spark.sql.adaptive.coalescePartitions.enabled</code> configurations are true. This feature simplifies the tuning of shuffle partition number when running queries. You do not need to set a proper shuffle partition number to fit your dataset. Spark can pick the proper shuffle partition number at runtime once you set a large enough initial number of shuffle partitions via <code class="language-plaintext highlighter-rouge">spark.sql.adaptive.coalescePartitions.initialPartitionNum</code> configuration.</p>
<table class="spark-config">
   <thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
   <tr>
     <td><code>spark.sql.adaptive.coalescePartitions.enabled</code></td>
     <td>true</td>
     <td>
       When true and <code>spark.sql.adaptive.enabled</code> is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by <code>spark.sql.adaptive.advisoryPartitionSizeInBytes</code>), to avoid too many small tasks.
     </td>
     <td>3.0.0</td>
   </tr>
   <tr>
     <td><code>spark.sql.adaptive.coalescePartitions.parallelismFirst</code></td>
     <td>true</td>
     <td>
       When true, Spark ignores the target size specified by <code>spark.sql.adaptive.advisoryPartitionSizeInBytes</code> (default 64MB) when coalescing contiguous shuffle partitions, and only respect the minimum partition size specified by <code>spark.sql.adaptive.coalescePartitions.minPartitionSize</code> (default 1MB), to maximize the parallelism. This is to avoid performance regressions when enabling adaptive query execution. It's recommended to set this config to false on a busy cluster to make resource utilization more efficient (not many small tasks).
     </td>
     <td>3.2.0</td>
   </tr>
   <tr>
     <td><code>spark.sql.adaptive.coalescePartitions.minPartitionSize</code></td>
     <td>1MB</td>
     <td>
       The minimum size of shuffle partitions after coalescing. Its value can be at most 20% of <code>spark.sql.adaptive.advisoryPartitionSizeInBytes</code>. This is useful when the target size is ignored during partition coalescing, which is the default case.
     </td>
     <td>3.2.0</td>
   </tr>
   <tr>
     <td><code>spark.sql.adaptive.coalescePartitions.initialPartitionNum</code></td>
     <td>(none)</td>
     <td>
       The initial number of shuffle partitions before coalescing. If not set, it equals to <code>spark.sql.shuffle.partitions</code>. This configuration only has an effect when <code>spark.sql.adaptive.enabled</code> and <code>spark.sql.adaptive.coalescePartitions.enabled</code> are both enabled.
     </td>
     <td>3.0.0</td>
   </tr>
   <tr>
     <td><code>spark.sql.adaptive.advisoryPartitionSizeInBytes</code></td>
     <td>64 MB</td>
     <td>
       The advisory size in bytes of the shuffle partition during adaptive optimization (when <code>spark.sql.adaptive.enabled</code> is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.
     </td>
     <td>3.0.0</td>
   </tr>
 </table>

<h3 id="splitting-skewed-shuffle-partitions">Splitting skewed shuffle partitions</h3>
<table class="spark-config">
   <thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
   <tr>
     <td><code>spark.sql.adaptive.optimizeSkewsInRebalancePartitions.enabled</code></td>
     <td>true</td>
     <td>
       When true and <code>spark.sql.adaptive.enabled</code> is true, Spark will optimize the skewed shuffle partitions in RebalancePartitions and split them to smaller ones according to the target size (specified by <code>spark.sql.adaptive.advisoryPartitionSizeInBytes</code>), to avoid data skew.
     </td>
     <td>3.2.0</td>
   </tr>
   <tr>
     <td><code>spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor</code></td>
     <td>0.2</td>
     <td>
       A partition will be merged during splitting if its size is small than this factor multiply <code>spark.sql.adaptive.advisoryPartitionSizeInBytes</code>.
     </td>
     <td>3.3.0</td>
   </tr>
 </table>

<h3 id="converting-sort-merge-join-to-broadcast-join">Converting sort-merge join to broadcast join</h3>
<p>AQE converts sort-merge join to broadcast hash join when the runtime statistics of any join side are smaller than the adaptive broadcast hash join threshold. This is not as efficient as planning a broadcast hash join in the first place, but it&#8217;s better than continuing the sort-merge join, as we can avoid sorting both join sides and read shuffle files locally to save network traffic (provided <code class="language-plaintext highlighter-rouge">spark.sql.adaptive.localShuffleReader.enabled</code> is true).</p>

<table class="spark-config">
     <thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
     <tr>
       <td><code>spark.sql.adaptive.autoBroadcastJoinThreshold</code></td>
       <td>(none)</td>
       <td>
         Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1, broadcasting can be disabled. The default value is the same as <code>spark.sql.autoBroadcastJoinThreshold</code>. Note that, this config is used only in adaptive framework.
       </td>
       <td>3.2.0</td>
     </tr>
     <tr>
       <td><code>spark.sql.adaptive.localShuffleReader.enabled</code></td>
       <td>true</td>
       <td>
         When true and <code>spark.sql.adaptive.enabled</code> is true, Spark tries to use local shuffle reader to read the shuffle data when the shuffle partitioning is not needed, for example, after converting sort-merge join to broadcast-hash join.
       </td>
       <td>3.0.0</td>
     </tr>
  </table>

<h3 id="converting-sort-merge-join-to-shuffled-hash-join">Converting sort-merge join to shuffled hash join</h3>
<p>AQE converts sort-merge join to shuffled hash join when all post shuffle partitions are smaller than the threshold configured in <code class="language-plaintext highlighter-rouge">spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold</code>.</p>

<table class="spark-config">
     <thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
     <tr>
       <td><code>spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold</code></td>
       <td>0</td>
       <td>
         Configures the maximum size in bytes per partition that can be allowed to build local hash map. If this value is not smaller than <code>spark.sql.adaptive.advisoryPartitionSizeInBytes</code> and all the partition sizes are not larger than this config, join selection prefers to use shuffled hash join instead of sort merge join regardless of the value of <code>spark.sql.join.preferSortMergeJoin</code>.
       </td>
       <td>3.2.0</td>
     </tr>
  </table>

<h3 id="optimizing-skew-join">Optimizing Skew Join</h3>
<p>Data skew can severely downgrade the performance of join queries. This feature dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed tasks into roughly evenly sized tasks. It takes effect when both <code class="language-plaintext highlighter-rouge">spark.sql.adaptive.enabled</code> and <code class="language-plaintext highlighter-rouge">spark.sql.adaptive.skewJoin.enabled</code> configurations are enabled.</p>
<table class="spark-config">
     <thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
     <tr>
       <td><code>spark.sql.adaptive.skewJoin.enabled</code></td>
       <td>true</td>
       <td>
         When true and <code>spark.sql.adaptive.enabled</code> is true, Spark dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed partitions.
       </td>
       <td>3.0.0</td>
     </tr>
     <tr>
       <td><code>spark.sql.adaptive.skewJoin.skewedPartitionFactor</code></td>
       <td>5.0</td>
       <td>
         A partition is considered as skewed if its size is larger than this factor multiplying the median partition size and also larger than <code>spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes</code>.
       </td>
       <td>3.0.0</td>
     </tr>
     <tr>
       <td><code>spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes</code></td>
       <td>256MB</td>
       <td>
         A partition is considered as skewed if its size in bytes is larger than this threshold and also larger than <code>spark.sql.adaptive.skewJoin.skewedPartitionFactor</code> multiplying the median partition size. Ideally, this config should be set larger than <code>spark.sql.adaptive.advisoryPartitionSizeInBytes</code>.
       </td>
       <td>3.0.0</td>
     </tr>
     <tr>
       <td><code>spark.sql.adaptive.forceOptimizeSkewedJoin</code></td>
       <td>false</td>
       <td>
         When true, force enable OptimizeSkewedJoin, which is an adaptive rule to optimize skewed joins to avoid straggler tasks, even if it introduces extra shuffle.
       </td>
       <td>3.3.0</td>
     </tr>
   </table>

<h3 id="advanced-customization">Advanced Customization</h3>

<p>You can control the details of how AQE works by providing your own cost evaluator class or by excluding AQE optimizer rules.</p>

<table class="spark-config">
    <thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
    <tr>
      <td><code>spark.sql.adaptive.optimizer.excludedRules</code></td>
      <td>(none)</td>
      <td>
        Configures a list of rules to be disabled in the adaptive optimizer, in which the rules are specified by their rule names and separated by comma. The optimizer will log the rules that have indeed been excluded.
      </td>
      <td>3.1.0</td>
    </tr>
    <tr>
      <td><code>spark.sql.adaptive.customCostEvaluatorClass</code></td>
      <td>(none)</td>
      <td>
        The custom cost evaluator class to be used for adaptive execution. If not being set, Spark will use its own <code>SimpleCostEvaluator</code> by default.
      </td>
      <td>3.2.0</td>
    </tr>
  </table>

<h2 id="storage-partition-join">Storage Partition Join</h2>

<p>Storage Partition Join (SPJ) is an optimization technique in Spark SQL that makes use the existing storage layout to avoid the shuffle phase.</p>

<p>This is a generalization of the concept of Bucket Joins, which is only applicable for <a href="sql-data-sources-load-save-functions.html#bucketing-sorting-and-partitioning">bucketed</a> tables, to tables partitioned by functions registered in FunctionCatalog. Storage Partition Joins are currently supported for compatible V2 DataSources.</p>

<p>The following SQL properties enable Storage Partition Join in different join queries with various optimizations.</p>

<table class="spark-config">
    <thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr></thead>
    <tr>
      <td><code>spark.sql.sources.v2.bucketing.enabled</code></td>
      <td>false</td>
      <td>
        When true, try to eliminate shuffle by using the partitioning reported by a compatible V2 data source.
      </td>
      <td>3.3.0</td>
    </tr>
    <tr>
      <td><code>spark.sql.sources.v2.bucketing.pushPartValues.enabled</code></td>
      <td>true</td>
      <td>
        When enabled, try to eliminate shuffle if one side of the join has missing partition values from the other side. This config requires <code>spark.sql.sources.v2.bucketing.enabled</code> to be true.
      </td>
      <td>3.4.0</td>
    </tr>
    <tr>
      <td><code>spark.sql.requireAllClusterKeysForCoPartition</code></td>
      <td>true</td>
      <td>
        When true, require the join or MERGE keys to be same and in the same order as the partition keys to eliminate shuffle. Hence, set to <b>false</b> in this situation to eliminate shuffle.
      </td>
      <td>3.4.0</td>
    </tr>
    <tr>
      <td><code>spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled</code></td>
      <td>false</td>
      <td>
        When true, and when the join is not a full outer join, enable skew optimizations to handle partitions with large amounts of data when avoiding shuffle. One side will be chosen as the big table based on table statistics, and the splits on this side will be partially-clustered. The splits of the other side will be grouped and replicated to match. This config requires both <code>spark.sql.sources.v2.bucketing.enabled</code> and <code>spark.sql.sources.v2.bucketing.pushPartValues.enabled</code> to be true.
      </td>
      <td>3.4.0</td>
    </tr>
    <tr>
      <td><code>spark.sql.sources.v2.bucketing.allowJoinKeysSubsetOfPartitionKeys.enabled</code></td>
      <td>false</td>
      <td>
        When enabled, try to avoid shuffle if join or MERGE condition does not include all partition columns. This config requires both <code>spark.sql.sources.v2.bucketing.enabled</code> and <code>spark.sql.sources.v2.bucketing.pushPartValues.enabled</code> to be true, and <code>spark.sql.requireAllClusterKeysForCoPartition</code> to be false.
      </td>
      <td>4.0.0</td>
    </tr>
    <tr>
      <td><code>spark.sql.sources.v2.bucketing.allowCompatibleTransforms.enabled</code></td>
      <td>false</td>
      <td>
        When enabled, try to avoid shuffle if partition transforms are compatible but not identical. This config requires both <code>spark.sql.sources.v2.bucketing.enabled</code> and <code>spark.sql.sources.v2.bucketing.pushPartValues.enabled</code> to be true.
      </td>
      <td>4.0.0</td>
    </tr>
    <tr>
      <td><code>spark.sql.sources.v2.bucketing.shuffle.enabled</code></td>
      <td>false</td>
      <td>
        When enabled, try to avoid shuffle on one side of the join, by recognizing the partitioning reported by a V2 data source on the other side.
      </td>
      <td>4.0.0</td>
    </tr>
  </table>

<p>If Storage Partition Join is performed, the query plan will not contain Exchange nodes prior to the join.</p>

<p>The following example uses Iceberg (<a href="https://iceberg.apache.org/docs/latest/spark-getting-started/">https://iceberg.apache.org/docs/latest/spark-getting-started/</a>), a Spark V2 DataSource that supports Storage Partition Join.</p>
<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">prod</span><span class="p">.</span><span class="n">db</span><span class="p">.</span><span class="n">target</span> <span class="p">(</span><span class="n">id</span> <span class="nb">INT</span><span class="p">,</span> <span class="n">salary</span> <span class="nb">INT</span><span class="p">,</span> <span class="n">dep</span> <span class="n">STRING</span><span class="p">)</span>
<span class="k">USING</span> <span class="n">iceberg</span>
<span class="n">PARTITIONED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">dep</span><span class="p">,</span> <span class="n">bucket</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">id</span><span class="p">))</span>

<span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">prod</span><span class="p">.</span><span class="n">db</span><span class="p">.</span><span class="k">source</span> <span class="p">(</span><span class="n">id</span> <span class="nb">INT</span><span class="p">,</span> <span class="n">salary</span> <span class="nb">INT</span><span class="p">,</span> <span class="n">dep</span> <span class="n">STRING</span><span class="p">)</span>
<span class="k">USING</span> <span class="n">iceberg</span>
<span class="n">PARTITIONED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">dep</span><span class="p">,</span> <span class="n">bucket</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">id</span><span class="p">))</span>

<span class="k">EXPLAIN</span> <span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">target</span> <span class="n">t</span> <span class="k">INNER</span> <span class="k">JOIN</span> <span class="k">source</span> <span class="n">s</span>
<span class="k">ON</span> <span class="n">t</span><span class="p">.</span><span class="n">dep</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">dep</span> <span class="k">AND</span> <span class="n">t</span><span class="p">.</span><span class="n">id</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="n">id</span>

<span class="c1">-- Plan without Storage Partition Join</span>
<span class="o">==</span> <span class="n">Physical</span> <span class="n">Plan</span> <span class="o">==</span>
<span class="o">*</span> <span class="n">Project</span> <span class="p">(</span><span class="mi">12</span><span class="p">)</span>
<span class="o">+-</span> <span class="o">*</span> <span class="n">SortMergeJoin</span> <span class="k">Inner</span> <span class="p">(</span><span class="mi">11</span><span class="p">)</span>
   <span class="p">:</span><span class="o">-</span> <span class="o">*</span> <span class="n">Sort</span> <span class="p">(</span><span class="mi">5</span><span class="p">)</span>
   <span class="p">:</span>  <span class="o">+-</span> <span class="n">Exchange</span> <span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">//</span> <span class="k">DATA</span> <span class="n">SHUFFLE</span>
   <span class="p">:</span>     <span class="o">+-</span> <span class="o">*</span> <span class="n">Filter</span> <span class="p">(</span><span class="mi">3</span><span class="p">)</span>
   <span class="p">:</span>        <span class="o">+-</span> <span class="o">*</span> <span class="n">ColumnarToRow</span> <span class="p">(</span><span class="mi">2</span><span class="p">)</span>
   <span class="p">:</span>           <span class="o">+-</span> <span class="n">BatchScan</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span>
   <span class="o">+-</span> <span class="o">*</span> <span class="n">Sort</span> <span class="p">(</span><span class="mi">10</span><span class="p">)</span>
      <span class="o">+-</span> <span class="n">Exchange</span> <span class="p">(</span><span class="mi">9</span><span class="p">)</span> <span class="o">//</span> <span class="k">DATA</span> <span class="n">SHUFFLE</span>
         <span class="o">+-</span> <span class="o">*</span> <span class="n">Filter</span> <span class="p">(</span><span class="mi">8</span><span class="p">)</span>
            <span class="o">+-</span> <span class="o">*</span> <span class="n">ColumnarToRow</span> <span class="p">(</span><span class="mi">7</span><span class="p">)</span>
               <span class="o">+-</span> <span class="n">BatchScan</span> <span class="p">(</span><span class="mi">6</span><span class="p">)</span>


<span class="k">SET</span> <span class="s1">'spark.sql.sources.v2.bucketing.enabled'</span> <span class="s1">'true'</span>
<span class="k">SET</span> <span class="s1">'spark.sql.iceberg.planning.preserve-data-grouping'</span> <span class="s1">'true'</span>
<span class="k">SET</span> <span class="s1">'spark.sql.sources.v2.bucketing.pushPartValues.enabled'</span> <span class="s1">'true'</span>
<span class="k">SET</span> <span class="s1">'spark.sql.requireAllClusterKeysForCoPartition'</span> <span class="s1">'false'</span>
<span class="k">SET</span> <span class="s1">'spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled'</span> <span class="s1">'true'</span>

<span class="c1">-- Plan with Storage Partition Join</span>
<span class="o">==</span> <span class="n">Physical</span> <span class="n">Plan</span> <span class="o">==</span>
<span class="o">*</span> <span class="n">Project</span> <span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="o">+-</span> <span class="o">*</span> <span class="n">SortMergeJoin</span> <span class="k">Inner</span> <span class="p">(</span><span class="mi">9</span><span class="p">)</span>
   <span class="p">:</span><span class="o">-</span> <span class="o">*</span> <span class="n">Sort</span> <span class="p">(</span><span class="mi">4</span><span class="p">)</span>
   <span class="p">:</span>  <span class="o">+-</span> <span class="o">*</span> <span class="n">Filter</span> <span class="p">(</span><span class="mi">3</span><span class="p">)</span>
   <span class="p">:</span>     <span class="o">+-</span> <span class="o">*</span> <span class="n">ColumnarToRow</span> <span class="p">(</span><span class="mi">2</span><span class="p">)</span>
   <span class="p">:</span>        <span class="o">+-</span> <span class="n">BatchScan</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span>
   <span class="o">+-</span> <span class="o">*</span> <span class="n">Sort</span> <span class="p">(</span><span class="mi">8</span><span class="p">)</span>
      <span class="o">+-</span> <span class="o">*</span> <span class="n">Filter</span> <span class="p">(</span><span class="mi">7</span><span class="p">)</span>
         <span class="o">+-</span> <span class="o">*</span> <span class="n">ColumnarToRow</span> <span class="p">(</span><span class="mi">6</span><span class="p">)</span>
            <span class="o">+-</span> <span class="n">BatchScan</span> <span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>


                </div>
            
             <!-- /container -->
        </div>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>
        <script src="https://code.jquery.com/jquery.js"></script>

        <script src="/js/vendor/anchor.min.js"></script>
        <script src="/js/main.js"></script>

        <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
        <script type="text/javascript">
            // DocSearch is entirely free and automated. DocSearch is built in two parts:
            // 1. a crawler which we run on our own infrastructure every 24 hours. It follows every link
            //    in your website and extract content from every page it traverses. It then pushes this
            //    content to an Algolia index.
            // 2. a JavaScript snippet to be inserted in your website that will bind this Algolia index
            //    to your search input and display its results in a dropdown UI. If you want to find more
            //    details on how works DocSearch, check the docs of DocSearch.
            docsearch({
    apiKey: 'd62f962a82bc9abb53471cb7b89da35e',
    appId: 'RAI69RXRSK',
    indexName: 'apache_spark',
    inputSelector: '#docsearch-input',
    enhancedSearchInput: true,
    algoliaOptions: {
      'facetFilters': ["version:4.0.0-preview2"]
    },
    debug: false // Set debug to true if you want to inspect the dropdown
});

        </script>

        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js' +
                    '?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    </body>
</html>
